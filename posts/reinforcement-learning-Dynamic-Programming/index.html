<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="zh-CN" 
  
>
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="强化学习（动态规划）" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="本文介绍了强化学习的动态规划法（Dynamic Programming，DP），采用动态规划的思想，分别介绍策略迭代和价值迭代方法。" />
<meta property="og:description" content="本文介绍了强化学习的动态规划法（Dynamic Programming，DP），采用动态规划的思想，分别介绍策略迭代和价值迭代方法。" />
<link rel="canonical" href="http://localhost:4000/posts/reinforcement-learning-Dynamic-Programming/" />
<meta property="og:url" content="http://localhost:4000/posts/reinforcement-learning-Dynamic-Programming/" />
<meta property="og:site_name" content="SIRLIS" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-11-26T11:24:19+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="强化学习（动态规划）" />
<meta name="twitter:site" content="@none" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-24T12:50:58+08:00","datePublished":"2022-11-26T11:24:19+08:00","description":"本文介绍了强化学习的动态规划法（Dynamic Programming，DP），采用动态规划的思想，分别介绍策略迭代和价值迭代方法。","headline":"强化学习（动态规划）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/reinforcement-learning-Dynamic-Programming/"},"url":"http://localhost:4000/posts/reinforcement-learning-Dynamic-Programming/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>强化学习（动态规划） | SIRLIS
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SIRLIS">
<meta name="application-name" content="SIRLIS">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.0/dist/tocbot.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/head.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <div class="site-title">
      <a href="/">SIRLIS</a>
    </div>
    <div class="site-subtitle fst-italic">分享科研和生活的日常</div>
  </div>
  <!-- .profile-wrapper -->

  <ul class="nav flex-column flex-grow-1 w-100 ps-0">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home"></i>
        <span>首页</span>
      </a>
    </li>
    <!-- the real tabs -->
    
      <li class="nav-item">
        <a href="/categories/" class="nav-link">
          <i class="fa-fw fas fa-stream"></i>
          

          <span>分类</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/tags/" class="nav-link">
          <i class="fa-fw fas fa-tags"></i>
          

          <span>标签</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/archives/" class="nav-link">
          <i class="fa-fw fas fa-archive"></i>
          

          <span>归档</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/about/" class="nav-link">
          <i class="fa-fw fas fa-info-circle"></i>
          

          <span>关于</span>
        </a>
      </li>
      <!-- .nav-item -->
    
  </ul>
  <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/sirlis"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/none"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['lihongjue','buaa.edu.cn'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</div>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container px-xxl-5">
        <!-- The Top Bar -->

<div id="topbar-wrapper">
  <div
    id="topbar"
    class="container d-flex align-items-center justify-content-between h-100"
  >
    <span id="breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                首页
              </a>
            </span>

          
        
          
        
          
            
              <span>强化学习（动态规划）</span>
            

          
        
      
    </span>
    <!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      文章
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="搜索..."
      >
    </span>
    <span id="search-cancel">取消</span>
  </div>
</div>

        











<div class="row">
  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pe-xl-4">
    

    <div class="post px-1 px-md-2">
      

      
        
      
        <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->



  
  

  <!-- CDN URL -->
  

  <!-- Add image path -->
  

  
    
      
      
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  




<!-- return -->




<h1 data-toc-skip>强化学习（动态规划）</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      发表于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1669433059"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2022/11/26
</em>

    </span>

    <!-- lastmod date -->
    
    <span>
      更新于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1742791858"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2025/03/24
</em>

    </span>
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      作者

      <em>
      
        <a href="https://github.com/sirlis">sirlis</a>
      
      </em>
    </span>

    <div>
      <!-- read time -->
      <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="5633 字"
>
  <em>31 分钟</em>阅读</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <p>本文介绍了强化学习的动态规划法（Dynamic Programming，DP），采用动态规划的思想，分别介绍策略迭代和价值迭代方法。</p>

<!--more-->

<hr />

<ul>
  <li><a href="#1-强化学习问题的求解">1. 强化学习问题的求解</a></li>
  <li><a href="#2-动态规划">2. 动态规划</a>
    <ul>
      <li><a href="#21-策略迭代">2.1. 策略迭代</a>
        <ul>
          <li><a href="#211-策略评估">2.1.1. 策略评估</a></li>
          <li><a href="#212-策略改进">2.1.2. 策略改进</a></li>
          <li><a href="#213-算法流程">2.1.3. 算法流程</a></li>
        </ul>
      </li>
      <li><a href="#22-价值迭代">2.2. 价值迭代</a>
        <ul>
          <li><a href="#221-算法流程">2.2.1. 算法流程</a></li>
          <li><a href="#222-策略提升">2.2.2. 策略提升</a></li>
          <li><a href="#223-价值提升">2.2.3. 价值提升</a></li>
        </ul>
      </li>
      <li><a href="#23-截断策略迭代">2.3. 截断策略迭代</a></li>
      <li><a href="#24-对动态规划过程进行改进">2.4. 对动态规划过程进行改进</a></li>
    </ul>
  </li>
  <li><a href="#3-参考文献">3. 参考文献</a></li>
</ul>

<h2 id="1-强化学习问题的求解"><span class="me-2">1. 强化学习问题的求解</span><a href="#1-强化学习问题的求解" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>强化学习的最终目标是为了求解最优策略，而最优策略一定对应着最优的价值函数（只要知道了最优价值函数就能很轻松的得到最优策略），因此可将强化学习的目标分解为以下两个基本问题：</p>

<ul>
  <li><strong>预测问题</strong>，也即预测给定策略的状态价值函数。给定强化学习的6个要素：状态集$S$, 动作集$A$, 模型状态转移概率矩阵$P$, 即时奖励$R$，衰减因子$\gamma$,  给定策略$\pi$， 求解该策略的状态价值函数$v_\pi(s)$ 或动作价值函数 $q_\pi(s,a)$；</li>
  <li><strong>控制问题</strong>，也即求解最优的价值函数和策略。给定强化学习的5个要素：状态集$S$, 动作集$A$, 模型状态转移概率矩阵$P$, 即时奖励$R$，衰减因子$\gamma$, 求解最优的状态价值函数$v_{\star}$ 和最优策略 $\pi_{\star}$。　</li>
</ul>

<h2 id="2-动态规划"><span class="me-2">2. 动态规划</span><a href="#2-动态规划" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>动态规划（Dynamic Programming，DP）是一种将复杂问题简单化的思想，而不是指某种具体的算法。DP算法通过把复杂问题分解为子问题，通过求解子问题进而得到整个问题的解。在解决子问题的时候，其结果通常需要存储起来被用来解决后续复杂问题。</p>

<p>一个复杂问题可以使用DP思想来求解，只要满足两个性质就可以：(1)一个复杂问题的最优解由数个小问题的最优解构成，可以通过寻找子问题的最优解来得到复杂问题的最优解；(2)子问题在复杂问题内重复出现，使得子问题的解可以被存储起来重复利用。</p>

<p>巧了，强化学习要解决的问题刚好满足这两个条件。还记得贝尔曼方程吗？</p>

\[\begin{aligned}
v(s)&amp; \leftarrow\mathbb{E}_\pi[R_{t+1}+\gamma v(s^\prime)]\\
\Rightarrow v(s) &amp; = \sum_{a\in A}\pi(a\vert s)[R_s^a+\gamma \sum_{s^\prime \in S}P_{ss^\prime}^a v(s^\prime)]\\
\end{aligned}\]

<p>不难发现，当模型已知时（即 $A,P_{ss^\prime}^a, R_s^a$ 已知），我们可以定义出子问题求解每个状态的状态价值函数，同时这个式子又是一个递推的式子, 意味着利用它，我们可以使用上一个迭代周期内的状态价值来计算更新当前迭代周期某状态 $s$ 的状态价值（详见策略迭代）。可见，使用动态规划来求解强化学习问题是比较自然的。</p>

<blockquote>
  <p>此处有一个概念：值函数的计算用到了bootstapping的方法。所谓bootstrpping本意是指自举，此处是指当前值函数的计算用到了后继状态的值函数，也即用后继状态的值函数计算当前值函数。</p>
</blockquote>

<p>动态规划要求马尔可夫决策过程五元组完全已知，即 $&lt;S,A,P,R,\gamma&gt;$ 是完全确定的。在求解强化学习问题时，动态规划方法就是一种基于模型的方法（model-based）。</p>

<h3 id="21-策略迭代"><span class="me-2">2.1. 策略迭代</span><a href="#21-策略迭代" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>知道了动态规划与强化学习的联系，我们就能用DP的思想去求解强化学习问题。策略迭代包括策略评估（Policy Evaluation）和策略改进（Policy Improvement），其基本过程是<strong>从一个初始化的策略出发，先进行策略评估，然后策略改进，评估改进的策略，再进一步改进策略，经过不断迭代更新，直到策略收敛。</strong></p>

<p>下面具体介绍策略评估和策略改进。</p>

<h4 id="211-策略评估"><span class="me-2">2.1.1. 策略评估</span><a href="#211-策略评估" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<ul>
  <li>解析求解</li>
</ul>

<p>记</p>

\[\begin{aligned}
v_\pi &amp;\doteq [v_\pi(s_1)\; v_\pi(s_2)\; \cdots]^T_{n\times 1}\\
r_\pi &amp;\doteq [r_\pi(s_1)\; r_\pi(s_2)\; \cdots]^T_{n\times 1}\\
P_\pi &amp;\doteq [P_\pi(s,s^\prime)]_{n\times n}
\end{aligned}\]

<p>根据贝尔曼期望方程，可以看出其是一个关于 $v_\pi(s)$ 的线性方程</p>

\[\begin{aligned}
v_\pi(s) =&amp; \sum_a \pi(a\vert s) \sum_{s^\prime}\sum_r p(s^\prime,r \vert s,a) [  r+\gamma v_\pi(s^\prime)  ]\\
=&amp;\sum_a \pi(a\vert s) \sum_{s^\prime}\sum_r p(s^\prime,r \vert s,a)r + \gamma \sum_a \pi(a\vert s) \sum_{s^\prime}\sum_r p(s^\prime,r \vert s,a)v_\pi(s^\prime)\\
\end{aligned}\]

<p>根据前一章内容我们知道，对系统动态特性 $p(s^\prime,r \vert s,a)$ 可以分解为两个概率：奖励概率和状态转移概率。</p>

<p>其中第一项（积掉 $s^\prime$，对应奖励概率）</p>

\[\begin{aligned}
\sum_a \pi(a\vert s) \sum_{s^\prime}\sum_r p(s^\prime,r \vert s,a)r &amp;=\sum_a \pi(a\vert s) \sum_r p(r \vert s,a) r\\
&amp;= \sum_a \pi(a\vert s) \mathbb{E}_\pi[R_{t+1}\vert s,a]\\
&amp;\doteq \sum_a \pi(a\vert s) r(s,a)\quad \textcolor{blue}{（定义r(s,a)）}\\ 
&amp;\doteq r_\pi(s)\quad \quad \quad \quad \quad \quad \textcolor{blue}{（定义r_\pi(s)）}
\end{aligned}\]

<p>其中第二项（积掉 $r$，对应状态转移概率）</p>

\[\begin{aligned}
\gamma \sum_a \pi(a\vert s) \sum_{s^\prime}\sum_r p(s^\prime,r \vert s,a)v_\pi(s^\prime) &amp;=\gamma \sum_a \pi(a\vert s) \sum_{s^\prime} p(s^\prime \vert s,a)v_\pi(s^\prime)\\
&amp;=\gamma \sum_{s^\prime} \sum_a \pi(a\vert s) p(s^\prime \vert s,a)v_\pi(s^\prime)\\
&amp;\doteq\gamma \sum_{s^\prime} P_\pi(s,s^\prime)v_\pi(s^\prime)\quad \textcolor{blue}{（积掉a，定义P_\pi(s,s^\prime)）}\\
\end{aligned}\]

<p>于是有</p>

\[\begin{aligned}
v_\pi(s) =&amp; r_\pi(s) + \gamma \sum_{s^\prime} P_\pi(s,s^\prime) v_\pi(s^\prime)
\end{aligned}\]

<p>这里我们发现，正好可以得到上一章提到的贝尔曼方程的矩阵形式，令 $s_i = s, s_j = s^\prime$ 有</p>

\[\begin{aligned}
v_\pi(s_i) =&amp; r_\pi(s_i) + \gamma \sum_{j=1}^n P_\pi(s_i,s_j) v_\pi(s_j)\\
\Rightarrow V_\pi &amp;= R_\pi + \gamma P_\pi V_\pi \quad （上式即为该式第i行）\\
\Rightarrow V_\pi &amp;= (I-\gamma P_\pi)^{-1}R_\pi
\end{aligned}\]

<p>上述计算复杂度为 $O(n^3)$（$n$ 阶矩阵求逆时间复杂度为$O(n^3)$，相乘时间复杂度为$O(n^3)$，二者顺序执行时间复杂度为$2O(n^3)$，但考虑计算复杂度时不用考虑系数）。当状态维度较高时，上述计算过于复杂，因此用后面的迭代求解方法。</p>

<ul>
  <li>迭代求解</li>
</ul>

<p>策略评估迭代求解的基本思路是从<strong>任意初始的策略和初始的状态价值函数开始</strong>，结合贝尔曼方程、状态转移概率和奖励，同步迭代更新状态价值函数，直至其收敛，得到该策略下最终的状态价值函数。策略评估旨在求解预测问题。</p>

<p>假设给定一个策略 $\pi$，和初始时刻所有状态的状态价值 $v_0(s)$。</p>

<p>第 $k$ 轮迭代，已经计算出所有状态价值，则第 $k+1$ 轮迭代如何计算？</p>

<p>可以结合贝尔曼方程构造第 $k+1$ 轮迭代的状态价值函数如下</p>

\[\begin{aligned}
v_{\color{red}{k+1}}(s)  \doteq &amp; \sum_{a\in A}\pi(a\vert s)[R_s^a+\gamma \sum_{s^\prime \in S}P_{ss^\prime}^a v_{\color{red}k}(s^\prime)]\\
&amp; = \sum_{a\in A}\pi(a\vert s)\sum_{s\prime, r}p(s^\prime,r \vert s, a)[r+\gamma v_{\color{red}k}(s^\prime)]    
\end{aligned}\]

<p>问题转化为上式是否收敛？是否收敛到 $v_\pi$？前面的章节我们已经介绍了，使用<strong>不动点定理</strong>可以证明其解的唯一性和收敛性，另可参考：</p>

<blockquote>
  <p>知乎：Model-Based方法：策略迭代与价值迭代：https://zhuanlan.zhihu.com/p/699134142 ，包含收敛性证明</p>
</blockquote>

<p>为了使用顺序执行的计算机程序实现策略评估，我们需要构造两个数组：一个用于存储旧的价值函数 $v_k(s)$，一个用于存储新的价值函数 $v_{k+1}(s)$。这样，在旧的价值函数不变的情况下，新的价值函数可以一个个被计算出来。同样，也可以简单使用一个数组来进行<strong>就地更新</strong>（in-place），即每次直接使用新的价值函数替换旧的价值函数。这种就地更新的方式依然可以保证收敛到$v_\pi$，并且收敛速度更快。就地更新时的价值函数迭代式子如下：</p>

\[{\color{red}v(s)} \leftarrow \sum_a \pi(a\vert s) \sum_{s^\prime,r}p(s^\prime,r\vert s,a)[r+\gamma {\color{red}v(s^\prime)}]\]

<hr />

<p>下面给出一个经典的 Grid World 例子。假设有一个4x4的16宫格。只有左上和右下的格子是终止格子。该位置的价值固定为0，个体如果到达了该两个格子，则停止移动，此后每轮奖励都是0。注意个体每次只能移动一个格子，且只能上下左右4种移动选择，不能斜着走, 如果在边界格往外走，则会直接移动回到之前的边界格。</p>

<p><a href="/assets/img/postsimg/20221126/gridworld.png" class="popup img-link "><img data-src="/assets/img/postsimg/20221126/gridworld.png" alt="Grid World" class="lazyload" data-proofer-ignore></a></p>

<p>下面对问题进行定义：</p>
<ul>
  <li>$R_s^a=-1$，即个体在16宫格其他格的每次移动，得到的<strong>即时奖励</strong>都为-1。</li>
  <li>$\gamma=1$，即<strong>奖励的累计不衰减</strong>。</li>
  <li>$P_{ss^\prime}^a=1$，即每次移动到的下一格都是固定的（如往上走一定到上面的格子），<strong>不考虑转移不确定</strong>的情况；</li>
  <li>$\pi(a\vert s) = 0.25, \forall a\in A$，即<strong>给定随机策略</strong>，每个格子里有25%的概率向周围的4个格子移动。</li>
</ul>

<p>至此，马尔可夫决策过程的所有参数均已知，下面进行状态价值函数预测。</p>

<p><a href="/assets/img/postsimg/20221126/gridworldVprediction.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221126/gridworldVprediction.jpg" alt="Grid World state value prediction" class="lazyload" data-proofer-ignore></a></p>

<p>$k=1$ 时，带入贝尔曼方程，计算第二行第一个格子的价值（其他的格子类似）</p>

\[v_1^{21}=0.25[(-1+0(up))+(-1+0(down))+(-1+0(left))+(-1+0(right))]=-1\]

<p>$k=2$ 时，继续计算第二行第一个格子的价值（其他的格子类似）</p>

\[v_2^{21}=0.25[(-1+0(up))+(-1-1(down))+(-1-1(left))+(-1-1(right))]=-1.75\]

<p><strong>如此迭代直至每个格子的状态价值改变很小为止</strong>。这时我们就得到了所有格子的基于随机策略的状态价值。</p>

<p>可以看到，动态规划的策略评估计算过程并不复杂，但是如果我们的问题是一个非常复杂的模型的话，这个计算量还是非常大的。</p>

<h4 id="212-策略改进"><span class="me-2">2.1.2. 策略改进</span><a href="#212-策略改进" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>在给定策略 $\pi$ 的条件下，通过上面的策略评估可以迭代计算得到价值函数，但仍然没有得到最优策略。因为如果从状态 $s$ 开始执行现有策略，最终结果就是 $v_\pi(s)$，但我们不知道是否有更好的策略。那么如何进行策略改进呢？我们可以利用 “策略改进定理” 来实现。</p>

<ul>
  <li><strong>策略改进定理</strong></li>
</ul>

<blockquote>
  <p>策略改进定理
给定 $\pi,\pi^\prime$，
如果 $\forall s\in S, q_\pi(s,\pi^\prime(s))\geq v_\pi(s)$，
那么有 $\forall s\in S, v_{\pi^\prime}(s)\geq v_\pi(s)$，
即 $\pi^\prime \geq \pi$。</p>
</blockquote>

<p>证明如下：</p>

\[\begin{aligned}
v_\pi(s) &amp;\leq q_\pi(s,\pi^\prime(s))\\
&amp;=\mathbb{E}[R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots\vert S_t=s,A_t = \pi^\prime(s)]\\
&amp;=\mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})\vert S_t=s,A_t = \pi^\prime(s)]\\
&amp;=\mathbb{E}_{\pi^\prime}[R_{t+1}+\gamma v_\pi(S_{t+1})\vert S_t=s]\quad (走1步：R_{t+1}由\pi^\prime 控制，后面由\pi控制)\\
&amp;\textcolor{red}{\leq} \mathbb{E}_{\pi^\prime}[R_{t+1}+\gamma \textcolor{red}{q_\pi(S_{t+1},\pi^\prime(S_{t+1}))}\vert S_t=s]\quad （条件带入）\\
&amp;=\mathbb{E}_{\pi^\prime}[R_{t+1}+\gamma \textcolor{blue}{\mathbb{E}_{\pi^{\prime}}[R_{t+2}+\gamma v_\pi(S_{t+2})\vert S_{t+1}]}   \vert S_t=s]\quad （前式带入）\\
&amp;=\mathbb{E}_{\pi^\prime}[R_{t+1}+\gamma \mathbb{E}_{\pi^{\prime}}[R_{t+2}\vert S_{t+1}]+\gamma^2 \mathbb{E}_{\pi^{\prime}} [v_\pi(S_{t+2}) \vert S_{t+1} ]  \vert S_t=s]\\
&amp;=\mathbb{E}_{\pi^\prime}[R_{t+1}+R_{t+2}+\gamma^2 v_\pi(S_{t+2})\vert S_t=s] \quad （走2步）\\
&amp;\leq\cdots\\
&amp;=\mathbb{E}_\pi^\prime[R_{t+1}+\gamma R_{t+2}+\cdots \vert S_t=s] \\
&amp;= v_{\pi^\prime}(s)
\end{aligned}\]

<blockquote>
  <p>参考推导见：<a href="https://zhuanlan.zhihu.com/p/533279050">策略改进定理及证明中隐式期望的处理</a></p>

</blockquote>

<p>基于以上证明，我们知道策略改进是切实可行的，那么究竟怎么做才能更新策略呢？</p>

<ul>
  <li><strong>贪心方法</strong></li>
</ul>

<p>我们知道：</p>

\[v_{\pi}(s) = \mathbb{E}_{a}[q_{\pi}(s,a)]\]

<p>而根据期望的定义，最大的 $q$ 函数肯定是大于等于其期望的，即</p>

\[q_\pi(s,a)_{max} \geq v_\pi(s)\]

<p>那么我们每次都选择使得 $q_\pi(s,a)$ 最大的那个动作，构成新策略 $\pi^\prime$，就可以保证满足策略改进定理，构成的策略就是更好的策略了！这就是贪心方法。具体而言，在当前策略对应的<strong>状态价值函数</strong>下，智能体在每个状态都计算一下所有动作各自的<strong>状态-动作价值函数</strong>，选出值最大的执行就可以。贪心方法如下：</p>

\[\forall s\in S,\;  \pi^\prime(s) =
\left\{
\begin{aligned}
1, \quad &amp; a=\mathop{\text{argmax}}\limits_a\; q_\pi(s,a) \\
0, \quad &amp; \text{otherwise}
\end{aligned}
\right.\]

<p>此时</p>

\[\forall s\in S,\; v_\pi(s) = \mathbb{E}_a[q_\pi(s,a)] \leq \max_a q_\pi(s,a) = \max_a q_\pi(s,\pi^\prime(s))\]

<p>满足策略改进定理条件，因此有</p>

\[\forall s\in S,\;v_{\pi^\prime}(s) \geq v_\pi(s)\]

<p>得证。</p>

<p>因此，策略改进定理提供了一种更新策略的方式：对每个状态 $s$ ，寻找贪婪动作 $\mathop{\text{argmax}}\limits_a q_\pi(s,a)$ ，以贪婪动作作为新的策略 $\pi^\prime$ ，根据策略改进定理必然有 $\pi^\prime \geq \pi$ 。</p>

<hr />

<p>依然是 Grid World 例子，前面我们给定一个随机策略 $\pi(a\vert s) = 0.25, \forall a\in A$，并得到了其对应的状态价值函数。</p>

<p>根据前面的策略改进定理，可以采用<strong>贪婪法</strong>来改进我们的这个策略。具体而言，<strong>个体在某个状态下选择的行为是其能够到达后续所有可能的状态中状态价值最大的那个状态</strong>，如上图右侧所示，最终求解控制问题。</p>

<p>当我们计算出最终的状态价值后，我们发现：</p>
<ul>
  <li>第二行第一个格子周围的价值分别是0,-18,-20，此时我们用贪婪法，则我们调整行动策略为向状态价值为0的方向（上方）移动，而不是随机移动。而此时</li>
  <li>第二行第二个格子周围的价值分别是-14,-14,-20, -20。那么我们整行动策略为向状态价值为-14的方向（左或者上）移动。</li>
  <li>……以此类推。</li>
</ul>

<blockquote>
  <p>注意到：上述过程的策略是基于最大的 $v_\pi(s)$，但实际上应该根据 $q_\pi(s,a)$ 来调整策略 
$q_\pi(s,a) = \mathbb{E}[R_{t+1}+\gamma v_\pi(s^\prime) ]$
但由于本例中，所有可行的的状态转化概率P=1，瞬时奖励都是-1，衰减因子定义为1，所以其实 $q$ 函数的值就是下一个状态的状态价值 $v$，这也就是为什么直接往状态价值最大的那个状态移动就可以的原因。</p>
</blockquote>

<p><strong>总结</strong>，策略迭代就是在循环进行两部分工作，第一步是使用当前策略 $\pi$ 评估计算当前策略的最终状态价值 $v$，第二步是根据状态价值 $v$ 根据一定的方法（比如贪婪法）更新策略 $\pi$，接着回到第一步，一直迭代下去，最终得到收敛的策略 $\pi_{\star}$ 和状态价值 $v_∗$。</p>

<h4 id="213-算法流程"><span class="me-2">2.1.3. 算法流程</span><a href="#213-算法流程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>最终的策略迭代的算法表述如下：</p>

<ol>
  <li><strong>初始化</strong>
    <ul>
      <li>对于所有 $s\in S$，任意初始化$V(s)$ 和 $\pi(s)$</li>
      <li>给定 $p(s^\prime,r\vert s,a)$ 和 $r$ 和 $\gamma$</li>
      <li>给定一个很小的正整数 $\theta$</li>
    </ul>
  </li>
  <li><strong>策略评估</strong>
    <ul>
      <li>循环($k$)：
        <ul>
          <li>$\Delta = 0$</li>
          <li>对于每个 $s\in S$：
            <ul>
              <li>$v \leftarrow v_k(s)$</li>
              <li>$v_{k+1}(s) =\sum_a \pi(a\vert s) \sum_{s^\prime,r}p(s^\prime,r\vert s,a)[r+\gamma v_{k}(s^\prime)]\qquad\textcolor{red}{贝尔曼方程}$</li>
              <li>$\Delta = \max(\Delta, v-v_{k+1}(s))$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>直至 $\Delta &lt; \theta$  （$v(s)$收敛）</li>
    </ul>
  </li>
  <li><strong>策略改进</strong>
    <ul>
      <li>$policy\;stable \leftarrow true$</li>
      <li>对于每个 $s\in S$：
        <ul>
          <li>$a_{old} = \pi(s)$</li>
          <li>$q_\pi(s,a) = \sum_{s^\prime,r}p(s^\prime,r\vert s,a)[r+\gamma v(s^\prime)]$</li>
          <li>$\pi(s)\leftarrow \mathop{\text{argmax}}\limits_a\; q_\pi(s,a) $ （贪婪法）</li>
          <li>如果 $a_{old} \neq \pi(s)$ 那么 $policy\;stable \leftarrow false$</li>
        </ul>
      </li>
      <li>如果 $policy\;stable \leftarrow true$，停止迭代，得到 $v_\star\approx v, \pi_\star\approx \pi$；否则返回步骤 2</li>
    </ul>
  </li>
</ol>

<p>算法的流程示意图如下（含有 $v$ 迭代和 $\pi$ 迭代两层迭代）：</p>

\[\begin{aligned}
&amp; v_1 \rightarrow \cdots \rightarrow v_{1fin} \rightarrow \pi_1 &amp;\text{第1次策略迭代}\\
\rightarrow &amp; v_2 \rightarrow \cdots \rightarrow v_{2fin} \rightarrow \pi_2 &amp;\text{第2次策略迭代}\\\
\rightarrow &amp; \cdots &amp;\\
\rightarrow &amp;v_i \rightarrow \cdots \rightarrow v_\star \rightarrow \pi_\star &amp;\text{第n次策略迭代}\
\end{aligned}\]

<h3 id="22-价值迭代"><span class="me-2">2.2. 价值迭代</span><a href="#22-价值迭代" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>在策略迭代中，状态价值函数在策略评估过程中时通过迭代的形式计算的，收敛后再通过策略改进更新策略，策略发生变化后需要再次迭代状态价值函数，如此往复迭代非常耗时。注意到，在迭代计算状态价值函数的时候策略并不是最优的，用一个非最优的策略来计算完全准确的状态价值函数并没有太大意义。另外，整个策略迭代通常在迭代几次后就可以收敛（如上文中的 Grid World 例子，第三次迭代（$k=3$）后，策略就已经达到最优），因此我们可以提前截断迭代过程。</p>

<p>一种重要的特殊的情况是，<strong>只进行一次策略迭代后即刻停止</strong>（对每一个状态进行一轮迭代更新），该算法被称为<strong>价值迭代</strong>。价值迭代的本质是利用 <strong>贝尔曼最优方程</strong> 来进行价值函数的迭代优化，其能保证最终收敛到最优价值函数，此时对应的策略也是<strong>最优</strong>的。</p>

<p>价值迭代算法是策略评估过程只进行一次迭代的策略迭代算法，其过程为 ：对每一个当前状态 $s$ ，对每个可能的动作 $a$ 都计算一下采取这个动作后到达的下一个状态的期望价值。<strong>选择最大的期望价值函数作为当前状态的价值函数</strong> $v(s)$ ，循环执行这个步骤，直到价值函数收敛。</p>

<h4 id="221-算法流程"><span class="me-2">2.2.1. 算法流程</span><a href="#221-算法流程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>价值迭代的算法表述如下：</p>

<ol>
  <li><strong>初始化</strong>
    <ul>
      <li>对于所有 $s\in S$，任意初始化$V(s)$ 和 $\pi(s)$</li>
      <li>给定 $p(s^\prime,r\vert s,a)$ 和 $r$ 和 $\gamma$</li>
      <li>给定一个很小的正整数 $\theta$</li>
    </ul>
  </li>
  <li><strong>策略评估</strong>
    <ul>
      <li>循环($k$)：
        <ul>
          <li>$\Delta = 0$</li>
          <li>对于每个 $s\in S$：
            <ul>
              <li>$v \leftarrow v_k(s)$</li>
              <li>$v_{k+1}(s) \leftarrow {\color{red}\max_a} \sum_{s^\prime,r}p(s^\prime,r\vert s,a)[r+\gamma v_{k}(s^\prime)]\qquad\textcolor{red}{贝尔曼最优方程}$</li>
              <li>$\Delta = \max(\Delta, v-v_{k+1}(s))$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>直至 $\Delta &lt; \theta$  （$v(s)$收敛）</li>
    </ul>
  </li>
  <li><strong>策略改进</strong>
    <ul>
      <li>$q_\pi(s,a) = \sum_{s^\prime,r}p(s^\prime,r\vert s,a)[r+\gamma v(s^\prime)]$</li>
      <li>$\pi(s)\leftarrow \mathop{\text{argmax}}\limits_a\; q_\pi(s,a) $</li>
    </ul>
  </li>
</ol>

<p>可以看出：</p>
<ul>
  <li>更新目标：根据 <code class="language-plaintext highlighter-rouge">max</code> 操作符我们可以发现，价值迭代的更新目标不再是 $v_\pi$ ，公式中没有任何显示的策略 $\pi$ 的影子，反之，其更新目标正是最优价值函数（也即最优策略），一旦价值迭代收敛，基于其产生的贪婪策略就是最优策略；</li>
  <li>更新方式： 从 $\sum_{s^\prime,r}p(s^\prime,r\vert s,a)$ 项我们可以看出，依然是期望更新的方式，这一点和策略评估是一致的。不同之处在于策略评估中的 $\pi(s)$ 被更换为了具有最大价值的 $a$ ，即策略迭代中策略评估估计状态价值采用了关于策略分布的期望，而价值迭代中的策略评估采用了最大值；</li>
</ul>

<p>线性化示意图如下（只含有 $v$ 迭代）：</p>

\[v_1 \rightarrow \cdots \rightarrow v_2 \rightarrow \cdots \rightarrow v_{\star} \rightarrow \pi_{\star}\]

<p>可以看出，价值迭代是极端情况下的策略迭代。</p>

<p>重新回顾价值迭代中的策略评估部分，如果从更加常用的状态-动作价值函数的角度考虑，可以拆分成两步：</p>

<h4 id="222-策略提升"><span class="me-2">2.2.2. 策略提升</span><a href="#222-策略提升" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>首先根据初始化或者上一步迭代得到的状态价值函数 $v_k(s), v_k(s^\prime)$，确定新的贪婪策略</p>

\[\pi_{k+1}(a\vert s) = \arg\max_\pi \sum_{s^\prime,r}p(s^\prime,r\vert s,a)[r+\gamma v(s_k^\prime)] = \arg\max_a q_k(s,a)\]

<h4 id="223-价值提升"><span class="me-2">2.2.3. 价值提升</span><a href="#223-价值提升" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>然后根据新的贪婪策略确定新的状态价值函数，由于策略是贪婪的，相当于选择最大的状态-动作价值函数作为状态价值函数</p>

\[v_{k+1}(s) = \max_a \sum_{s^\prime,r}p(s^\prime,r\vert s,a)[r+\gamma v(s_k^\prime)] = \max_a q_k(s,a)\]

<h3 id="23-截断策略迭代"><span class="me-2">2.3. 截断策略迭代</span><a href="#23-截断策略迭代" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>对于价值迭代而言，前述策略提升和策略改进合并简化后，即为前述价值迭代的算法流程中的一行（贝尔曼最优方程）迭代表示。</p>

<p>注意到，从算法流程的角度，二者存在两种解读视角，如下图所示</p>

<p><a href="/assets/img/postsimg/20221126/policy%20and%20value%20iteration.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221126/policy%20and%20value%20iteration.jpg" alt="policy and value iteration" class="lazyload" data-proofer-ignore></a></p>

<font color="red">此时，【价值迭代】可以看作是策略评估【只进行一步计算的特殊策略迭代】。</font>

<p>那么，有没有一种中间方式，既不会只算一次这么极端，又不要一直计算到收敛那么极端呢？有，即<strong>截断策略迭代</strong>。截断策略迭代的初衷是理论的策略评估过程需要计算无穷步，这在实际计算中代价太大。因此，在传统的策略迭代（贝尔曼方程）的基础上，对策略评估过程进行人为截断，比如设置 $n=3$ 进行截断，从而在保证依然可以收敛的前提下加速计算。</p>

<p>收敛性证明比较复杂，可参考赵世钰老师的《Mathematical Foundation of Reinforcement Learning》。</p>

<p>几种不同算法收敛结果如下图所示：</p>

<p><a href="/assets/img/postsimg/20221126/truncated%20policy%20iteration.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221126/truncated%20policy%20iteration.jpg" alt="convergence" class="lazyload" data-proofer-ignore></a></p>

<h3 id="24-对动态规划过程进行改进"><span class="me-2">2.4. 对动态规划过程进行改进</span><a href="#24-对动态规划过程进行改进" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ul>
  <li>
    <p><strong>异步动态规划</strong>：每次进行价值估计和更新时都要对全部的状态进行一次遍历，这未免太过繁琐，尤其是当状态空间足够大时，消耗在遍历上的资源就不尽人意了。那能不能一次更新只针对部分状态进行呢？只更新部分状态还能保证价值更新的收敛吗？可以的，在异步动态规划算法中，每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性地更新部分状态的价值，这种算法能显著节约计算资源，并且只要所有状态能够得到持续的访问更新，那么也能确保算法收敛至最优解。</p>

    <ul>
      <li>
        <font color="Blue">优先级动态规划</font>
        <p>(prioritised sweeping)：对每一个状态进行优先级分级，优先级越高的状态其状态价值优先得到更新。</p>
      </li>
      <li>
        <font color="Blue">实时动态规划</font>
        <p>(real-time dynamic programming)：直接使用个体与环境交互产生的实际经历来更新状态价值，对于那些个体实际经历过的状态进行价值更新。</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>原位动态规划</strong> (in-place dynamic programming)：直接利用当前状态的后续状态的价值来更新当前状态的价值，可以大幅降低动态规划过程中的资源消耗，同样也可以收敛至最优解。比如，价值迭代中同样需要存储两份状态价值函数 $v_{k+1}(s),\; v_k(s)$，实际上也可以只保存一份，即采用就地更新方法，此时迭代式变为
 \({\color{red}v(s)} \leftarrow \max_a \sum_{s^\prime,r}p(s^\prime,r\vert s,a)[r+\gamma {\color{red}v(s^\prime)}]\)</p>
  </li>
</ul>

<h2 id="3-参考文献"><span class="me-2">3. 参考文献</span><a href="#3-参考文献" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>[1] 刘建平Pinard. <a href="https://www.cnblogs.com/pinard/p/9463815.html">强化学习（三）用动态规划（DP）求解</a>.</p>

<p>[2] Zeal. <a href="https://zhuanlan.zhihu.com/p/358464793">知乎：强化学习二：策略迭代法</a></p>

<p>[3] shuhuai008. <a href="https://www.bilibili.com/video/BV1nV411k7ve">bilibili【强化学习】动态规划【白板推导系列】</a></p>

<p>[4] 韵尘. <a href="https://zhuanlan.zhihu.com/p/537229275">知乎：4.2 —— 策略改进（Policy Improvement）</a>（含收敛性证明）</p>

<p>[5] 韵尘. <a href="https://zhuanlan.zhihu.com/p/537705334">知乎：4.5 —— 异步动态规划（Asynchronous Dynamic Programming）</a></p>

</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw me-1"></i>
    
      <a href='/categories/academic/'>Academic</a>,
      <a href='/categories/knowledge/'>Knowledge</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw me-1"></i>
      
      <a href="/tags/python/"
          class="post-tag no-text-decoration" >python</a>
      
      <a href="/tags/reinforcement-learning/"
          class="post-tag no-text-decoration" >reinforcement learning</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        本文由作者按照 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         进行授权

      
    </div>

    <!-- Post sharing snippet -->

<div class="share-wrapper">
  <span class="share-label text-muted me-1">分享</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BC%89%20-%20SIRLIS&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Dynamic-Programming%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BC%89%20-%20SIRLIS&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Dynamic-Programming%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Dynamic-Programming%2F&text=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BC%89%20-%20SIRLIS"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <i
      id="copy-link"
      class="fa-fw fas fa-link small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="分享链接"
      data-title-succeed="链接已复制！"
    >
    </i>
  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
      
    </div>
  </div>
  <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
    <div class="access">
      <!-- Get the last 5 posts from lastmod list. -->














  <div id="access-lastmod" class="post">
    <div class="panel-heading">最近更新</div>
    <ul class="post-content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Value-Approximation/">强化学习（值函数近似）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/vscode-c/">VSCode部署C/C++开发环境</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-markov-process/">强化学习（马尔可夫决策过程）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Temporal-Differences/">强化学习（时序差分法）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Monte-Carlo/">强化学习（蒙特卡洛法）</a>
        </li>
      
    </ul>
  </div>
  <!-- #access-lastmod -->


      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>

    
      
      



  <div id="toc-wrapper" class="ps-0 pe-4 mb-5">
    <div class="panel-heading ps-3 pt-2 mb-2">文章内容</div>
    <nav id="toc"></nav>
  </div>


    
  </div>
</div>

<!-- tail -->

  <div class="row">
    <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-3 pe-xl-4 mt-5">
      
        
        <!--
  Recommend the other 3 posts according to the tags and categories of the current post,
  if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->








  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  






<!-- Fill with the other newlest posts -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ms-1" data-toc-skip>
      相关文章
    </h3>
    <div class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-markov-process/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1667968579"
  data-df="YYYY/MM/DD"
  
>
  2022/11/09
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（马尔可夫决策过程）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的基本概念和模型，主要包括马尔可夫过程、马尔可夫奖励过程和马尔可夫决策过程。






  1. 强化学习
    
      1.1. 状态空间
        
          1.1.1. 状态
          1.1.2. 观测
        
      
      1.2. 动作空间
      1.3. 策略
    
  
  2. 马尔可夫...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Monte-Carlo/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1669712839"
  data-df="YYYY/MM/DD"
  
>
  2022/11/29
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（蒙特卡洛法）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的 model-free 方法——蒙特卡洛法。






  1. 引言
  2. 蒙特卡洛法
    
      2.1. 大数定律与蒙特卡洛思想
      2.2. 蒙特卡洛基础算法
        
          2.2.1. 蒙特卡洛采样
          2.2.2. 蒙特卡洛价值估计
          2.2.3. 算法流程
        
 ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Temporal-Differences/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1671346759"
  data-df="YYYY/MM/DD"
  
>
  2022/12/18
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（时序差分法）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文首先引入了随机近似理论，然后通过比较动态规划和蒙特卡洛，引出结合二者优势的时序差分法。通过分析可知，时序差分法是随机近似理论的一个特例。随后详细介绍了同轨策略下的时序差分控制（SARSA）、离轨策略下的时序差分控制（Q-Learning）和期望SARSA。最后介绍了基于价值的深度强化学习方法：Deep Q-Network（DQN）。






  1. 引言
  2. 随机近似理论
 ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
    </div>
    <!-- .card-deck -->
  </div>
  <!-- #related-posts -->


      
        
        <!-- Navigation buttons at the bottom of the post. -->

<div class="post-navigation d-flex justify-content-between">
  
    <a
      href="/posts/reinforcement-learning-markov-process/"
      class="btn btn-outline-primary"
      prompt="上一篇"
    >
      <p>强化学习（马尔可夫决策过程）</p>
    </a>
  

  
    <a
      href="/posts/reinforcement-learning-Monte-Carlo/"
      class="btn btn-outline-primary"
      prompt="下一篇"
    >
      <p>强化学习（蒙特卡洛法）</p>
    </a>
  
</div>

      
        
        <!--  The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script src="https://utteranc.es/client.js"
        repo="sirlis/sirlis.github.io"
        issue-term="pathname"
        crossorigin="anonymous"
        async>
</script>

<script type="text/javascript">
  $(function() {
    const origin = "https://utteranc.es";
    const iframe = "iframe.utterances-frame";
    const lightTheme = "github-light";
    const darkTheme = "github-dark";
    let initTheme = lightTheme;

    if ($("html[data-mode=dark]").length > 0
        || ($("html[data-mode]").length == 0
            && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
      initTheme = darkTheme;
    }

    addEventListener("message", (event) => {
      let theme;

      /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */
      if (event.origin === origin) {
        /* page initial */
        theme = initTheme;

      } else if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {
        /* global theme mode changed */
        const mode = event.data.message;
        theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme);

      } else {
        return;
      }

      const message = {
        type: "set-theme",
        theme: theme
      };

      const utterances = document.querySelector(iframe).contentWindow;
      utterances.postMessage(message, origin);
    });

  });
</script>



      
    </div>
  </div>


        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 post-content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>
    </div>

    <!-- The Footer -->

<footer>
  <div class="container px-lg-4">
    <div class="d-flex justify-content-center align-items-center text-muted mx-md-3">
      <p>本站采用 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 主题 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
      </p>

      <p>©
        2025
        <a href="https://github.com/sirlis">sirlis</a>.
        
          <span
            data-bs-toggle="tooltip"
            data-bs-placement="top"
            title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。"
          >保留部分权利。</span>
        
      </p>
    </div>
  </div>
</footer>


    <div id="mask"></div>

    <button id="back-to-top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow">
      <i class="fas fa-angle-up"></i>
    </button>

    
      <div
        id="notification"
        class="toast"
        role="alert"
        aria-live="assertive"
        aria-atomic="true"
        data-bs-animation="true"
        data-bs-autohide="false"
      >
        <div class="toast-header">
          <button
            type="button"
            class="btn-close ms-auto"
            data-bs-dismiss="toast"
            aria-label="Close"
          ></button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="px-2 mb-3">发现新版本的内容。</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            更新
          </button>
        </div>
      </div>
    

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.0/dist/jquery.min.js,npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.7/dayjs.min.js,npm/dayjs@1.11.7/locale/zh.min.js,npm/dayjs@1.11.7/plugin/relativeTime.min.js,npm/dayjs@1.11.7/plugin/localizedFormat.min.js,npm/tocbot@4.21.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '<div class="px-1 px-sm-2 px-lg-4 px-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

