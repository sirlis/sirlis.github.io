<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="zh-CN" 
  
>
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="深度学习基础（概率与统计）" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="本文主要介绍概率与统计的相关知识，包括概率的基本概念，似然函数，全概率，条件概率，贝叶斯公式，信息熵等概念的介绍。" />
<meta property="og:description" content="本文主要介绍概率与统计的相关知识，包括概率的基本概念，似然函数，全概率，条件概率，贝叶斯公式，信息熵等概念的介绍。" />
<link rel="canonical" href="http://localhost:4000/posts/deep-learning-probability-basic/" />
<meta property="og:url" content="http://localhost:4000/posts/deep-learning-probability-basic/" />
<meta property="og:site_name" content="SIRLIS" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-30T10:46:19+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="深度学习基础（概率与统计）" />
<meta name="twitter:site" content="@none" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-10-30T10:46:19+08:00","datePublished":"2020-10-30T10:46:19+08:00","description":"本文主要介绍概率与统计的相关知识，包括概率的基本概念，似然函数，全概率，条件概率，贝叶斯公式，信息熵等概念的介绍。","headline":"深度学习基础（概率与统计）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/deep-learning-probability-basic/"},"url":"http://localhost:4000/posts/deep-learning-probability-basic/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>深度学习基础（概率与统计） | SIRLIS
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SIRLIS">
<meta name="application-name" content="SIRLIS">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.0/dist/tocbot.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/head.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <div class="site-title">
      <a href="/">SIRLIS</a>
    </div>
    <div class="site-subtitle fst-italic">分享科研和生活的日常</div>
  </div>
  <!-- .profile-wrapper -->

  <ul class="nav flex-column flex-grow-1 w-100 ps-0">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home"></i>
        <span>首页</span>
      </a>
    </li>
    <!-- the real tabs -->
    
      <li class="nav-item">
        <a href="/categories/" class="nav-link">
          <i class="fa-fw fas fa-stream"></i>
          

          <span>分类</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/tags/" class="nav-link">
          <i class="fa-fw fas fa-tags"></i>
          

          <span>标签</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/archives/" class="nav-link">
          <i class="fa-fw fas fa-archive"></i>
          

          <span>归档</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/about/" class="nav-link">
          <i class="fa-fw fas fa-info-circle"></i>
          

          <span>关于</span>
        </a>
      </li>
      <!-- .nav-item -->
    
  </ul>
  <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/sirlis"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/none"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['lihongjue','buaa.edu.cn'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</div>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container px-xxl-5">
        <!-- The Top Bar -->

<div id="topbar-wrapper">
  <div
    id="topbar"
    class="container d-flex align-items-center justify-content-between h-100"
  >
    <span id="breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                首页
              </a>
            </span>

          
        
          
        
          
            
              <span>深度学习基础（概率与统计）</span>
            

          
        
      
    </span>
    <!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      文章
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="搜索..."
      >
    </span>
    <span id="search-cancel">取消</span>
  </div>
</div>

        











<div class="row">
  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pe-xl-4">
    

    <div class="post px-1 px-md-2">
      

      
        
      
        <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->



  
  

  <!-- CDN URL -->
  

  <!-- Add image path -->
  

  
    
      
      
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  

  
  

  

  
  

  




<!-- return -->




<h1 data-toc-skip>深度学习基础（概率与统计）</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      发表于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1604025979"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2020/10/30
</em>

    </span>

    <!-- lastmod date -->
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      作者

      <em>
      
        <a href="https://github.com/sirlis">sirlis</a>
      
      </em>
    </span>

    <div>
      <!-- read time -->
      <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="11478 字"
>
  <em>63 分钟</em>阅读</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <p>本文主要介绍概率与统计的相关知识，包括概率的基本概念，似然函数，全概率，条件概率，贝叶斯公式，信息熵等概念的介绍。</p>

<!--more-->

<hr />
<ul>
  <li><a href="#1-基本概念">1. 基本概念</a>
    <ul>
      <li><a href="#11-概率定义">1.1. 概率定义</a></li>
      <li><a href="#12-随机变量">1.2. 随机变量</a></li>
      <li><a href="#13-概率分布与概率密度">1.3. 概率分布与概率密度</a></li>
      <li><a href="#14-概率和统计">1.4. 概率和统计</a></li>
      <li><a href="#15-概率函数与似然函数">1.5. 概率函数与似然函数</a></li>
      <li><a href="#16-极大似然估计">1.6. 极大似然估计</a></li>
    </ul>
  </li>
  <li><a href="#2-概率">2. 概率</a>
    <ul>
      <li><a href="#21-条件概率公式">2.1. 条件概率公式</a></li>
      <li><a href="#22-全概率公式">2.2. 全概率公式</a></li>
      <li><a href="#23-概率的两大学派">2.3. 概率的两大学派</a></li>
      <li><a href="#24-贝叶斯公式">2.4. 贝叶斯公式</a></li>
    </ul>
  </li>
  <li><a href="#3-熵">3. 熵</a>
    <ul>
      <li><a href="#31-自信息">3.1. 自信息</a></li>
      <li><a href="#32-信息熵">3.2. 信息熵</a></li>
      <li><a href="#33-相对熵kl散度">3.3. 相对熵（KL散度）</a></li>
      <li><a href="#34-交叉熵">3.4. 交叉熵</a></li>
      <li><a href="#35-softmax-函数">3.5. softmax 函数</a></li>
    </ul>
  </li>
  <li><a href="#4-参考文献">4. 参考文献</a></li>
</ul>

<h1 id="1-基本概念">1. 基本概念</h1>

<h2 id="11-概率定义"><span class="me-2">1.1. 概率定义</span><a href="#11-概率定义" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p><strong>条件概率</strong>：$P(A\vert B)$ 在某条件下事件发生的概率。</p>

<p><strong>先验概率</strong>：指根据以往经验和分析得到的概率，如全概率公式，它往往作为”由因求果”问题中的”因”出现的概率。</p>

<p><strong>后验概率</strong>：已知原分布，在实际发生某事件时,是原先某情况的可能性。后验概率是信息理论的基本概念之一。后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的”果”。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础。</p>

<p>事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。</p>

<p><strong>后验概率是一种条件概率</strong>。一种解释认为，条件概率是个数学名称，后验概率是建模的时候赋予了一定的意义。一般的条件概率，条件和事件可以是任意的；对于后验概率，它限定了事件为隐变量取值，而条件为观测结果。</p>

<p><strong>联合概率</strong>：$P(AB)$，表示两个事件共同发生的概率。</p>

<p><strong>边缘概率</strong>：是某个事件发生的概率，而与其它事件无关。在联合概率中，把最终结果中不需要的那些事件合并成其事件的全概率而消失（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率）。这称为边缘化（marginalization）。$A$ 的边缘概率表示为 $P(A)$，$B$ 的边缘概率表示为 $P(B)$。</p>

<p>需要注意的是，在这些定义中 $A$ 与 $B$ 之间不一定有因果或者时间顺序关系。$A$ 可能会先于 $B$ 发生，也可能相反，也可能二者同时发生。$A$ 可能会导致 $B$ 的发生，也可能相反，也可能二者之间根本就没有因果关系。</p>

<h2 id="12-随机变量"><span class="me-2">1.2. 随机变量</span><a href="#12-随机变量" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>产品经理马忠信. <a href="https://www.jianshu.com/p/b570b1ba92bb">应该如何理解概率分布函数和概率密度函数？</a></p>

<blockquote>
  <p>微积分是研究<strong>变量</strong>的数学，概率论与数理统计是研究<strong>随机变量</strong>的数学。</p>
</blockquote>

<blockquote>
  <p>研究一个随机变量，不只是要看它能取哪些值，更重要的是它取各种值的<strong>概率</strong>如何。</p>
</blockquote>

<p>随机变量（random variable）表示随机试验各种结果的实值单值函数。随机事件不论与数量是否直接有关，都可以数量化，即都能用数量化的方式表达。</p>

<p>随机事件数量化的好处是可以用数学分析的方法来研究随机现象。例如某一时间内公共汽车站等车乘客人数，电话交换台在一定时间内收到的呼叫次数，灯泡的寿命等等，都是随机变量的实例。</p>

<p>按照随机变量可能取得的值，可以把它们分为两种基本类型：</p>

<ul>
  <li><strong>离散型随机变量</strong>：随机变量的值可以逐个列举。
    <ul>
      <li>离散型随机变量通常依据概率质量函数分类，主要分为：伯努利随机变量、二项随机变量、几何随机变量和泊松随机变量。</li>
      <li>离散型随机变量的期望为 $\mathbb E[X] = \sum_{x:p(x)&gt;0}xp(x)$。</li>
    </ul>
  </li>
  <li><strong>连续型随机变量</strong>：随机变量的取值无法逐个列举。
    <ul>
      <li>有几个重要的连续随机变量常常出现在概率论中，如：均匀随机变量、指数随机变量、伽马随机变量和正态随机变量。</li>
      <li>连续型随机变量的期望为 $\mathbb E[X] = \int_{-\infty}^{+\infty}xf(x)dx$（一阶矩）。「见后文推导」</li>
    </ul>
  </li>
</ul>

<h2 id="13-概率分布与概率密度"><span class="me-2">1.3. 概率分布与概率密度</span><a href="#13-概率分布与概率密度" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<blockquote>
  <p>对于离散型随机变量，其概率可以用概率分布列表来描述。如两个离散型随机变量 $x$ 的概率分布列表可以表述为
\(\begin{bmatrix}
x\\ 
P(x)
\end{bmatrix} = \begin{bmatrix}
x_1 &amp; x_2\\ 
0.99 &amp; 0.01
\end{bmatrix}\)
解读为：$x$ 取值为 $x_1$ 的概率 $P(x=x_1) = 0.99$，取值为 $x_2$ 的概率 $P(x=x_2) = 0.01$。</p>
</blockquote>

<p><strong>概率分布函数</strong>（累积概率函数）是描述随机变量取值分布规律的数学表示。概率分布函数是随机变量特性的表征，它决定了随机变量取值的分布规律，只要已知了概率分布函数，就可以算出随机变量落于某处的概率。</p>

<p><strong>对于离散型随机变量，概率分布函数定义为</strong></p>

\[F(x) = P(x\leq x_i)=\sum_{x\leq x_i} P(x_i)\]

<p>对于连续型随机变量，设变量 $x$ 的取值区间为 $(a,b)$，其概率分布函数为</p>

\[F(x) = P(a&lt;x&lt;b) = F(b) - F(a)\]

<p>引入 “<strong>概率密度函数</strong> $f(x)$” 的概念，定义为概率分布函数的导数，是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。即</p>

\[f(x) = F'(x) = \mathop{lim}\limits_{\Delta x\rightarrow 0}\frac{F(x+\Delta x) - F(x)}{\Delta x}\]

<p>概率密度函数的性质有</p>

<ul>
  <li>$f(x) \geq 0$</li>
  <li>$\int_{-\infty}^{+\infty} f(x)dx=1$</li>
</ul>

<p>随机变量的取值落在某个区域之内的概率则为概率密度函数在这个区域上的积分。当概率密度函数存在的时候，概率分布函数是概率密度函数的积分。</p>

<p><strong>对于连续型随机变量，其概率分布函数为</strong></p>

\[F(x) = \int_{-\infty}^{x_i} f(x)dx\]

<p>那么前面连续型随机变量 $x\in(a,b)$ 的概率分布函数为</p>

\[F(x) = P(a&lt;x&lt;b) = F(b) - F(a) =\int_{a}^{b} f(x)dx\]

<p>连续型随机变量 $X$ 的<strong>期望</strong> $\mathbb E(X)$ 为</p>

\[\mathbb E(X) = \int_{-\infty}^{+\infty}xf(x)dx\]

<p>可以通过将概率密度函数划分为 $n$ 个小区间，对每个小区间求概率分布表，当小区间个数 $n \rightarrow +\infty$ 时，$max(\Delta x_i)\rightarrow 0$，借助对离散型随机变量的期望求极限，得到上述定义式。即</p>

\[\mathbb E(X) = \mathop{lim}\limits_{n\rightarrow+\infty}\sum_{i=1}^n x_if(x_i)\Delta x_i =  \int_{-\infty}^{+\infty}xf(x)dx\]

<h2 id="14-概率和统计"><span class="me-2">1.4. 概率和统计</span><a href="#14-概率和统计" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>概率（probabilty）和统计（statistics）看似两个相近的概念，其实研究的问题刚好相反。</p>

<p>概率研究的问题是，已知一个模型和参数，怎么去预测这个模型产生的结果的特性（例如均值，方差，协方差等等）。 举个例子，我想研究怎么养猪（模型是猪），我选好了想养的品种、喂养方式、猪棚的设计等等（选择参数），我想知道我养出来的猪大概能有多肥，肉质怎么样（预测结果）。</p>

<p>统计研究的问题则相反。统计是，有一堆数据，要利用这堆数据去预测模型和参数。仍以猪为例。现在我买到了一堆肉，通过观察和判断，我确定这是猪肉（这就确定了模型。在实际研究中，也是通过观察数据推测模型是／像高斯分布的、指数分布的、拉普拉斯分布的等等），然后，可以进一步研究，判定这猪的品种、这是圈养猪还是跑山猪还是网易猪，等等（推测模型参数）。</p>

<p>一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。</p>

<p>统计领域有两个基本概念：<strong>极大似然估计</strong> 和 <strong>极大后验概率估计</strong>。它们都是用来推测参数的方法。为什么会存在着两种不同方法呢？ 这需要理解概率的两个学派。（很奇怪的一点在于，两个估计是属于统计领域，但是确需要理解概率的两个学派）</p>

<h2 id="15-概率函数与似然函数"><span class="me-2">1.5. 概率函数与似然函数</span><a href="#15-概率函数与似然函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>似然（likelihood）这个词其实和概率（probability）是差不多的意思，Colins字典这么解释：The likelihood of something happening is how likely it is to happen. 你把likelihood换成probability，这解释也读得通。但是在统计里面，似然函数和概率函数却是两个不同的概念（其实也很相近就是了）。</p>

<p>对于 $P(x\vert \theta)$，输入有2个：$x$ 表示某一具体的数据，$\theta$ 表示模型参数。</p>

<ul>
  <li>
    <p>如果 $\theta$ 是已知的，$x$ 是变量，这个函数叫做<strong>概率函数</strong>，它描述对于不同样本点 $x$，其出现的概率是多少。</p>
  </li>
  <li>
    <p>如果 $x$ 是已知的，$\theta$ 是变量，这个函数叫做<strong>似然函数</strong>，它描述对于不同的模型参数，出现 $x$ 这个样本点的概率是多少。</p>
  </li>
</ul>

<p>大概直观的理解一下，假设某组数据 $x$ 服从正态分布，其概率密度函数为</p>

\[f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]

<blockquote>
  <p>说到这里，插一个啼笑皆非的事情，在推免研究生面试时，一帮教授大佬们突然恶趣味想出这么一个面试题 “高斯分布和正态分布是什么关系？”，答案是：“它们是一个东西”。结果在面试期间，大佬们真的对好几个学生都问了这个问题。尴尬的是，只有一个学生犹犹豫豫的回复说：“它们好象是一个东西把…”</p>
</blockquote>

<ul>
  <li>如果我们已知分布的参数 $\theta = [\mu,\sigma]$ 的具体取值，那么 $P(x\vert \theta)$ 就是取到数据 $x$ 的概率。</li>
  <li>如果我们不知道分布的参数，而是通过一次采样得到数据 $x$，那么 $P(x\vert \theta)$ 描述的就是对于不同的模型参数，出现数据 $x$ 这个样本点的概率。</li>
</ul>

<p>注意到，上面两种不同的解读与概率的两个学派紧密相联。</p>

<h2 id="16-极大似然估计"><span class="me-2">1.6. 极大似然估计</span><a href="#16-极大似然估计" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<blockquote>
  <p>忆臻. <a href="https://zhuanlan.zhihu.com/p/26614750">一文搞懂极大似然估计</a>
拓季. <a href="https://www.jianshu.com/p/191c029ad369">交叉熵与最大似然估计</a>
贺勇，明杰秀编著．概率论与数理统计．武汉：武汉大学出版社，2012.08：216-217</p>
</blockquote>

<p>极大似然估计（Maximum likelihood estimation, MLE），是建立在极大似然原理的基础上的一个统计方法。通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。</p>

<p>在现实世界的研究中，我们可以首先假定某些被研究对象的概率分布服从某种分布形态，例如二项分布、正态分布、指数分布，但对于相应分布中的具体参数的取值却无从知晓，此时我们可以借助从总体中抽样的方式，对总体的概率分布的参数进行一个点对点的估计。</p>

<blockquote>
  <p>一种直观的理解：设甲箱中有 99 个白球，1 个黑球；乙箱中有 1 个白球．99 个黑球。现随机取出一箱，再从抽取的一箱中随机取出一球。假设取出的结果是黑球，由于这一黑球从乙箱抽取的概率比从甲箱抽取的概率大得多，这时我们自然更多地相信这个黑球是取自乙箱的。</p>
</blockquote>

<p>极大似然估计中采样需满足一个重要的假设，就是抽样样本集 $X$ 当中的样本 $x_1,x_2,…,x_n$ 彼此<strong>独立同分布</strong>。一般说来，事件 $x_i$ 发生的概率与未知的模型参数 $\theta$ 有关，参数 $\theta$ 的取值不同，则事件 $x_i$ 发生的概率 $P(x_i\vert \theta)$ 也不同。当我们在一次试验中事件 $x_i$ 发生了，则认为此时的 $\theta$ 值应是的一切可能取值中使 $P(x_i\vert \theta)$ 达到最大的那一个，极大似然估计法就是要选取这样的值作为参数 $\theta$ 的估计值，使所选取的样本在被选的总体中出现的可能性为最大。</p>

<p>对于样本集 $X$，由于样本之间彼此独立且服从参数为 $\theta$ 的分布，那么取得任意一个样本 $x_i$ 的概率可以表示为 $P(x_i \vert \theta)$，因而取得当前这个样本集的概率为：</p>

\[P(X) = P(x_1\vert \theta)P(x_2\vert \theta)...P(x_n\vert \theta)\]

<p>由于在抽样后会得到总体的一个观测，因此相应的概率在具体的样本集中都是已知的，上面这个函数则可以看作在不同的参数 $\theta$ 的取值下，取得当前这个样本集的概率，也即可能性（likelihood），因此将其称为参数 $\theta$ 相对于样本集 $X$ 的<strong>似然函数（likelihood function）</strong>，记为 $L(\theta)$，即有</p>

\[\begin{aligned}
L(\theta) &amp; = L(x_1,x_2,...,x_n\vert \theta)\\
&amp;= P(x_1\vert \theta)P(x_2\vert \theta)...P(x_n\vert \theta)\\
&amp;= \prod P(x_i\vert \theta), i=1,2,...,n
\end{aligned}\]

<p><strong>对于抽样得到样本集 $X$ 这个既成事实，我们认为这就是最该发生的结果，也即所有可能的结果中概率最大的那一个，这就是最大似然估计的命名由来</strong>。此时，我们需要寻找的就是那个使似然函数取得最大值的 $\theta$ 值</p>

\[{\rm argmax}\ L(\theta) = {\rm max}_\theta L(\theta)\]

<p>${\rm argmax} f(x)$ 是使得函数 $f(x)$ 取得其最大值的所有自变量 $x$ 的集合。</p>

<p>总结起来，利用最大似然函数求解总体参数的估计值的一般步骤为：</p>

<ul>
  <li>获取似然函数</li>
  <li>对似然函数取自然对数</li>
  <li>将对数似然函数求（偏）导数，令其为 0，得到似然方程</li>
  <li>求解似然方程，得到的一个或多个数值，即为总体参数的最大似然估计值</li>
</ul>

<hr />

<p>由于在计算当中，多个概率的乘法最终会得到一个非常小的值，从而可能造成下溢（underflow），因此一般会对似然函数取一个对数，将连续乘法转化为加法</p>

\[{\rm argmax}\ log L(\theta) = {\rm argmax}\  log \prod P(x_i\vert \theta) = {\rm argmax}\  \sum_{i=1}^n logP(x_i\vert \theta), i=1,2,...,n\]

<p>再进一步，加一个负号，问题转化为</p>

\[{\rm argmin}\  [-log L(\theta)] = {\rm argmin}\  \sum_{i=1}^n [-logP(x_i\vert \theta)], i=1,2,...,n\]

<p>因为当重新缩放代价函数时 argmin 不会改变，我们可以除以 $n$ 得到和训练数据经验分布 $P$ 相关的期望作为准则</p>

\[{\rm argmin}\ \mathbb E_P [-logP(X\vert \theta)]\]

<p>令 $Q(X) = P(X\vert \theta)$ 上式转化为</p>

\[{\rm argmin}\ \mathbb E_P [-log Q(X)]\]

<p>后面在介绍 KL 散度和交叉熵时会发现，<strong>任何一个负对数似然组成的损失都是定义在训练集上的经验分布和定义在模型上的真实分布之间的交叉熵。</strong></p>

<hr />

<p><strong>举一个例子</strong>：假定被研究对象的总体服从二项分布，如果知道了基础的 (0，1) 分布中结果指定为 1 时的概率 $p$，也就知道了这个分布形态的全部。</p>

<blockquote>
  <p>假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？</p>

  <p>很多人马上就有答案了：<strong>70%</strong>。而其后的理论支撑是什么呢？</p>

  <p>我们假设罐中白球的比例是 $p$，那么黑球的比例就是 $1-p$，这里 $p$ 就是模型参数。因为每抽一个球出来，在记录颜色之后，我们把抽出的球放回了罐中并摇匀，所以每次抽出来的球的颜色服从同一独立分布。</p>

  <p>这里我们把一次抽出来球的颜色称为一次抽样。题目中在一百次抽样中，七十次是白球的,三十次为黑球事件的概率是：
\(P(样本结果|Model)\)
如果第一次抽象的结果记为 $x_1$，第二次抽样的结果记为 $x_2$…，那么样本结果为 $(x_1,x_2,…,x_{100})$。这样，我们可以得到如下表达式：
\(\begin{aligned}
P(样本结果|Model)&amp;= P(x_1,x_2,…,x_{100}|Model)\\
&amp;= P(x_1|M)P(x_2|M)\cdots P(x_{100}|M)\\
&amp;= p^{70}(1-p)^{30}
\end{aligned}\)
上面就是观察样本结果出现的概率函数了。那么，如何求出模型参数 $p$ 呢？不同的 $p$ 会直接导致上面概率函数的不同。比如:</p>
  <ul>
    <li>$p=0.5$（黑50%白50%）时 $p^{70}(1-p)^{30}=7.8\times 10^{-31}$</li>
    <li>$p=0.7$ （黑70%白30%）时 $p^{70}(1-p)^{30}=2.95\times 10^{-27}$</li>
  </ul>

  <div class="table-wrapper"><table>
    <tbody>
      <tr>
        <td>那么问题来了，既然有无数种分布可以选择，极大似然估计应该按照什么原则去选取这个分布呢？答案是，<strong>让这个样本结果出现的可能性最大</strong>，既然事情已经发生了，为什么不让这个出现的结果的可能性最大呢？这也就是最大似然估计的核心。转化为数学问题，就是使得 $P(样本结果</td>
        <td>Model)=p^{70}(1-p)^{30}$ 的值最大。</td>
      </tr>
    </tbody>
  </table></div>

  <p>如何求极值呢，那么我们就可以将概率函数看成是 $p$ 的方程，对其<strong>求导</strong>，导数为 0 的点就是极值点！
\(\begin{aligned}
f'(p) &amp;= {\rm d}(p^{70}(1-p)^{30})/{\rm d}p\\
&amp;= 70\cdot p^{69}(1-p)^{30}-p^{70}\cdot 30\cdot(1-p)^{29}\\
&amp;= 70\cdot p^{69}\cdot(1-p)^{29}[1-p-3/7\cdot p] = 0\\
\end{aligned}\)
即
\(\begin{aligned}
1 - 10/7 \cdot p = 0
\end{aligned}\)
可求出 $p=0.7$，与我们一开始认为的 70% 是一致的。</p>
</blockquote>

<p><strong>再举另外一个例子</strong></p>

<blockquote>
  <p>假设我们要统计全国人民的年均收入，首先假设这个收入服从服从正态分布，但是该分布的均值与方差未知。我们没有人力与物力去统计全国每个人的收入。我们国家有10几亿人口呢？那么岂不是没有办法了？</p>

  <p>不不不，有了极大似然估计之后，我们可以采用嘛！我们比如选取一个城市，或者一个乡镇的人口收入，作为我们的观察样本结果。然后通过最大似然估计来获取上述假设中的正态分布的参数。</p>

  <p>有了参数的结果后，我们就可以知道该正态分布的期望和方差了。也就是我们通过了一个小样本的采样，反过来知道了全国人民年收入的一系列重要的数学指标量！</p>

  <p>那么我们就知道了极大似然估计的核心关键就是对于一些情况，样本太多，无法得出分布的参数值，可以采样小样本后，利用极大似然估计获取假设中分布的参数值。</p>
</blockquote>

<h1 id="2-概率">2. 概率</h1>

<h2 id="21-条件概率公式"><span class="me-2">2.1. 条件概率公式</span><a href="#21-条件概率公式" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>条件概率是指事件 $A$ 在事件 $B$ 发生的条件下发生的概率。</p>

<p>若只有两个事件 $A$，$B$，那么条件概率表示为：$P(A\vert B)$，读作 “$A$ 在 $B$ 发生的条件下发生的概率”。</p>

\[P(A\vert B) = \frac{P(AB)}{P(B)}\]

<p>其中，$P(AB)$ 是两个事件的联合概率，$P(B)$ 是事件 $B$ 的边缘概率。</p>

<p><strong>一种不准确的理解方式</strong>，$P(AB)$ 是 $A,B$ 同时发生的概率，$P(B)$ 是 $B$ 发生的概率。我们要求已知 $B$ 发生后 $A$ 发生的概率，相当于 B 的发生与否不再是一个不确定的概率，而是确定的条件。由于不同事件同时发生的概率是用乘积的形式来表示，反之某个事件从概率变为已经发生就用除法来表示，那么就应该用 $P(AB)$ 除以 $P(B)$ 来得到这么个条件概率。</p>

<p><strong>另一种理解方式</strong>，将等式做一个变换</p>

\[P(B)P(A\vert B) = P(AB)\]

<p>也就是说，$A,B$ 同时发生的概率，等于 $B$ 发生的概率，乘以 $B$ 发生（作为条件）后 $A$ 发生的概率。</p>

<p>这给我们一个启示，即交换 $A,B$ 的顺序，等式依然成立</p>

\[P(B)P(A\vert B) = P(AB) = P(BA) = P(A)P(B\vert A)\]

<h2 id="22-全概率公式"><span class="me-2">2.2. 全概率公式</span><a href="#22-全概率公式" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>设 $B_1,…,A_n$ 是样本空间 $S$ 的一个完备事件组，即</p>

<ul>
  <li>$B_1,…,B_n$ 两两不相容：$B_i \cap B_j = \varnothing\quad (i\neq j)$</li>
  <li>$B_i \cup…\cup B_n = S$</li>
</ul>

<p>每一次试验中，完备事件组中有且仅有一个事件发生。完备事件组构成样本空间的一个划分。</p>

<p><strong>全概率公式</strong>。定理：设实验 $E$ 的样本空间为 $S$，$B_1, B_2,…,B_n$ 为 $S$ 的一个划分（完备事件组），且 $P(B_i)&gt;0\quad i=1,2,…n$，$A$ 为 $E$ 的一个事件，则</p>

\[\begin{aligned}
P(A) &amp;= P(B_1)P(A\vert B_1)+P(B_2)P(A\vert B_2)+...+P(B_n)P(A\vert B_n)\\
&amp;= \sum_{i=1}^n P(B_i)P(A\vert B_i)
\end{aligned}\]

<p>其推导过程如下，如图：</p>

<p><a href="/assets/img/postsimg/20201030/1.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20201030/1.jpg" alt="full probability" class="lazyload" data-proofer-ignore></a></p>

\[\begin{aligned}
A &amp;= AS = A(B_1\cup B_2\cup ... \cup B_n)\\
&amp;= AB_1\cup AB_2\cup ... \cup AB_n \quad (AB_i两两互斥)\\
P(A) &amp;= P(AB_1\cup AB_2\cup ... \cup AB_n)\\
&amp;=P(AB_1) + P(AB_2) + ... + P(AB_n)
\end{aligned}\]

<p>根据条件概率公式</p>

\[P(AB_i) = P(A)P(B_i\vert A) = P(B_i)P(A\vert B_i)\]

<p>带入有</p>

\[P(A) = \sum_{i=1}^n P(B_i)P(A\vert B_i)\]

<p>即为全概率公式。</p>

<p><strong>全概率公式的意义1</strong>：将复杂的事件 $A$ 划分为比较简单的事件 $AB_1,…,AB_n$，再结合加法和乘法计算 $A$ 的（边缘）概率。</p>

<p><strong>全概率公式的意义2</strong>：事件 $A$ 的发生可能有各种原因 $B_i\quad (i=1,2,…,n)$，如果 $A$ 是由 $B_i$ 引起，则此时 $A$ 发生的（条件）概率为</p>

\[P(AB_i) = P(B_i)P(A\vert B_i)\]

<p>若每个原因都可能导致 $A$ 的发生，那么 $A$ 发生的概率是全部原因引起其发生的概率的综合，即为全概率公式。</p>

<p>因此可以把全概率公式看成是 “<strong>由原因推结果</strong>”。每一个原因对结果的发生由一定的作用，结果发生的可能性与各种原因的作用大小有关，全概率公式表达了它们之间的关系。</p>

<h2 id="23-概率的两大学派"><span class="me-2">2.3. 概率的两大学派</span><a href="#23-概率的两大学派" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>对于概率看法不同的两大派别频率学派与贝叶斯派。他们看待世界的视角不同，导致他们对于产生数据的模型参数的理解也不同。</p>

<ul>
  <li>
    <p><strong>频率学派</strong></p>

    <ul>
      <li>他们认为世界是确定的。他们直接为事件本身建模，也就是说事件在多次重复实验中趋于一个稳定的值 $p$，那么这个值就是该事件的概率。</li>
      <li>他们认为模型参数是个定值，希望通过类似解方程组的方式从数据中求得该未知数。这就是频率学派使用的参数估计方法：<strong>极大似然估计</strong>（MLE），这种方法往往在大数据量的情况下可以很好的还原模型的真实情况。</li>
      <li>频率派把需要推断的模型参数 $\theta$ 看做是固定的未知常数，即参数 $\theta$ 虽然是未知的，但最起码是确定的一个值，同时，样本 $X$ 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本 $X$ 的分布；</li>
    </ul>
  </li>
  <li>
    <p><strong>贝叶斯派</strong></p>

    <ul>
      <li>他们认为世界是不确定的，因获取的信息不同而异。假设对世界先有一个预先的估计，然后通过获取的信息来不断调整之前的预估计。 他们不试图对事件本身进行建模，而是从旁观者的角度来说。因此对于同一个事件，不同的人掌握的先验不同的话，那么他们所认为的事件状态也会不同。</li>
      <li>他们认为模型参数源自某种潜在分布，希望从数据中推知该分布。对于数据的观测方式不同或者假设不同，那么推知的该参数也会因此而存在差异。这就是贝叶斯派视角下用来估计参数的常用方法：<strong>最大后验概率估计</strong>（MAP），这种方法在先验假设比较靠谱的情况下效果显著，随着数据量的增加，先验假设对于模型参数的主导作用会逐渐削弱，相反真实的数据样例会大大占据有利地位。极端情况下，比如把先验假设去掉，或者假设先验满足均匀分布的话，那她和极大似然估计就如出一辙了。</li>
      <li>贝叶斯学派认为，模型参数 $\theta$ 是随机变量，而样本 $X$ 是固定的，由于样本是固定的，所以他们重点研究的是参数 $\theta$ 的分布。</li>
    </ul>
  </li>
</ul>

<p>贝叶斯学派的创始人，托马斯·贝叶斯（Thomas Bayes），发表了一篇名为《An essay towards solving a problem in the doctrine of chances》，翻译过来则是《机遇理论中一个问题的解》。这篇论文发表后，在当时并未产生多少影响，在20世纪后，这篇论文才逐渐被人们所重视。</p>

<p>贝叶斯派既然把 $\theta$ 看做是一个随机变量，所以要计算 $\theta$ 的分布，便得事先知道 $\theta$ 的无条件分布，即在有样本之前（或观察到 $X$ 之前）， $\theta$ 有着怎样的分布呢？</p>

<p>比如往台球桌上扔一个球，这个球落会落在何处？如果是不偏不倚的把球抛出去，那么此球落在台球桌上的任一位置都有着相同的机会，即球落在台球桌上某一位置的概率服从均匀分布。这种在实验之前定下的属于基本前提性质的分布称为先验分布，或的无条件分布。</p>

<p>至此，贝叶斯及贝叶斯派提出了一个思考问题的固定模式：</p>

\[先验分布 \pi(\theta) + 样本信息x \Rightarrow 后验分布 \pi(\theta \vert x)\]

<h2 id="24-贝叶斯公式"><span class="me-2">2.4. 贝叶斯公式</span><a href="#24-贝叶斯公式" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<ul>
  <li><strong>一种形式</strong></li>
</ul>

<p>设 $B_1,…,B_n$ 是样本空间 $S$ 的一个完备事件组，则对任一事件 $A$，$P(A)&gt;0$，有</p>

\[P(B_i\vert A) = \frac{P(B_i)P(A\vert B_i)}{P(A)}=\frac{P(B_i)P(A\vert B_i)}{\sum_{j=1}^n P(B_j)P(A\vert B_j)}\]

<p>贝叶斯公式的推导可以通过条件概率公式得到</p>

\[\begin{aligned}
P(A\vert B_i) &amp;= \frac{P(AB_i)}{P(B_i)} \quad &amp;(条件概率公式)\\
\Rightarrow  P(AB_i) &amp;= P(B_i)P(A\vert B_i) \quad &amp;(移项)\\
\Rightarrow  P(A)P(B_i\vert A) &amp;= P(B_i)P(A\vert B_i) \quad &amp;(条件概率公式)\\
\Rightarrow  P(B_i\vert A) &amp;= \frac{P(B_i)P(A\vert B_i)}{P(A)} \quad &amp;(移项)
\end{aligned}\]

<p>一种直观的理解：假设 $B_1,…,B_n$ 是发生时间 $A$ 的各种原因，那么已知事件 $A$ 已经发生了，问是原因 $B_i$ 导致的概率 $P(B_i \vert A)$ 是多少？</p>

<ul>
  <li><strong>另一种形式</strong></li>
</ul>

<p>使用另一套字母体系：</p>
<ul>
  <li>$H$：hypothesis，假设，规律</li>
  <li>$E$：evidence，证据，现象，数据</li>
</ul>

<p>那么贝叶斯的推理过程可以表达为：通过不断收集证据 $E$ 来完善假设 $H$ 。那么贝叶斯公式可以描述为</p>

\[P(H\vert E) = \frac{P(H)P(E\vert H)}{P(E)} = P(H)\frac{P(E\vert H)}{P(E)}\]

<p>因此贝叶斯公式实际上阐述的是如下事实</p>

\[新信息E出现后假设H的概率 = H 的概率\times 新信息带来的调整\]

<p>可以理解为，新观察到的样本信息将修正人们以前对事物的认知。</p>

<p>对于一个场景，可能有几种不同的规律来解释，根据对场景的一些现象的观测，如何知道各种可能的规律在背后发生作用的概率？这个问题就是如何求解 $P(规律H\vert现象E)$。</p>

<div class="table-wrapper"><table>
  <tbody>
    <tr>
      <td>直接去计算有难度，但是如果我们知道在某个规律 $H$ 下，不同现象 $E$ 发生的概率 $P(现象E</td>
      <td>规律H)$，和每个规律发生的概率 $P(规律H)$，和不同现象发生的概率 $P(现象E)$，就可以通过已知数据求解。贝叶斯就是告诉怎么用这些知道的知识去<strong>计算现象后面的规律发生的概率</strong>。</td>
    </tr>
  </tbody>
</table></div>

<ul>
  <li><strong>第三种形式</strong></li>
</ul>

<p>马同学. <a href="https://www.matongxue.com/madocs/910">如何理解贝叶斯推断和beta分布？</a></p>

<p>假设实验数据 $X\vert p$ 服从二项分布（比如抛硬币）</p>

\[\underbrace{f(p\vert X=k)}_{后验分布}=\frac{\overbrace{P(X=k\vert p)}^{实验数据}\overbrace{f(p)}^{先验分布}}{\underbrace{P(X=k)}_{常数}}\]

<p>其中 $k$ 为正面的次数。分母与实验数据无关，可以视作常数。</p>

<h1 id="3-熵">3. 熵</h1>

<p>《信息论：基础理论与应用》</p>

<h2 id="31-自信息"><span class="me-2">3.1. 自信息</span><a href="#31-自信息" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>在信息传输的一般情况下，收信者获得的信息量等于信息传输前后不确定性的减少量</p>

\[获得的信息量 = 不确定性的减少量 = 收信前某事件发生的不确定性 - 收信后某事件发生的不确定性\]

<p>假设无噪声，信道传输不失真，因此收到消息后某事件发生的不确定性完全消除，即</p>

\[获得的信息量 = 不确定性的减少量 = 收信前某事件发生的不确定性 = 信源输出某消息中含有的信息量\]

<p>事件发生的不确定性与事件发生的概率有关。事件发生的概率越大，该事件发生后能够提供的信息就越少，不确定性就越小。对于发生概率等于 1 的事件（必然事件），提供不了任何信息，就不存在不确定性。因此，某事件发生所含有的信息量就应该是该事件发生先验概率的<strong>函数</strong>。</p>

\[I(a_i) = f(P(a_i))\]

<p>式中，$P(a_i)$ 是事件 $a_i$ 发生的先验概率，$I(a_i)$ 表示事件 $a_i$ 发生所含有的信息量，我们称之为 $a_i$ 的<strong>自信息</strong>。</p>

<p>下面分析自信息与先验概率之间函数关系 $f$ 的具体形式。</p>

<blockquote>
  <p>一种形象理解：事件发生概率越大，发生后能够提供的信息就越少（老发生没啥价值，获得不了什么信息），那么不确定性就越小，自信息就小。反之，如果事件发生概率越小，一旦发生了提供的信息量就很大，发生后消除的不确定性就很大，自信息量就大。因此，概率和信息量可能是负相关的。</p>
</blockquote>

<p>根据客观事实和人们的习惯概念，不确定性函数 $f$ 应该满足如下的条件：</p>

<ul>
  <li><strong>减函数</strong>：$f$ 是 $P$ 的单调递减函数，
    <ul>
      <li>$P(a_i)=1$ 时 $f(P_i)=0$，</li>
      <li>$P(a_i)=0$ 时 $f(P_i)=\infty$</li>
    </ul>
  </li>
  <li><strong>可加性</strong>：两个独立消息所产生的不确定性应等于各自不确定性之和，$f(P_1,P_2)=f(P_1)+f(P_2)$</li>
  <li>由于事件 $P_1,P_2$ 是独立不相关的，因此 $P(1,2)=P_1P_2$。</li>
</ul>

<p>同时满足这 3 个条件的函数 $f$ 是<strong>负对数函数</strong>，即</p>

\[f(P) = {\rm log}\frac{1}{P} = -{\rm log}P\quad P \in [0,1]\]

<p>函数图像如下图所示</p>

<p><a href="/assets/img/postsimg/20201030/2.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20201030/2.jpg" alt="-log" class="lazyload" data-proofer-ignore></a></p>

<p><strong>事件发生前，$f$ 表示事件发生的不确定性；事件发生后，$f$ 表示事件所含有（所提供）的信息量</strong>。自信息采用的单位取决于对数所选的底。由于概率 $P$（作为自变量） 是小于 1 的正数，又根据实际情况自信息也必然是正数，所以对数的底应选取为大于 1 的任意数。</p>

<p>信息论中一般取 2 为底，则所得的信息量单位为 <strong>比特</strong>（bit，binary unit）。机器学习中一般取 $e$ 为底的自然对数，信息量单位为 <strong>奈特</strong>（nat, nature unit）。如果取 10为底，信息量单位为 <strong>哈特</strong>（hart, Hartley 的缩写，纪念他首先提出用对数来度量信息）。</p>

<p>取 2 为底时可以略去不写。如果 $P(a_i)=0.5$，则 $f(P)=1$ 比特。所以 1 比特信息量就是两个互不相容的等可能事件之一发生时提供的信息量。注意，这里的比特时抽象的信息量单位，与计算机术语中的比特含义不同，它代表二元数字（binary digits）。这两种概念之间的关系是，每个二元数字所能提供的最大平均信息量为 1 比特。</p>

<h2 id="32-信息熵"><span class="me-2">3.2. 信息熵</span><a href="#32-信息熵" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>前面定义的自信息是某一信源发出某一消息所含有的信息量。如果发出的消息不同，所含有的信息量也就不同。因此，自信息是一个随机变量，不能用来作为整个信源的信息测度。</p>

<p>我们定义<strong>自信息的数学期望为信源的平均自信息量</strong>。信源一般包括多种信号，如果考虑这个信源所有可能发生情况的平均不确定性，假设信源消息有 $n$ 种取值：$x_1,…,x_i,…,x_n$，对应概率为：$P(x_1),…,P(x_i),…,P(x_n)$，且各种消息的出现彼此独立。这时，信源的平均不确定性应当为单个消息不确定性 $-{\rm log}P(x_i)$ 的数学期望（$\mathbb E$），可称为<strong>信息熵</strong></p>

\[H(X) = \mathbb E[-{\rm log} P(x_i)] = -\sum_{i=1}^n P(x_i){\rm log} P(x_i)\]

<p><strong>信息熵：信源的平均不确定性，为单个消息不确定性的数学期望（统计平均值）</strong>。1948年，C.E.Shannon（香农）提出了 “信息熵” 的概念，才解决了对信息的量化度量问题。信息熵这个词是香农从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。</p>

<p>对于连续分布，信息熵的表示如下（连续型随机变量的数学期望）</p>

\[H(X) = \mathbb E[-{\rm log} P(x)] = -\int_x P(x){\rm log} P(x) {\rm d}x\]

<p>香农用<strong>信息熵的概念来描述信源的平均不确定度，相当于每个消息所提供的平均信息量</strong>。</p>

<p>例如，两个信源分别为</p>

\[\begin{bmatrix}
X\\ 
P(x)
\end{bmatrix} = \begin{bmatrix}
a_1 &amp; a_2\\ 
0.99 &amp; 0.01
\end{bmatrix},\quad \begin{bmatrix}
Y\\ 
P(y)
\end{bmatrix} = \begin{bmatrix}
b_1 &amp; b_2\\ 
0.5 &amp; 0.5
\end{bmatrix}\]

<p>两个信源的信息熵分别为</p>

\[\begin{aligned}
  H(X) &amp;= -0.99{\rm log} 0.99-0.01{\rm log} 0.01 = 0.08 (bit/info)\\
  H(Y) &amp;= -0.5{\rm log} 0.5-0.5{\rm log} 0.5 = 1 (bit/info)
\end{aligned}\]

<p>可见，信源 $Y$ 的平均不确定性大。我们观察信源 $Y$，它的两个输出消息是等可能性的，所以在没有输出前猜测输出哪个消息的不确定性要大。反之，对于信源 $X$，由于事件 $a_1$ 出现的概率远超事件 $a_2$，虽然具有不确定性，但是大致猜测事件 $a_1$ 会出现。</p>

<p>举个例子说明信息熵的作用。</p>

<blockquote>
  <p>赌马比赛，有4匹马 ${ A, B, C, D}$，获胜概率分别为 ${ 1/2, 1/4, 1/8, 1/8 }$，将哪一匹马获胜视为随机变量X属于 ${ A, B, C, D }$ 。
假定我们需要用尽可能少的二元问题来确定随机变量 X 的取值。
例如，问题1：A获胜了吗？　问题2：B获胜了吗？　问题3：C获胜了吗？
最后我们可以通过最多3个二元问题，来确定取值：</p>
  <ul>
    <li>如果X = A，那么需要问1次（问题1：是不是A？），概率为1/2</li>
    <li>如果X = B，那么需要问2次（问题1：是不是A？问题2：是不是B？），概率为1/4</li>
    <li>如果X = C，那么需要问3次（问题1，问题2，问题3），概率为1/8</li>
    <li>如果X = D，那么需要问3次（问题1，问题2，问题3），概率为1/8</li>
  </ul>
</blockquote>

<p>在二进制计算机中，一个比特为 0 或 1，其实就代表了一个二元问题的回答。我们可以使用一个 2 比特的数字来完成上面的赌马结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>00 - A获胜，  01 - B获胜，  10 - C获胜，  11 - D获胜
</pre></td></tr></tbody></table></code></div></div>

<p>然而，我们可以利用非均匀分布这个特点，使用更短的编码来描述更可能的事件，使用更长的编码来描述不太可能的事件。我们希望这样做能够得到一个更短的平均编码长度。我们可以使用下面的编码串（哈夫曼编码）：</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>0 - A获胜,   10 - B获胜,   110 - C获胜,   111 - D获胜
</pre></td></tr></tbody></table></code></div></div>

<p>此时，传输的编码的平均长度就是：</p>

\[1/2\times 1bit + 1/4\times 2bit + 1/8\times 3bit + 1/8\times 3bit = \frac{7}{4}bits = 1.75bits\]

<p>回到信息熵的定义，会发现通过之前的信息熵公式，神奇地得到了</p>

\[H(N) = \frac{1}{2}\cdot {\rm log}(2)+\frac{1}{4}\cdot {\rm log}(4)+\frac{1}{8}\cdot {\rm log}(8)+\frac{1}{8}\cdot {\rm log}(8) =\frac{1}{2}+\frac{2}{4}+\frac{3}{8}+\frac{3}{8} = \frac{7}{4} bits\]

<p>也就是说，在计算机中，我们给哪一匹马夺冠这个事件进行编码，所需要的平均码长为 1.75 个比特。所以，信息熵 $H(X)$ 可以看做<strong>对信源 $X$ 中的样本进行编码所需要的编码长度的期望值</strong>，同时也表明，信息熵是传输一个随机变量状态值所需的比特位下界（最短平均编码长度）。</p>

<h2 id="33-相对熵kl散度"><span class="me-2">3.3. 相对熵（KL散度）</span><a href="#33-相对熵kl散度" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>相对熵，relative entropy = Kullback-Leibler divergence，用于衡量两个概率分布之间的差异。</p>

<p>设 $P(x)$ 和 $Q(x)$ 是离散随机变量 $X$ 中取值的两个概率分布。其中，$P(x)$ 为我们往往不能测得的真实分布，而 $Q(x)$ 是我们可以测得的预测分布。</p>

<p>对于 $P(x)$ 而言，其信息熵为</p>

\[H(P) = -\int P(x){\rm log}P(x)\]

<p>对于 $Q(x)$ 而言，由于其样本来自真实分布 $P(x)$，因此其信息熵中的概率依然为 $P(X)$，则其信息熵为</p>

\[H(P,Q) = -\int P(x){\rm log}Q(x)\]

<p>为了衡量二者的差异，定义 <strong>相对熵</strong> 为</p>

\[\begin{aligned}
KL(P\Vert Q) &amp;= -\int P(x){\rm log}Q(x) - [-\int P(x){\rm log} P(x)]\\
&amp;= - \int P(x){\rm log}\frac{Q(x)}{P(x)}
\end{aligned}\]

<p>相对熵又被称为 <strong>KL 散度</strong>。</p>

<ul>
  <li>
    <p>由于 $P(x)$ 和 $Q(x)$ 在公式中的地位不是相等的，所以 $KL(P\Vert Q)\not\equiv KL(Q\Vert P)$。</p>
  </li>
  <li>
    <p>当，$P(x) = Q(x)$时，$KL(P\Vert Q) = 0$。若 $P(x)$ 和 $Q(x)$ 有差异，$KL(P\Vert Q) &gt; 0$。</p>
  </li>
</ul>

<p>KL 散度的非负性证明，利用了负指数函数是严格凸函数的性质，证明过程如下：</p>

<blockquote>
  <p>首先，$-log()$ 函数是上凹函数，如下面的示意图所示
根据 Jensen 不等式：$f(\mathbb E(x))\leq \mathbb E(f(x))$
根据含隐随机变量的 Jensen 不等式：$f(\mathbb E[\xi(z)])\leq \mathbb E[f(\xi(z))]$
令 $f(\cdot) = -log(\cdot)$，令 $\xi(z) = \frac{Q(x)}{P(x)}$，带入上式得
\(\begin{aligned}
\mathbb E_x[f(\xi(x))] &amp;\geq f(\mathbb E_x[\xi (x)])\\
\mathbb E_x[-log(\frac{Q(x)}{P(x)})] &amp;\geq -log(E_x[\frac{Q(x)}{P(x)}]))\\
-\int log(\frac{Q(x)}{P(x)}) P(x){\rm d}x &amp;\geq -log(\int \frac{Q(x)}{P(x)}P(x){\rm d}x)\\
-\int log(\frac{Q(x)}{P(x)}) P(x){\rm d}x &amp;\geq -log(\int Q(x){\rm d}x)=-log(1)=0\\
\end{aligned}\)</p>
</blockquote>

<h2 id="34-交叉熵"><span class="me-2">3.4. 交叉熵</span><a href="#34-交叉熵" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>遍地胡说. <a href="https://www.cnblogs.com/kyrieng/p/8694705.html">详解机器学习中的熵、条件熵、相对熵和交叉熵</a></p>

<p>KL 散度可以用来评估两个分布的差异程度，假设未知真实分布为 $P(x)$，已知估计分布为 $Q(x)$，根据其定义</p>

\[\begin{aligned}
KL(P\Vert Q) &amp;= -\int P(x){\rm log}Q(x) - [-\int P(x){\rm log} P(x)]\\
&amp;= H(P,Q) - H(P)
\end{aligned}\]

<p>注意到，后一项 $H(P)$ 是未知真实分布的信息熵，对于一个确定的未知分布而言是个常数。因此，在实际应用中我们只关心前一项的取值，前一项 $H(P,Q)$ 我们称为 <strong>交叉熵</strong>。</p>

<p>因此，我们可以得到如下的等式</p>

\[相对熵（KL散度） = 交叉熵 - 信息熵\]

<p>根据前面 $KL 散度\geq 0$ 的性质，我们可以知道，$交叉熵 \geq 信息熵$。</p>

<hr />

<p>在机器学习中，我们希望在训练数据上模型学到的分布 $Q(x)$ 和真实数据的分布  $P(x)$ 越接近越好，所以我们可以使其相对熵最小。但是我们没有真实数据的分布，所以只能希望模型学到的分布 $Q(x)$ 和训练数据的分布 $P_t(x)$ 尽量相同。</p>

<p>假设训练数据是从总体中独立同分布采样的，那么我们可以通过最小化训练数据的经验误差来降低模型的泛化误差。即：</p>

<ul>
  <li><strong>希望</strong>学到的模型的分布和真实分布一致，$Q(x) \simeq P(x)$</li>
  <li>但是真实分布不可知，假设训练数据是从真实数据中独立同分布采样的，$P_t(x) \simeq P(x)$</li>
  <li>因此，我们<strong>希望</strong>学到的模型分布至少和训练数据的分布一致，$Q(x) \simeq P_t(x)$</li>
</ul>

<p>根据之前的描述，最小化训练数据上的分布 $P_t(x)$ 与最小化模型分布 $Q(x)$ 的差异等价于最小化<strong>相对熵</strong>，即 ${\rm minimize}\ [KL(P_t(x)\vert \vert Q(x))]$。此时，$P_t(x)$ 就是 $KL(p\vert \vert q)$ 中的 $p$，即真实分布，$Q(x)$ 就是 $q$。又因为训练数据的分布 $p$ 是给定的，所以求 $KL(p\vert \vert q)$ 等价于求 $H(p,q)$。得证，<strong>交叉熵可以用来计算学习模型分布与训练分布之间的差异</strong>。</p>

<p>实际上，由于 $P(x)$ 是已知</p>

\[{\rm minimize}\ H(p,q) = {\rm minimize}\ [-\int P(x){\rm log}Q(x)] = {\rm minimize}\ \mathbb E_p[-log Q(x)]\]

<p><strong>任何一个负对数似然组成的损失都是定义在训练集上的经验分布和定义在模型上的真实分布之间的交叉熵。</strong></p>

<blockquote>
  <p>ccj_zj. <a href="https://blog.csdn.net/ccj_ok/article/details/78066619">多分类问题中的交叉熵</a></p>

  <p><strong>交叉熵是直接衡量两个分布，或者说两个model之间的差异。而似然函数则是解释以model的输出为参数的某分布模型对样本集的解释程度。因此，可以说这两者是“同貌不同源”，但是“殊途同归”啦。</strong></p>
</blockquote>

<h2 id="35-softmax-函数"><span class="me-2">3.5. softmax 函数</span><a href="#35-softmax-函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p><strong>$n$ 分类问题</strong> 的 softmax 函数定义如下</p>

\[\hat y_i = P(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}\]

<p>假设某个训练样本经过神经网络后到达 softmax 层之前的输出向量为 $z=[ 1, 5, 3 ]$, 那么经过 softmax 函数后的概率分别为 $\hat y=[0.015,0.866,0.117]$。</p>

<p>假设期望输出标签为 $y = [0,1,0]$，那么交叉熵损失函数可以定义为</p>

\[L(\hat y, y) = -\sum_{j=1}^3 y_j {\rm ln} \hat{y}_j\]

<p>由于在分类问题中，期望输出标签是 one-hot 型变量。不失一般性，假设第 $j$ 个分量为 1 ，则损失函数为</p>

\[L(\hat y, y) = - {\rm ln} \hat{y}_j\ (y_j = 1)\]

<p>进行梯度下降时，根据链式法则</p>

\[\frac{\partial L}{\partial \omega} = \frac{\partial L}{\partial \hat y_j}\frac{\partial \hat y_j}{\partial z_i}\frac{\partial z_i}{\partial \omega_k}\]

<p>其中</p>

\[\frac{\partial L}{\partial \hat y_j} = -\frac{1}{\hat y_j}\]

<p>而 $\frac{\partial z_i}{\partial \omega_k}$ 根据网络具体形式，一般比较好求。</p>

<p>因此，重点在于求解中间的偏导项，需要分情况讨论</p>

<p>$j=i$ 时，表明反向传播至同样下标的上一层节点：</p>

\[\begin{aligned}
{\rm if}\quad j&amp;=i:\\
\frac{\partial \hat y_j}{\partial z_i}&amp;=\frac{\partial \hat y_i}{\partial z_i}\\
&amp;=\frac{\partial }{\partial z_i}(\frac{e^{z_i}}{\sum_{k} e^{z_k}})\\
&amp;=\frac{(e^{z_i})'\cdot \sum_{k} e^{z_k}-e^{z_i}\cdot e^{z_i}}{(\sum_{k} e^{z_k})^2}\quad(分式函数求导法则)\\
&amp;=\frac{e^{z_i}}{\sum_{k} e^{z_k}} - \frac{e^{z_i}}{\sum_{k} e^{z_k}}\frac{e^{z_i}}{\sum_{k} e^{z_k}}\\
&amp;= \hat y_j(1-\hat y_j)
\end{aligned}\]

<p>此时</p>

\[\frac{\partial L}{\partial z_j} = -\frac{1}{\hat y_j} \cdot \hat y_j(1-\hat y_j) = \hat y_j - 1\]

<p>可以看出形式非常简单，只要正向求一次得出结果，然后反向传梯度的时候，将结果减 1 即可。</p>

<p>$j\neq i$ 时，表明反向传播至不同下标的上一层节点：</p>

\[\begin{aligned}
{\rm if}\quad j&amp;\neq i:\\
\frac{\partial \hat y_j}{\partial z_i}&amp;=\frac{\partial }{\partial z_i}(\frac{e^{z_j}}{\sum_{k} e^{z_k}})\\
&amp;=\frac{ {\rm d} e^{z_j}/{\rm d} e^{z_i}\cdot \sum_{k} e^{z_k}-e^{z_j}\cdot e^{z_i} }{(\sum_{k} e^{z_k})^2}\\
&amp;=\frac{0\cdot \sum_{k} e^{z_k}-e^{z_j}\cdot e^{z_i}}{(\sum_{k} e^{z_k})^2}\\
&amp;=-\frac{e^{z_j}}{\sum_{k} e^{z_k}}\frac{e^{z_i}}{\sum_{k} e^{z_k}}\\
&amp;= -\hat y_j\hat y_i
\end{aligned}\]

<p>此时</p>

\[\frac{\partial L}{\partial z_j} = -\frac{1}{\hat y_j} \cdot (-\hat y_j\hat y_i) = \hat y_i\]

<p>形式同样非常简单，只要正向求一次得出结果，然后反向传梯度的时候，将它结果保存即可。</p>

<p>还是上面的例子，假设输出向量为 $z=[ 1, 5, 3 ]$, 那么经过 softmax 函数后的概率分别为 $\hat y=[0.015,0.866,0.117]$，交叉熵损失函数对 $z$ 的篇导数为 $\hat y’=[0.015,0.866-1,0.117] = [0.015,-0.134,0.117]$。</p>

<p>可以看出，softmax 配合 交叉熵损失函数 可以使得梯度下降非常简单。</p>

<h1 id="4-参考文献">4. 参考文献</h1>

<p>[1] 产品经理马忠信. <a href="https://www.jianshu.com/p/b570b1ba92bb">应该如何理解概率分布函数和概率密度函数？</a></p>

<p>[2] 忆臻. <a href="https://zhuanlan.zhihu.com/p/26614750">一文搞懂极大似然估计</a></p>

<p>[3] 马同学. <a href="https://www.matongxue.com/madocs/910">如何理解贝叶斯推断和beta分布？</a></p>

<p>[4] 遍地胡说. <a href="https://www.cnblogs.com/kyrieng/p/8694705.html">详解机器学习中的熵、条件熵、相对熵和交叉熵</a></p>

</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw me-1"></i>
    
      <a href='/categories/academic/'>Academic</a>,
      <a href='/categories/knowledge/'>Knowledge</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw me-1"></i>
      
      <a href="/tags/statistics/"
          class="post-tag no-text-decoration" >statistics</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        本文由作者按照 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         进行授权

      
    </div>

    <!-- Post sharing snippet -->

<div class="share-wrapper">
  <span class="share-label text-muted me-1">分享</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E6%A6%82%E7%8E%87%E4%B8%8E%E7%BB%9F%E8%AE%A1%EF%BC%89%20-%20SIRLIS&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fdeep-learning-probability-basic%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E6%A6%82%E7%8E%87%E4%B8%8E%E7%BB%9F%E8%AE%A1%EF%BC%89%20-%20SIRLIS&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fdeep-learning-probability-basic%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fdeep-learning-probability-basic%2F&text=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E6%A6%82%E7%8E%87%E4%B8%8E%E7%BB%9F%E8%AE%A1%EF%BC%89%20-%20SIRLIS"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <i
      id="copy-link"
      class="fa-fw fas fa-link small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="分享链接"
      data-title-succeed="链接已复制！"
    >
    </i>
  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
      
    </div>
  </div>
  <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
    <div class="access">
      <!-- Get the last 5 posts from lastmod list. -->














  <div id="access-lastmod" class="post">
    <div class="panel-heading">最近更新</div>
    <ul class="post-content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-DNN/">模式识别（经典神经网络）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Linear-Classifier/">模式识别（线性分类器）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Unsupervised-Classifier/">模式识别（无监督分类器）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-LDA&PCA/">模式识别（LDA和PCA）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Feature-Selection-and-Extraction/">模式识别（特征选择与特征提取）</a>
        </li>
      
    </ul>
  </div>
  <!-- #access-lastmod -->


      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pattern-recognition/">pattern recognition</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
    </div>
  </div>


    </div>

    
      
      



  <div id="toc-wrapper" class="ps-0 pe-4 mb-5">
    <div class="panel-heading ps-3 pt-2 mb-2">文章内容</div>
    <nav id="toc"></nav>
  </div>


    
  </div>
</div>

<!-- tail -->

  <div class="row">
    <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-3 pe-xl-4 mt-5">
      
        
        <!--
  Recommend the other 3 posts according to the tags and categories of the current post,
  if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->








  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  






<!-- Fill with the other newlest posts -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ms-1" data-toc-skip>
      相关文章
    </h3>
    <div class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        
        
        <div class="col">
          <a href="/posts/statistics-kernel-regression/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1617245449"
  data-df="YYYY/MM/DD"
  
>
  2021/04/01
</em>

              <h4 class="pt-0 my-2" data-toc-skip>统计学基础（核回归）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了统计学中的核回归方法，并铺垫了非参数化统计方法等一些基础知识。






  1. 基本知识
    
      1.1. 回归
      1.2. 近邻回归
      1.3. 核回归
    
  
  2. 参考文献


1. 基本知识

1.1. 回归

回归分析（Regression Analysis）是一种统计学上分析数据的方法，目的在于了解两个或多个变量间是否...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Policy-Gradient/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1701074599"
  data-df="YYYY/MM/DD"
  
>
  2023/11/27
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（策略梯度法）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的策略梯度法（Policy Gradient）。






  1. 回顾基于价值的强化学习
  2. 策略梯度
    
      2.1. 策略函数
      2.2. 策略函数的分布形式
      2.3. 策略梯度
        
          2.3.1. 回顾价值梯度
          2.3.2. 策略梯度
        
      
 ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/Pattern-Recognition-Linear-Classifier/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1709447119"
  data-df="YYYY/MM/DD"
  
>
  2024/03/03
</em>

              <h4 class="pt-0 my-2" data-toc-skip>模式识别（线性分类器）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了模式识别的线性分类器，包括线性分类器的基础概念、垂直平分准则以及垂直平分分类器、Fisher 投影准则、感知准则、最小错分样本准则、最小平方误差准则、最小二乘估计。






  1. 引言
  2. 线性分类器基础
    
      2.1. 线性判别函数
      2.2. 广义线性判别函数
      2.3. 二分类与多分类
      2.4. 线性分类器设计的基...
                </p>
              </div>
            </div>
          </a>
        </div>
      
    </div>
    <!-- .card-deck -->
  </div>
  <!-- #related-posts -->


      
        
        <!-- Navigation buttons at the bottom of the post. -->

<div class="post-navigation d-flex justify-content-between">
  
    <a
      href="/posts/deep-learning-encoder-decoder/"
      class="btn btn-outline-primary"
      prompt="上一篇"
    >
      <p>深度学习基础（Encoder-Decoder）</p>
    </a>
  

  
    <a
      href="/posts/rime-shift-switch-zh-en/"
      class="btn btn-outline-primary"
      prompt="下一篇"
    >
      <p>日常tips手册（Rime输入法）</p>
    </a>
  
</div>

      
        
        <!--  The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script src="https://utteranc.es/client.js"
        repo="sirlis/sirlis.github.io"
        issue-term="pathname"
        crossorigin="anonymous"
        async>
</script>

<script type="text/javascript">
  $(function() {
    const origin = "https://utteranc.es";
    const iframe = "iframe.utterances-frame";
    const lightTheme = "github-light";
    const darkTheme = "github-dark";
    let initTheme = lightTheme;

    if ($("html[data-mode=dark]").length > 0
        || ($("html[data-mode]").length == 0
            && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
      initTheme = darkTheme;
    }

    addEventListener("message", (event) => {
      let theme;

      /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */
      if (event.origin === origin) {
        /* page initial */
        theme = initTheme;

      } else if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {
        /* global theme mode changed */
        const mode = event.data.message;
        theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme);

      } else {
        return;
      }

      const message = {
        type: "set-theme",
        theme: theme
      };

      const utterances = document.querySelector(iframe).contentWindow;
      utterances.postMessage(message, origin);
    });

  });
</script>



      
    </div>
  </div>


        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 post-content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pattern-recognition/">pattern recognition</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>
    </div>

    <!-- The Footer -->

<footer>
  <div class="container px-lg-4">
    <div class="d-flex justify-content-center align-items-center text-muted mx-md-3">
      <p>本站采用 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 主题 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
      </p>

      <p>©
        2025
        <a href="https://github.com/sirlis">sirlis</a>.
        
          <span
            data-bs-toggle="tooltip"
            data-bs-placement="top"
            title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。"
          >保留部分权利。</span>
        
      </p>
    </div>
  </div>
</footer>


    <div id="mask"></div>

    <button id="back-to-top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow">
      <i class="fas fa-angle-up"></i>
    </button>

    
      <div
        id="notification"
        class="toast"
        role="alert"
        aria-live="assertive"
        aria-atomic="true"
        data-bs-animation="true"
        data-bs-autohide="false"
      >
        <div class="toast-header">
          <button
            type="button"
            class="btn-close ms-auto"
            data-bs-dismiss="toast"
            aria-label="Close"
          ></button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="px-2 mb-3">发现新版本的内容。</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            更新
          </button>
        </div>
      </div>
    

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.0/dist/jquery.min.js,npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.7/dayjs.min.js,npm/dayjs@1.11.7/locale/zh.min.js,npm/dayjs@1.11.7/plugin/relativeTime.min.js,npm/dayjs@1.11.7/plugin/localizedFormat.min.js,npm/tocbot@4.21.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '<div class="px-1 px-sm-2 px-lg-4 px-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

