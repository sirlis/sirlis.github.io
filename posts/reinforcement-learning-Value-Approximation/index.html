<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="zh-CN" 
  
>
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="强化学习（值函数近似）" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="本文首先介绍了值函数近似（Value Approximation），然后分别结合 SARSA 和 Q-Learning 给出了两种 Q 函数近似的方法。通过分析线性函数作为估计函数的局限性，自然引入神经网络来进行非线性函数近似，引出了基于深度学习的 Q 函数估计网络：Deep Q-Network（DQN）。" />
<meta property="og:description" content="本文首先介绍了值函数近似（Value Approximation），然后分别结合 SARSA 和 Q-Learning 给出了两种 Q 函数近似的方法。通过分析线性函数作为估计函数的局限性，自然引入神经网络来进行非线性函数近似，引出了基于深度学习的 Q 函数估计网络：Deep Q-Network（DQN）。" />
<link rel="canonical" href="http://localhost:4000/posts/reinforcement-learning-Value-Approximation/" />
<meta property="og:url" content="http://localhost:4000/posts/reinforcement-learning-Value-Approximation/" />
<meta property="og:site_name" content="SIRLIS" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-01-02T18:12:19+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="强化学习（值函数近似）" />
<meta name="twitter:site" content="@none" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-09T00:13:33+08:00","datePublished":"2023-01-02T18:12:19+08:00","description":"本文首先介绍了值函数近似（Value Approximation），然后分别结合 SARSA 和 Q-Learning 给出了两种 Q 函数近似的方法。通过分析线性函数作为估计函数的局限性，自然引入神经网络来进行非线性函数近似，引出了基于深度学习的 Q 函数估计网络：Deep Q-Network（DQN）。","headline":"强化学习（值函数近似）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/reinforcement-learning-Value-Approximation/"},"url":"http://localhost:4000/posts/reinforcement-learning-Value-Approximation/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>强化学习（值函数近似） | SIRLIS
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SIRLIS">
<meta name="application-name" content="SIRLIS">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.0/dist/tocbot.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/head.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <div class="site-title">
      <a href="/">SIRLIS</a>
    </div>
    <div class="site-subtitle fst-italic">分享科研和生活的日常</div>
  </div>
  <!-- .profile-wrapper -->

  <ul class="nav flex-column flex-grow-1 w-100 ps-0">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home"></i>
        <span>首页</span>
      </a>
    </li>
    <!-- the real tabs -->
    
      <li class="nav-item">
        <a href="/categories/" class="nav-link">
          <i class="fa-fw fas fa-stream"></i>
          

          <span>分类</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/tags/" class="nav-link">
          <i class="fa-fw fas fa-tags"></i>
          

          <span>标签</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/archives/" class="nav-link">
          <i class="fa-fw fas fa-archive"></i>
          

          <span>归档</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/about/" class="nav-link">
          <i class="fa-fw fas fa-info-circle"></i>
          

          <span>关于</span>
        </a>
      </li>
      <!-- .nav-item -->
    
  </ul>
  <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/sirlis"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/none"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['lihongjue','buaa.edu.cn'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</div>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container px-xxl-5">
        <!-- The Top Bar -->

<div id="topbar-wrapper">
  <div
    id="topbar"
    class="container d-flex align-items-center justify-content-between h-100"
  >
    <span id="breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                首页
              </a>
            </span>

          
        
          
        
          
            
              <span>强化学习（值函数近似）</span>
            

          
        
      
    </span>
    <!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      文章
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="搜索..."
      >
    </span>
    <span id="search-cancel">取消</span>
  </div>
</div>

        











<div class="row">
  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pe-xl-4">
    

    <div class="post px-1 px-md-2">
      

      
        
      
        <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->



  
  

  <!-- CDN URL -->
  

  <!-- Add image path -->
  

  
    
      
      
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  




<!-- return -->




<h1 data-toc-skip>强化学习（值函数近似）</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      发表于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1672654339"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2023/01/02
</em>

    </span>

    <!-- lastmod date -->
    
    <span>
      更新于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1744128813"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2025/04/09
</em>

    </span>
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      作者

      <em>
      
        <a href="https://github.com/sirlis">sirlis</a>
      
      </em>
    </span>

    <div>
      <!-- read time -->
      <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="6101 字"
>
  <em>33 分钟</em>阅读</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <p>本文首先介绍了值函数近似（Value Approximation），然后分别结合 SARSA 和 Q-Learning 给出了两种 Q 函数近似的方法。通过分析线性函数作为估计函数的局限性，自然引入神经网络来进行非线性函数近似，引出了基于深度学习的 Q 函数估计网络：Deep Q-Network（DQN）。</p>

<!--more-->

<hr />

<ul>
  <li><a href="#1-引言">1. 引言</a></li>
  <li><a href="#2-状态价值函数近似">2. 状态价值函数近似</a>
    <ul>
      <li><a href="#21-目标函数">2.1. 目标函数</a></li>
      <li><a href="#22-优化算法">2.2. 优化算法</a></li>
      <li><a href="#23-估计函数设计">2.3. 估计函数设计</a>
        <ul>
          <li><a href="#231-采用线性函数">2.3.1. 采用线性函数</a></li>
          <li><a href="#232-线性函数实例分析">2.3.2. 线性函数实例分析</a></li>
          <li><a href="#233-采用神经网络">2.3.3. 采用神经网络</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#3-状态动作价值函数近似">3. 状态动作价值函数近似</a>
    <ul>
      <li><a href="#31-sarsa-值函数近似">3.1. SARSA 值函数近似</a></li>
      <li><a href="#32-q-learning-值函数近似">3.2. Q-Learning 值函数近似</a></li>
    </ul>
  </li>
  <li><a href="#4-deep-q-learningdqn">4. Deep Q-Learning（DQN）</a>
    <ul>
      <li><a href="#41-损失函数">4.1. 损失函数</a></li>
      <li><a href="#42-优化方法">4.2. 优化方法</a>
        <ul>
          <li><a href="#421-分离拷贝">4.2.1. 分离拷贝</a></li>
          <li><a href="#422-经验回放experience-replay">4.2.2. 经验回放（experience replay）</a></li>
        </ul>
      </li>
      <li><a href="#43-算法流程">4.3. 算法流程</a></li>
      <li><a href="#44-实例分析">4.4. 实例分析</a></li>
      <li><a href="#45-过高估计double-dqn">4.5. 过高估计（Double DQN）</a></li>
    </ul>
  </li>
</ul>

<h2 id="1-引言"><span class="me-2">1. 引言</span><a href="#1-引言" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>前面介绍的 TD 算法都仅针对对离散的（表格形式的）状态和动作进行研究，但实际应用中的场景可能会很复杂，很难定义出离散的状态；即使能够定义，数量也非常大，无法用数组存储。</p>

<p>对于强化学习来说，很多实际应用问题的输入数据是高维的，如图像，声音，算法要根据它们来选择一个动作执行以达到某一预期的目标。比如，对于自动驾驶算法，要根据当前的画面决定汽车的行驶方向和速度。经典的强化学习算法如 Q-Learning 需要列举出所有可能的情况（称为状态，这是对当前所处环境的抽象），然后进行迭代。 Q-Learning 的经典实现是列举出所有的状态和动作，构建 Q-Table，这是一个二维的表，然后迭代计算各种状态下执行各种动作的预期回报的最大值。对于高维的输入数据，显然是不现实的，因为如果直接以原始数据作为状态，维数太高，而且状态数量太多。</p>

<p>另一种方案是用一个函数来逼近价值函数，函数的输入是原始的状态数据，函数的输出是价值函数值。在有监督学习中，我们用神经网络来拟合分类或回归函数，因此当然也可以用神经网络可来拟合强化学习中的价值函数。</p>

<h2 id="2-状态价值函数近似"><span class="me-2">2. 状态价值函数近似</span><a href="#2-状态价值函数近似" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>给定策略 $\pi$，假设 $v_\pi(s)$ 和 $\hat{v}_\pi(s,w)$ 分别为状态 $s$ 的价值函数与其函数估计。我们的目标是找到最优的参数 $w$，使得在任意状态 $s$ 下，状态价值函数的估计 $\hat{v}(s,w)$ 都接近其真实值。</p>

<p>这本质上是一个策略估计问题，具体可分为以下两步：</p>

<ul>
  <li>定义目标函数；</li>
  <li>找到合适的算法优化这个目标函数；</li>
  <li>确定状态价值函数的估计 $\hat{v}(s,w)$ 的具体实现方式（线性？非线性？神经网络？）。</li>
</ul>

<h3 id="21-目标函数"><span class="me-2">2.1. 目标函数</span><a href="#21-目标函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>很显然，我们希望<strong>所有状态</strong>的状态价值函数的估计 $\hat{v}(s,w)$ <strong>接近</strong>其真实值，这是一种距离度量，那么目标函数设计为</p>

\[J(w) = \mathbb{E}_{\mathcal{S}}[(v_\pi(s)-\hat{v}(s,w))^2] = \Vert v_\pi - \hat{v} \Vert^2_D\]

<blockquote>
  <p>$D$ 为状态的分布 $D_\pi$，则上式还可写为 $(v_\pi - \hat{v})^\top D (v_\pi - \hat{v})$</p>
</blockquote>

<p>下面讨论如何采取合适的近似方法消除期望符号。</p>

<p>一种直觉方法是对所有状态进行平均。如果我们认为所有状态的权重都相同，那么采用 <strong>均匀分布</strong>（uniform distribution）进行加权，目标函数可以改写为</p>

\[J(w) =\textcolor{red}{\frac{1}{\vert\mathcal{S}\vert}}\sum_{s\in\mathcal{S}} (v_\pi(s)-\hat{v}(s,w))^2\]

<p>但在强化学习中，很明显状态的重要性是不一样的，比如 Grid World 场景中的终点状态和靠近终点的状态显然更加重要，而远离终点的某些状态可能没那么重要。因此，我们需要采用 <strong>平稳分布</strong>（stationary distribution）进行加权，目标函数改写为</p>

\[J(w) = \sum_{s\in\mathcal{S}} \textcolor{red}{d_\pi(s)} (v_\pi(s)-\hat{v}(s,w))^2\]

<p>其中，</p>

<ul>
  <li>$d_\pi(s)$ 为策略 $\pi$ 下马尔可夫过程的平稳分布，其定义为 $d_\pi(s)\geq 0,\; \sum_{s\in\mathcal{S}}d_\pi(s) = 1$；
    <ul>
      <li>平稳分布刻画了马尔可夫过程长期迭代后，各个状态出现的（被访问的）概率；</li>
      <li>平稳分布描述了马尔可夫过程的长期行为（long-run behaviour）；</li>
      <li>马尔可夫链可能存在唯一平稳分布，无穷多个平稳分布，或不存在平稳分布；</li>
    </ul>
  </li>
  <li>上述目标函数是一个带权重的均方误差，某个状态的出现概率越大，其误差的权重越大。</li>
</ul>

<p>根据上述定义，策略 $\pi$ 下经过长马尔可夫采样回合后，假设状态 $s$ 被采样到了 $n_\pi(s)$ 次，平稳分布可由下式估计</p>

\[d_\pi(s) \approx \frac{n_\pi(s)}{\sum_{s^\prime\in\mathcal{S}}n_\pi(s^\prime)}\]

<p>从其经过长期状态转移后的不变性的角度考虑，已知状态转移矩阵 $P_\pi = p(s^\prime \vert s, a)$，平稳分布必然满足如下等式</p>

\[d_\pi^T = d_\pi^T P_\pi\]

<p>也即，平稳分布就是状态转移矩阵特征值为 1 的特征向量。</p>

<h3 id="22-优化算法"><span class="me-2">2.2. 优化算法</span><a href="#22-优化算法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>由于我们希望最小化上述目标函数，很自然想到使用 <strong>梯度下降法</strong></p>

\[w_{k+1} = w_k - \alpha_w \nabla J(w_k)\]

<p>其中梯度为</p>

\[\begin{aligned}
\nabla_w J(w) &amp;= \nabla_w\mathbb{E}[(v_\pi(s)-\hat{v}(s,w))^2]\\
&amp;= \mathbb{E}[\nabla_w(v_\pi(s)-\hat{v}(s,w))^2]\\
&amp;= -2\mathbb{E}[(v_\pi(s)-\hat{v}(s,w))\nabla_w\hat{v}(s,w)]
\end{aligned}\]

<p>为了避免在真实梯度计算中计算期望，很自然想到采用 随机梯度 代替 真实梯度，即</p>

\[\begin{aligned}
w_{t+1} &amp;= w_t + 2\alpha_t\mathbb{E}[(v_\pi(s)-\hat{v}(s,w))\nabla_w\hat{v}(s,w)]\\
\Rightarrow w_{t+1} &amp;= w_t + 2\alpha_t(v_\pi(s_t)-\hat{v}(s_t,w_t))\nabla_w\hat{v}(s_t,w_t)
\end{aligned}\]

<p>注意到，实际使用中，上述梯度下降迭代式要求状态价值函数 $v_\pi(s_t)$ 已知，但我们本身就是在做状态价值函数估计，所以肯定不可能提前知道其真值。因此，我们需要将其替换为某种估算形式，具体方式包括：</p>

<ul>
  <li>
    <p>采用 MC 的方式迭代优化，被称为 <strong>MC with Value Approximation</strong>：</p>

\[w_{t+1} = w_t + 2\alpha_t(\textcolor{red}{G_t}-\hat{v}(s_t,w_t))\nabla_w\hat{v}(s_t,w_t)\]
  </li>
  <li>
    <p>采用 TD 的方式迭代优化，被称为 <strong>TD with Value Approximation</strong>：</p>

\[w_{t+1} = w_t + 2\alpha_t[\textcolor{red}{r_{t+1}+\gamma \hat{v}_\pi(s_{t+1},w_t)}-\hat{v}(s_t,w_t)]\nabla_w\hat{v}(s_t,w_t)\]
  </li>
</ul>

<blockquote>
  <p>严格来说，采用 TD 的方式迭代优化后，求解的问题不再是原始目标（称其为 True Value Error）函数的最小化，而是优化 Projected Bellman Error。此处不再展开，可参考《Mathmatical Foundations of Reinforcement Learning》。</p>
</blockquote>

<h3 id="23-估计函数设计"><span class="me-2">2.3. 估计函数设计</span><a href="#23-估计函数设计" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="231-采用线性函数"><span class="me-2">2.3.1. 采用线性函数</span><a href="#231-采用线性函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>早期广泛使用线性函数作为状态值函数的估计方法</p>

\[\hat{v}(s,w) = aw_1+w_2 = \underbrace{[s,1]}_{\phi^\top(s)}
\underbrace{\begin{bmatrix}
  w_1\\w_2
\end{bmatrix}}_{w}=
\phi^\top(s)w\]

<p>可以看出，$\hat{v}(s,w)$ 是关于 $w$ 的线性函数，$\phi(s)$ 是<strong>特征向量</strong>，可以是一组多项式基、傅里叶基、…等等，但合适的基比较难选择。</p>

<blockquote>
  <p>从模式识别的角度，这里的线性函数本质上是设计一组能够反应状态价值函数特征的向量，且维数要小于原始状态价值函数的维数，后面的实例可以更好的反映这一点。</p>
</blockquote>

<p>线性函数估计的梯度为</p>

\[\nabla_w \hat{v}(s,w) = \nabla_w [\phi^\top(s)w] = \phi(s)\]

<p>将上述两式代入前述 TD 形式的迭代优化式中，有</p>

\[w_{t+1} = w_t + \alpha_t[r_{t+1}+\gamma \textcolor{red}{\phi^\top(s_{t+1})w_t}-\textcolor{red}{\phi^\top(s_t)w_t}] \textcolor{red}{\phi(s_t)}\]

<p>上述迭代式可被称为 <strong>TD-Linear</strong>。</p>

<p>更加特殊的情况下，我们选择如下线性函数</p>

\[\phi(s) = e_s\in\mathbb{R}^{\vert S \vert}\]

<p>其中 $e_s$ 是一个第 $s$ 个分量为 1，其余分量为 0 的向量（也即<u>独热向量</u>），那么</p>

\[\hat{v}(s,w) = e_s^\top w = w(s)\]

<p>就相当于取到 $w$ 的第 $s$ 个分量用来对 $v_\pi(s)$ 进行估计。将其代入 TD-Linear 有</p>

\[w_{t+1} = w_k + \alpha_k[r_{t+1}+\gamma \textcolor{red}{w(s_{t+1})}-\textcolor{red}{w(s_t)}] \textcolor{red}{e_{s_t}}\]

<p>注意到 $e_{s_t}$ 是一个第 $s_t$ 个分量为 1，其余分量为 0 的向量，那么上面的向量等式中，只有分量为 1 的那个（行）等式有意义，把这行提取出来有</p>

\[w_{t+1}(s_t) = w_t(s_t) + \alpha_t[r_{t+1}+\gamma w_t(s_{t+1}) - w_t(s_t)]\]

<p>可以看出，只要把 $w$ 换成 $v$，上式与前述 TD 方法的迭代式一模一样。也就是说，（对表格性任务而言）<u>TD 方法与采用独热函数的值函数估计等价</u>。</p>

<h4 id="232-线性函数实例分析"><span class="me-2">2.3.2. 线性函数实例分析</span><a href="#232-线性函数实例分析" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>下面通过一个 Grid World 实例来表明，<u>采用线性函数估计能够降低状态价值函数估计的参数个数</u>。在实例中，采取均匀随机分布的策略 $\pi(a\vert s) = 0.2,\forall s,a$，我们对所有 25 个状态的价值函数进行估计，本质是一个策略评估过程。实例中用到的参数为 $r_{\text{forbidden}}=r_{\text{boundary}}=-1,r_{\text{target}}=1, \gamma = 0.9$。</p>

<p>我们首先使用动态规划方法，得到在该策略下，每个状态的真实最优状态价值函数，如图所示。中间子图表明了二维情况下的各个状态价值函数，右侧子图则将其形象展示到了三维空间中：</p>

<p><a href="/assets/img/postsimg/20230102/linear-value-approx-true.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/linear-value-approx-true.jpg" alt="linear-value-approx-true" class="lazyload" data-proofer-ignore></a></p>

<p>接着我们采用线性函数进行价函数估计，通用实验设置如下：</p>

<ul>
  <li>采样 500 个 episodes；</li>
  <li>每个 episode 包含 500 步，初始状态-动作均匀随机选取；</li>
</ul>

<p>下面开始实验：</p>

<ul>
  <li>
    <p><strong>采用独热函数的值函数估计</strong>（Tabular TD）</p>

    <p><a href="/assets/img/postsimg/20230102/linear-value-approx-TD-table.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/linear-value-approx-TD-table.jpg" alt="linear-value-approx-true" class="lazyload" data-proofer-ignore></a></p>

    <p>可以看出，随着迭代次数逐渐增加，价值函数估计值与真实值间的均方根误差收敛（右侧子图），估计结果（中间子图）逐渐逼近真实值（左侧子图）。</p>

    <p>独热函数本质上是采用<u>与状态个数同阶的线性函数估计</u>，因此当然能够逼近最优值。</p>
  </li>
  <li>
    <p><strong>采用三阶线性函数的值函数估计</strong></p>

    <p>注意到该实例中状态可以坐标形式给出：$s={x,y}$，那么设计线性函数为</p>

\[\phi(s) = [1, x, y]^\top \in \mathbb{R}^3\]

    <p>则状态价值函数估计值为一个平面</p>

\[\hat{v}(s,w) = \phi^\top(s)w = [1,x,y]\begin{bmatrix}
  w_1\\w_2\\w_3
\end{bmatrix} = w_1+w_2x+w_3y\]

    <p>最终结果为</p>

    <p><a href="/assets/img/postsimg/20230102/linear-value-approx-TD-linear.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/linear-value-approx-TD-linear.jpg" alt="linear-value-approx-TD-linear" class="lazyload" data-proofer-ignore></a></p>

    <p>可以看出，收敛后价值函数估计值与真实值间的均方根误差存在一个常值偏差（右侧子图），估计结果（中间子图）的趋势和真实值（左侧子图）相同，但因为本质上是用一个平面估计非平面，因此近似能力有限，必然存在误差。</p>
  </li>
  <li>
    <p><strong>采用高阶性函数的值函数估计</strong></p>

    <p>采用</p>

\[\begin{aligned}
\phi(s) &amp;= [1,x,y,x^2,y^2,xy]^\top\in\mathbb{R}^6\\
\phi(s) &amp;= [1,x,y,x^2,y^2,xy,x^3,y^3,x^2y,xy^2,]^\top\in\mathbb{R}^{10}
\end{aligned}\]

    <p>则状态价值函数估计值为一个二次/三次曲面，最终结果为</p>

    <p>二次曲面：</p>

    <p><a href="/assets/img/postsimg/20230102/linear-value-approx-TD-nonlinear.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/linear-value-approx-TD-nonlinear.jpg" alt="linear-value-approx-TD-nonlinear" class="lazyload" data-proofer-ignore></a></p>

    <p>三次曲面：</p>

    <p><a href="/assets/img/postsimg/20230102/linear-value-approx-TD-nonlinear-2.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/linear-value-approx-TD-nonlinear-2.jpg" alt="linear-value-approx-TD-nonlinear" class="lazyload" data-proofer-ignore></a></p>

    <p>可以看出，随着参数个数的增多，其曲面的拟合能力越好。同时也注意到，即使继续增加阶数也不一定能保证完全消除估计误差，因为线性函数结构本身能力存在限制，这也是为什么如今更加广泛采用神经网络来估计值函数，因为神经网络从理论上来说可以近似任意一个非线性函数。</p>
  </li>
</ul>

<h4 id="233-采用神经网络"><span class="me-2">2.3.3. 采用神经网络</span><a href="#233-采用神经网络" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>目前被广泛使用的是神经网络作为一个非线性估计函数，其输入为状态 $s$，输出为 $\hat{v}(s,w)$，其中 $w$ 是神经网络的权重参数。关于神经网络估计函数这里不再展开介绍，后续对状态-动作价值函数进行估计时会详细介绍。</p>

<h2 id="3-状态动作价值函数近似"><span class="me-2">3. 状态动作价值函数近似</span><a href="#3-状态动作价值函数近似" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>前面介绍 TD 方法时，我们首先以状态价值函数（V 函数）为例引入了基本的 TD 思想，然后将其推广至状态动作价值函数（Q 函数），并结合不同的策略改进方法，分别得到 SARSA 和 Q-Learning 两种具体的时序差分方法。</p>

<p>这一节我们将采用类似的思想，将前述介绍的状态价值函数近似的基本思想，分别与 SARSA 和 Q-Learning 结合。</p>

<h3 id="31-sarsa-值函数近似"><span class="me-2">3.1. SARSA 值函数近似</span><a href="#31-sarsa-值函数近似" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>直接将状态价值函数近似的梯度下降迭代式中的 $v$ 替换为 $q$ 我们就得到</p>

\[w_{t+1} = w_t + \alpha_t[r_{t+1}+\gamma \hat{q}(s_{t+1},a_{t+1};w_t)-\hat{q}(s_t,a_t;w_t)]\nabla_w\hat{q}(s_t,a_t;w_t)\]

<p>结合策略改进算法，形成伪代码如下：</p>

<hr />

<ul>
  <li>Initialize $Q(s,a)$ arbitrarily, choose start state $s$, given a <u>behaviour</u> policy $\pi_t$ (e.g., $\varepsilon$-greedy)</li>
  <li>Repeat (for each episode)
    <ul>
      <li>Repeat (if current $s_t$ is not terminal state)
        <ul>
          <li><em>collect experience</em>:
            <ul>
              <li>choose action $a_t$ at $s_t$ following $\pi_t(s_t)$</li>
              <li>Take action $a$, get $r$, transfer to $s_{t+1}$</li>
              <li>choose action $a_{t+1}$ at $s_{t+1}$ following $\pi_t(s_{t+1})$</li>
            </ul>
          </li>
          <li><em><font color="red">parameter update</font></em>:
            <ul>
              <li>$w_{t+1} = w_t + \alpha_t[r_{t+1}+\gamma \hat{q}(s_{t+1},a_{t+1};w_t)-\hat{q}(s_t,a_t;w_t)]\nabla_w\hat{q}(s_t,a_t;w_t)$</li>
              <li>${s_t\leftarrow}s_{t+1}; a_t\leftarrow a_{t+1}$</li>
            </ul>
          </li>
          <li><em>policy improvement</em>:
            <ul>
              <li>(update same $\varepsilon$-greedy as <u>target</u> policy)</li>
              <li>$\pi_{t+1}(a\vert s) = 1-\frac{\varepsilon}{\vert \mathcal{A} \vert}(\mathcal{A}-1)$ if $a=\arg\max_a \textcolor{red}{\hat{q}(s,a;w_t)}$</li>
              <li>$\pi_{t+1}(a\vert s) = \frac{\varepsilon}{\vert \mathcal{A} \vert}$ otherwise</li>
              <li>$t \leftarrow t+1$</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<p>上述伪代码中标红部分即为 SARSA 值函数近似方法，与传统 SARSA 方法不同的地方。</p>

<p>设计一个采用 5 阶傅里叶基函数函数的 SARSA 值函数近似方法，在 Grid World 实例下的训练结果。其中实例设置为： $r_{\text{boundary}}=r_{\text{forbidden}}=-10,r_{\text{target}}=1,\alpha=0.001,\gamma=0.9$。从左上角初始状态出发，迭代500步，结果如下图所示</p>

<p><a href="/assets/img/postsimg/20230102/SARSA-linear.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/SARSA-linear.jpg" alt="SARSA-linear-result" class="lazyload" data-proofer-ignore></a></p>

<h3 id="32-q-learning-值函数近似"><span class="me-2">3.2. Q-Learning 值函数近似</span><a href="#32-q-learning-值函数近似" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>同样直接将状态价值函数近似的梯度下降迭代式中的 $v$ 替换为 $q$ 我们就得到</p>

\[w_{t+1} = w_t + \alpha_t[r_{t+1}+\gamma \textcolor{red}{\max_{a\in\mathcal{A}(s_{t+1})}}\hat{q}(s_{t+1},a;w_t)-\hat{q}(s_t,a_t;w_t)]\nabla_w\hat{q}(s_t,a_t;w_t)\]

<p>结合策略改进算法，形成伪代码如下（注意其策略评估迭代式并没有直接照搬上面的式子，而是将取最大 $q$ 函数部分放到经验采集环节）：</p>

<hr />

<ul>
  <li>Initialize $Q(s,a)$ arbitrarily, choose start state $s$, given a <u>behaviour</u> policy $\pi_b$ (e.g., $\varepsilon$-greedy, uniform, human, etc)</li>
  <li>Repeat (for each episode):
    <ul>
      <li>Repeat (if $s$ is not terminal state):
        <ul>
          <li><em>collect experience</em>:
            <ul>
              <li>choose action $a_t$ at $s_t$ following $\pi_t(s_t)$</li>
              <li>Take action $a$, get $r$, transfer to $s_{t+1}$</li>
              <li>choose $a_{t+1}= \arg\max_a \textcolor{red}{\hat{q}(s_{t+1}, a;w_t)}$</li>
            </ul>
          </li>
          <li><em><font color="red">parameter update</font></em>:
            <ul>
              <li>$w_{t+1} = w_t + \alpha_t[r_{t+1}+\gamma \hat{q}(s_{t+1},a_{t+1};w_t)-\hat{q}(s_t,a_t;w_t)]\nabla_w\hat{q}(s_t,a_t;w_t)$</li>
            </ul>
          </li>
          <li><em>policy improvement</em>:
            <ul>
              <li>(update greedy policy as <u>target</u> policy)</li>
              <li>$\pi_{t+1}(a\vert s) = 1$ if $a=\arg\max_a \textcolor{red}{\hat{q}(s,a;w_t)}$</li>
              <li>$\pi_{t+1}(a\vert s) = 0$ otherwise</li>
              <li>${s\leftarrow}s^\prime; a\leftarrow a^\prime$</li>
            </ul>
          </li>
          <li>$t \leftarrow t+1$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<p>上述伪代码中标红部分即为 Q-Learning 值函数近似方法，与传统 Q-Learning 方法不同的地方。</p>

<p>设计一个采用 5 阶傅里叶基函数的 Q-Learning 值函数近似方法（采样 <strong>on-policy</strong> 而不是上面介绍的 off-policy 版本），在 Grid World 实例下的训练结果。其中实例设置为： $r_{\text{boundary}}=r_{\text{forbidden}}=-10,r_{\text{target}}=1,\alpha=0.001,\gamma=0.9$。从左上角初始状态出发，迭代500步，结果如下图所示</p>

<p><a href="/assets/img/postsimg/20230102/Q-Learning-linear.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/Q-Learning-linear.jpg" alt="Q-Learning-linear-result" class="lazyload" data-proofer-ignore></a></p>

<p>可以看出，即使采用 on-policy 的 Q-Learning 也比 SARSA 更激进，收敛速度更快（因为里面的第二次动作采样使用 greedy 策略）。</p>

<h2 id="4-deep-q-learningdqn"><span class="me-2">4. Deep Q-Learning（DQN）</span><a href="#4-deep-q-learningdqn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>前面我们使用线性函数来做值函数近似，其本质是从高维数据中抽象出特征，作为状态，然后用强化学习建模，但这种做法很大程度上依赖于人工特征的设计（也即线性函数基的选取）。</p>

<p>因此，更加广泛的做法是使用一个非线性函数——乃至神经网络——来做值函数近似。Deep Q-Learning（原文称为 Deep Q-Network，DQN）就是源于上述思想，它也是强化学习领域最早且最成功将深度神经网络引入强化学习的尝试之一。</p>

<blockquote>
  <p>DQN 由DeepMind公司在2013年提出，2015年其改进型发表在Nature上。这种方法用卷积神经网络拟合价值函数，一般是Q函数。网络的输入为原始场景数据，如游戏的画面图像，输出为在这种场景下执行各种动作时所能得到的Q函数的极大值。</p>
</blockquote>

<p>如果使用一个神经网络来做值函数近似，可以有以下两种形式：</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>       +----+ ----&gt;Q(s,a1)                         +----+
s ---&gt; |f(θ)| ----&gt;Q(s,a2)         or        s ---&gt;|f(θ)| ----&gt;Q(s,ai)
       |    | ----&gt;...                       ai---&gt;|    | 
       +----+ ----&gt;Q(s,an)                         +----+
</pre></td></tr></tbody></table></code></div></div>

<p>那么应该采取哪种网络形式呢？</p>

<p>上面两种形式都可能出现，但一般而言，DQN 采用左边形式。因为：</p>

<ul>
  <li>左边形式：
    <ul>
      <li>DQN 需要找到 $\max_a q(s,a)$，左边输入一次就能得到所有动作的价值，而右边输入则需要多次输入，增加了计算复杂度；</li>
      <li>强化学习问题可能包含大量的动作选项。将动作作为网络输入会使网络的复杂度急剧增加，不利于训练和收敛。相反，通过仅输入状态，网络可以更专注地学习状态与价值之间的关系，而不必考虑如何直接处理动作空间的复杂性。</li>
    </ul>
  </li>
  <li>右边形式：
    <ul>
      <li>有时候，将动作作为网络输入是有必要的，比如在某些连续动作空间中，动作空间的维度可能比状态空间的维度还要高。</li>
    </ul>
  </li>
</ul>

<h3 id="41-损失函数"><span class="me-2">4.1. 损失函数</span><a href="#41-损失函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>神经网络的损失函数用 Q-Learning 的 TD Error 构造。回顾 Q-Learning 的策略评估迭代式</p>

\[q(s,a) \leftarrow q(s,a)-\alpha\{q(s,a)-[r+\gamma \max_a q(s^\prime,a)]\}\]

<p>当 $q(s,a)$ 收敛时必然有后一项为零。那么损失函数可以写为</p>

\[L(\theta) = \mathbb{E}[(r+\gamma \mathop{\text{max}}\limits_{a^\prime} q(s^\prime,a^\prime;\textcolor{red}{\theta})-q(s,a;\textcolor{red}{\theta}))^2]\]

<p>这个损失函数是神经网络的输出值与 Q 函数一步估计值之间的误差。其之所以能够成立的原因，是因为其本质上是在求解 $Q$ 函数形式的贝尔曼最优方程（前述马尔可夫决策4.7章节有过介绍）</p>

\[q(s,a) = \mathbb{E}[R_{t+1}+\gamma \max_a q(s_{t+1},a)\vert s,a], \forall s,a\]

<p>那么在期望形势下，上面的损失函数自然应该趋向于零。</p>

<h3 id="42-优化方法"><span class="me-2">4.2. 优化方法</span><a href="#42-优化方法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>神经网络的损失函数自然该采用梯度下降法求解，梯度下降要计算损失函数的梯度，但注意到，损失函数中有两项都包含网络参数 $\theta$，其中 TD Target 含参部分还含有求最大值操作，这个梯度求解比较困难，因此引出第一个技巧：<strong>分离拷贝</strong>。</p>

<p>另一方面，和 Q-Learning 类似，DQN 通过执行动作来生成经验数据 $S_t, A_t, R_{t+1},S_{t+1}$ 作为样本来进行训练的。具体而言，给定一个状态，用当前的神经网络进行预测，得到所有动作的 Q 函数，然后按照策略选择一个动作执行，得到下一个状态以及回报值，以此构造训练样本。</p>

<p>从监督学习的角度，一般要求训练样本之间是相互独立的，但在强化学习中，一次采样得到的序列中，取出的<u>经验数据是前后高度相关</u>。为了解决训练样本之间存在相关性，DQN 采用了<strong>经验回放</strong>机制。</p>

<p>下面分别介绍上述两个技巧。</p>

<h4 id="421-分离拷贝"><span class="me-2">4.2.1. 分离拷贝</span><a href="#421-分离拷贝" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>分离拷贝指的是在求损失函数梯度的过程中，短时间内冻结 TD Target 中的网络权重参数，此时我们假设被冻结的参数为 $\theta_T$，那么损失函数就可以写为如下形式：</p>

\[L(\theta) = \mathbb{E}[(\underbrace{r+\gamma \mathop{\text{max}}\limits_{a^\prime} q(s^\prime,a^\prime;\textcolor{red}{\theta_T})}_{\text{TD Target}}-q(s,a;\theta))^2]\]

<p>此时梯度只与最后一项有关，可求解得到</p>

\[\nabla_\theta L(\theta) = -[ r+\gamma \max_{a^\prime}q( s^\prime,a^\prime;\textcolor{red}{\theta_T} ) -q( s,a;\theta ) ] \nabla _{\theta}q( s,a;\theta )\]

<p>相应的网络参数更新式为</p>

\[\theta \leftarrow \theta - \alpha \nabla_\theta L(\theta)\]

<p>上述过程中，$\theta_T$ 只能冻结较短的时间，否则 TD Target 长时间得不到更新，梯度下降会偏离最优方向，导致算法失效。因此我们每隔一段时间就要对 $\theta_T$ 进行一次更新。</p>

<p>DQN 采用的分离拷贝思想就源于此，在训练开始时刻设计两个 Q-Network 而不是一个，他们的参数分别为 $\theta,\theta_T$，其中被持续更新的网络称为 <strong>主网络</strong>，被周期性冻结的网络称为 <strong>目标网络</strong>。</p>

<ul>
  <li>主网络：$\hat{q}(s,a;\theta)$</li>
  <li>目标网络：$\hat{q}(s,a;\theta_T)$</li>
</ul>

<p>初始时刻二者相等。每隔一段时间，直接将主网络较新的网络参数，直接拷贝给目标网络参数，然后再次冻结目标网络，继续更新主网络，如此循环直至训练结束。示意图如下：</p>

<p><a href="/assets/img/postsimg/20230102/separate-copy.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/separate-copy.jpg" alt="separate-copy" class="lazyload" data-proofer-ignore></a></p>

<h4 id="422-经验回放experience-replay"><span class="me-2">4.2.2. 经验回放（experience replay）</span><a href="#422-经验回放experience-replay" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>经验回放的具体做法是，训练时不再沿着采样的顺序逐个经验数据训练，而是先把经验数据存储到一个集合 $\mathcal{B}\doteq { (s,a,r,s^\prime) }$，称其为 <strong>回放缓冲</strong>（replay-buffer）。训练 DQN 时，每次从这个集合中<strong>均匀随机</strong>（uniformly）抽取出部分样本作为 mini-batch 训练样本，以此打破样本之间的相关性。</p>

<blockquote>
  <p>经验回放同样可以用于原始 Q-Learning 算法，因为它本身也是 off-policy 方法，行为策略和目标策略不一致。可以先用行为策略采样一批经验数据，然后再用目标策略进行训练。甚至引入经验回放后，可以进一步提高 Q-Learning 的数据利用效率，因为一组经验样本可能会被抽取和使用很多次。</p>
</blockquote>

<h3 id="43-算法流程"><span class="me-2">4.3. 算法流程</span><a href="#43-算法流程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>DQN 的伪代码如下：</p>

<hr />

<ul>
  <li>Given a <u>behaviour</u> policy $\pi_b$ (e.g., $\varepsilon$-greedy, uniform, human, etc)</li>
  <li>Store experience samples generated by $\pi_b$ in replay buffer $\mathcal{B}={(s,a,r,s^\prime)}$</li>
  <li>Initialize two network with parameters $\theta,\theta_T$ randomly</li>
  <li>Repeat (for each iteration):
    <ul>
      <li>Uniformly draw a mini-batch of $N$ samples from $\mathcal{B}$</li>
      <li>For each sample $(s,a,r,s^\prime)$ in the mini-batch:
        <ul>
          <li>Compute TD Target: $y=r+\gamma \max_{a^\prime} \hat{q}(s^\prime,a^\prime;\theta_T)$</li>
          <li>Compute TD Error: $\delta = y-\hat{q}(s,a;\theta)$</li>
          <li>Compute loss: $L(\theta) = \frac{1}{2}\delta^2$</li>
          <li>Update $\theta$ by minimizing $L(\theta)$</li>
        </ul>
      </li>
      <li>$\theta_T \leftarrow \theta$ every $C$ iterations</li>
    </ul>
  </li>
  <li>Get greedy policy from $\hat{q}(s,a;\theta)$</li>
</ul>

<hr />

<h3 id="44-实例分析"><span class="me-2">4.4. 实例分析</span><a href="#44-实例分析" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>下面我们使用 DQN 算法来解决一个简单的 Grid World 问题，我们希望得到所有状态下的最优策略，实例设置如下：</p>

<ul>
  <li>只采样 1 个episode，智能体达到目标状态后继续采样；</li>
  <li>该 episode 设置为 1000 步，采样 1000 步后停止；</li>
  <li>行为策略采用 $\pi(a\vert s) = 0.2,\forall s,a$ 策略；</li>
  <li>使用单隐层的浅层网络作为非线性近似器来估计 $\hat{q}(s,a;\theta)$，隐层有 100 个神经元。</li>
</ul>

<p>仿真结果如下：</p>

<p><a href="/assets/img/postsimg/20230102/DQN-1000step.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/DQN-1000step.jpg" alt="DQN-1000step" class="lazyload" data-proofer-ignore></a></p>

<p>可以看出，</p>

<ul>
  <li>图（a）：初始均匀随机策略；</li>
  <li>图（b）：经过 1000 步采样得到的数据已经基本覆盖了所有的状态动作对；</li>
  <li>图（d）：训练 DQN，其损失函数能够收敛，表明其很好地拟合了已有样本；</li>
  <li>图（c）：基于 DQN 的值函数估计给出的策略确实是最优策略；</li>
  <li>图（e）：DQN 对所有值函数的估计也收敛到真实值函数。</li>
</ul>

<p>将迭代步长降低到 100 步，相当于样本数量减少至原来的十分之一，结果如下：</p>

<p><a href="/assets/img/postsimg/20230102/DQN-100step.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/DQN-100step.jpg" alt="DQN-100step" class="lazyload" data-proofer-ignore></a></p>

<p>可以看出，</p>

<ul>
  <li>图（b）：经过 100 步采样得到的数据难以所有的状态动作对；</li>
  <li>图（d）：训练 DQN，其损失函数能够收敛，表明其很好地拟合了已有样本；</li>
  <li>图（c）：基于 DQN 的值函数估计给出的策略远不是最优策略；</li>
  <li>图（e）：DQN 对所有值函数的估计无法收敛到真实值函数。</li>
</ul>

<p>上述结果表明，即使 DQN 很强大，数据量不够或者采样不充分也没法保证其效果。</p>

<h3 id="45-过高估计double-dqn"><span class="me-2">4.5. 过高估计（Double DQN）</span><a href="#45-过高估计double-dqn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>DQN 算法在深度强化学习领域取得了不俗的成绩，不过其并不能保证一直收敛，研究表明这种估计目标价值的算法过于乐观的高估了一些情况下的行为价值（主要原因在于 <code class="language-plaintext highlighter-rouge">max</code> 操作），导致算法会将次优行为价值一致认为最优行为价值，最终不能收敛至最佳价值函数。</p>

<p>为了解决这个问题，Double Q-learning 率先使用了两个值函数进行解耦，其互相随机的更新两个值函数，并利用彼此的经验去更新网络权重 $\theta$ 和 $\theta_T$。对应的损失函数为</p>

\[L(\theta) = \mathbb{E}[(r+\gamma \textcolor{blue}{q(s^\prime,\textcolor{red}{\mathop{\text{argmax}}\limits_{a^\prime}q(s^\prime,a^\prime,\theta)};\theta_T)}-q(s,a;\theta))^2]\]

<p>其可拆分为如下两个等式</p>

\[\begin{aligned}
  L(\theta) &amp;= \mathbb{E}[(r+\gamma \textcolor{blue}{q(s^\prime,\textcolor{red}{a^\prime};\theta_T)}-q(s,a;\theta))^2]\\
  \textcolor{red}{a^\prime} &amp;=\mathop{\text{argmax}}\limits_{a^\prime}q(s^\prime,a^\prime,\theta)
\end{aligned}\]

<p>其网络示意图如下：</p>

<p><a href="/assets/img/postsimg/20230102/DDQN.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/DDQN.jpg" alt="DDQN" class="lazyload" data-proofer-ignore></a></p>

<p>带来的效果如下：</p>

<p><a href="/assets/img/postsimg/20230102/DDQN-benefit.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20230102/DDQN-benefit.jpg" alt="DDQN-benefit" class="lazyload" data-proofer-ignore></a></p>

</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw me-1"></i>
    
      <a href='/categories/academic/'>Academic</a>,
      <a href='/categories/knowledge/'>Knowledge</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw me-1"></i>
      
      <a href="/tags/artificial-intelligence/"
          class="post-tag no-text-decoration" >artificial intelligence</a>
      
      <a href="/tags/reinforcement-learning/"
          class="post-tag no-text-decoration" >reinforcement learning</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        本文由作者按照 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         进行授权

      
    </div>

    <!-- Post sharing snippet -->

<div class="share-wrapper">
  <span class="share-label text-muted me-1">分享</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%EF%BC%89%20-%20SIRLIS&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Value-Approximation%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%EF%BC%89%20-%20SIRLIS&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Value-Approximation%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Value-Approximation%2F&text=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%EF%BC%89%20-%20SIRLIS"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <i
      id="copy-link"
      class="fa-fw fas fa-link small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="分享链接"
      data-title-succeed="链接已复制！"
    >
    </i>
  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
      
    </div>
  </div>
  <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
    <div class="access">
      <!-- Get the last 5 posts from lastmod list. -->














  <div id="access-lastmod" class="post">
    <div class="panel-heading">最近更新</div>
    <ul class="post-content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Unsupervised-Classifier/">模式识别（无监督分类器）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Linear-Classifier/">模式识别（线性分类器）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-LDA&PCA/">模式识别（LDA和PCA）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Feature-Selection-and-Extraction/">模式识别（特征选择与特征提取）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Policy-Gradient/">强化学习（策略梯度法）</a>
        </li>
      
    </ul>
  </div>
  <!-- #access-lastmod -->


      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pattern-recognition/">pattern recognition</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
    </div>
  </div>


    </div>

    
      
      



  <div id="toc-wrapper" class="ps-0 pe-4 mb-5">
    <div class="panel-heading ps-3 pt-2 mb-2">文章内容</div>
    <nav id="toc"></nav>
  </div>


    
  </div>
</div>

<!-- tail -->

  <div class="row">
    <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-3 pe-xl-4 mt-5">
      
        
        <!--
  Recommend the other 3 posts according to the tags and categories of the current post,
  if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->








  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  






<!-- Fill with the other newlest posts -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ms-1" data-toc-skip>
      相关文章
    </h3>
    <div class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Policy-Gradient/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1701074599"
  data-df="YYYY/MM/DD"
  
>
  2023/11/27
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（策略梯度法）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的策略梯度法（Policy Gradient）。






  1. 回顾基于价值的强化学习
  2. 策略梯度
    
      2.1. 策略函数
      2.2. 策略函数的分布形式
      2.3. 策略梯度
        
          2.3.1. 回顾价值梯度
          2.3.2. 策略梯度
        
      
 ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-markov-process/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1667968579"
  data-df="YYYY/MM/DD"
  
>
  2022/11/09
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（马尔可夫决策过程）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的基本概念和模型，主要包括马尔可夫过程、马尔可夫奖励过程和马尔可夫决策过程。






  1. 强化学习
    
      1.1. 状态空间
        
          1.1.1. 状态
          1.1.2. 观测
        
      
      1.2. 动作空间
      1.3. 策略
    
  
  2. 马尔可夫...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Dynamic-Programming/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1669433059"
  data-df="YYYY/MM/DD"
  
>
  2022/11/26
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（动态规划）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的动态规划法（Dynamic Programming，DP），采用动态规划的思想，分别介绍策略迭代和价值迭代方法。






  1. 强化学习问题的求解
  2. 动态规划
    
      2.1. 策略迭代
        
          2.1.1. 策略评估
          2.1.2. 策略改进
          2.1.3. 算法流程
   ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
    </div>
    <!-- .card-deck -->
  </div>
  <!-- #related-posts -->


      
        
        <!-- Navigation buttons at the bottom of the post. -->

<div class="post-navigation d-flex justify-content-between">
  
    <a
      href="/posts/reinforcement-learning-Temporal-Differences/"
      class="btn btn-outline-primary"
      prompt="上一篇"
    >
      <p>强化学习（时序差分法）</p>
    </a>
  

  
    <a
      href="/posts/c-cmake-development/"
      class="btn btn-outline-primary"
      prompt="下一篇"
    >
      <p>使用CMake开发C++工程</p>
    </a>
  
</div>

      
        
        <!--  The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script src="https://utteranc.es/client.js"
        repo="sirlis/sirlis.github.io"
        issue-term="pathname"
        crossorigin="anonymous"
        async>
</script>

<script type="text/javascript">
  $(function() {
    const origin = "https://utteranc.es";
    const iframe = "iframe.utterances-frame";
    const lightTheme = "github-light";
    const darkTheme = "github-dark";
    let initTheme = lightTheme;

    if ($("html[data-mode=dark]").length > 0
        || ($("html[data-mode]").length == 0
            && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
      initTheme = darkTheme;
    }

    addEventListener("message", (event) => {
      let theme;

      /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */
      if (event.origin === origin) {
        /* page initial */
        theme = initTheme;

      } else if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {
        /* global theme mode changed */
        const mode = event.data.message;
        theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme);

      } else {
        return;
      }

      const message = {
        type: "set-theme",
        theme: theme
      };

      const utterances = document.querySelector(iframe).contentWindow;
      utterances.postMessage(message, origin);
    });

  });
</script>



      
    </div>
  </div>


        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 post-content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pattern-recognition/">pattern recognition</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>
    </div>

    <!-- The Footer -->

<footer>
  <div class="container px-lg-4">
    <div class="d-flex justify-content-center align-items-center text-muted mx-md-3">
      <p>本站采用 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 主题 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
      </p>

      <p>©
        2025
        <a href="https://github.com/sirlis">sirlis</a>.
        
          <span
            data-bs-toggle="tooltip"
            data-bs-placement="top"
            title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。"
          >保留部分权利。</span>
        
      </p>
    </div>
  </div>
</footer>


    <div id="mask"></div>

    <button id="back-to-top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow">
      <i class="fas fa-angle-up"></i>
    </button>

    
      <div
        id="notification"
        class="toast"
        role="alert"
        aria-live="assertive"
        aria-atomic="true"
        data-bs-animation="true"
        data-bs-autohide="false"
      >
        <div class="toast-header">
          <button
            type="button"
            class="btn-close ms-auto"
            data-bs-dismiss="toast"
            aria-label="Close"
          ></button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="px-2 mb-3">发现新版本的内容。</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            更新
          </button>
        </div>
      </div>
    

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.0/dist/jquery.min.js,npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.7/dayjs.min.js,npm/dayjs@1.11.7/locale/zh.min.js,npm/dayjs@1.11.7/plugin/relativeTime.min.js,npm/dayjs@1.11.7/plugin/localizedFormat.min.js,npm/tocbot@4.21.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '<div class="px-1 px-sm-2 px-lg-4 px-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

