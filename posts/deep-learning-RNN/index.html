<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="zh-CN" 
  
>
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="深度学习基础（RNN）" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="本文介绍了 RNN （循环神经网络）的基本概念，以及正/反向传播的推导过程。最后分析了 RNN 的梯度消失和梯度爆炸问题。" />
<meta property="og:description" content="本文介绍了 RNN （循环神经网络）的基本概念，以及正/反向传播的推导过程。最后分析了 RNN 的梯度消失和梯度爆炸问题。" />
<link rel="canonical" href="http://localhost:4000/posts/deep-learning-RNN/" />
<meta property="og:url" content="http://localhost:4000/posts/deep-learning-RNN/" />
<meta property="og:site_name" content="SIRLIS" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-29T09:43:19+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="深度学习基础（RNN）" />
<meta name="twitter:site" content="@none" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-09-29T09:43:19+08:00","datePublished":"2020-09-29T09:43:19+08:00","description":"本文介绍了 RNN （循环神经网络）的基本概念，以及正/反向传播的推导过程。最后分析了 RNN 的梯度消失和梯度爆炸问题。","headline":"深度学习基础（RNN）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/deep-learning-RNN/"},"url":"http://localhost:4000/posts/deep-learning-RNN/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>深度学习基础（RNN） | SIRLIS
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SIRLIS">
<meta name="application-name" content="SIRLIS">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.0/dist/tocbot.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/head.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <div class="site-title">
      <a href="/">SIRLIS</a>
    </div>
    <div class="site-subtitle fst-italic">分享科研和生活的日常</div>
  </div>
  <!-- .profile-wrapper -->

  <ul class="nav flex-column flex-grow-1 w-100 ps-0">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home"></i>
        <span>首页</span>
      </a>
    </li>
    <!-- the real tabs -->
    
      <li class="nav-item">
        <a href="/categories/" class="nav-link">
          <i class="fa-fw fas fa-stream"></i>
          

          <span>分类</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/tags/" class="nav-link">
          <i class="fa-fw fas fa-tags"></i>
          

          <span>标签</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/archives/" class="nav-link">
          <i class="fa-fw fas fa-archive"></i>
          

          <span>归档</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/about/" class="nav-link">
          <i class="fa-fw fas fa-info-circle"></i>
          

          <span>关于</span>
        </a>
      </li>
      <!-- .nav-item -->
    
  </ul>
  <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/sirlis"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/none"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['lihongjue','buaa.edu.cn'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</div>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container px-xxl-5">
        <!-- The Top Bar -->

<div id="topbar-wrapper">
  <div
    id="topbar"
    class="container d-flex align-items-center justify-content-between h-100"
  >
    <span id="breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                首页
              </a>
            </span>

          
        
          
        
          
            
              <span>深度学习基础（RNN）</span>
            

          
        
      
    </span>
    <!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      文章
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="搜索..."
      >
    </span>
    <span id="search-cancel">取消</span>
  </div>
</div>

        











<div class="row">
  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pe-xl-4">
    

    <div class="post px-1 px-md-2">
      

      
        
      
        <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->



  
  

  <!-- CDN URL -->
  

  <!-- Add image path -->
  

  
    
      
      
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  

  
  

  

  
  

  




<!-- return -->




<h1 data-toc-skip>深度学习基础（RNN）</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      发表于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1601343799"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2020/09/29
</em>

    </span>

    <!-- lastmod date -->
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      作者

      <em>
      
        <a href="https://github.com/sirlis">sirlis</a>
      
      </em>
    </span>

    <div>
      <!-- read time -->
      <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="4538 字"
>
  <em>25 分钟</em>阅读</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <p>本文介绍了 RNN （循环神经网络）的基本概念，以及正/反向传播的推导过程。最后分析了 RNN 的梯度消失和梯度爆炸问题。</p>

<!--more-->

<hr />

<ul>
  <li><a href="#1-rnn">1. RNN</a>
    <ul>
      <li><a href="#11-概念">1.1. 概念</a></li>
      <li><a href="#12-模型">1.2. 模型</a></li>
      <li><a href="#13-前向传播">1.3. 前向传播</a></li>
      <li><a href="#14-反向传播">1.4. 反向传播</a></li>
      <li><a href="#15-梯度消失">1.5. 梯度消失</a></li>
      <li><a href="#16-梯度爆炸">1.6. 梯度爆炸</a></li>
    </ul>
  </li>
  <li><a href="#2-参考文献">2. 参考文献</a></li>
</ul>

<h1 id="1-rnn">1. RNN</h1>

<h2 id="11-概念"><span class="me-2">1.1. 概念</span><a href="#11-概念" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>在前馈神经网络中，信息传递是单向的[<a href="#ref1">1</a>]。前馈神经网络可以看作一个复杂的函数，每次输入都是独立的，即网络的输出只依赖于当前的输入．但是在很多现实任务中，网络的输出不仅和当前时刻的输入相关，也和其过去一段时间的输出相关。此外，前馈网络难以处理时序数据，比如视频、语音、文本等．时序数据的长度一般是不固定的，而前馈神经网络要求输入和输出的维数都是固定的，不能任意改变．因此，当处理这一类和时序数据相关的问题时，就需要一种能力更强的模型．</p>

<p>循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络．在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构．和前馈神经网络相比，循环神经网络更加符合生物神经网络的结构．循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上。</p>

<p>RNN 用于分类的简单例子参考：</p>

<blockquote>
  <p>根据人名判断国家：字符级 RNN 对单词分类。输入人名单词，将每个字符进行编码后读取（one-hot），在每一步输出预测和“隐藏状态”，将其先前的隐藏状态输入至下一时刻。将最终时刻输出作为预测结果，即表示该词属于哪个类（哪个国家的人名）。</p>
</blockquote>

<blockquote>
  <p>根据句子判断语言：单词级 RNN 对句子分类。输入一个句子，将每个单词进行编码后读取（word2vec），在每一步输出预测和“隐藏状态”，将其先前的隐藏状态输入至下一时刻。将最终时刻输出作为预测结果，即表示该词属于哪个类（哪个国家的语言）。</p>
</blockquote>

<p>上述例子中，输入是对 字符 / 单词 序列的编码，编码方式可以是 one-hot 形式也可以采用 word2vec 方法。期望输出一般是一个 one-hot 向量，比如 10 种国家 / 语言，预测输出一般是一个多分类概率结果。</p>

<h2 id="12-模型"><span class="me-2">1.2. 模型</span><a href="#12-模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>循环神经网络（Recurrent Neural Network，RNN）通过使用带自反馈的神经元，能够处理任意长度的时序数据。</p>

<p>给定一个输入序列 $\boldsymbol x_{1:T} = (\boldsymbol x_1,…,\boldsymbol x_t,…,\boldsymbol x_T)$，通过下面的公式更新隐层活性值 $\boldsymbol h_t$：</p>

\[\boldsymbol h_t = f(\boldsymbol h_{t-1},\boldsymbol x_t)\]

<p>其中，$\boldsymbol h_0 = 0$，$f(\cdot)$ 是非线性函数，可以是一个前馈网络。</p>

<p>网络结构如下图所示</p>

<p><a href="/assets/img/postsimg/20200929/1.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20200929/1.jpg" alt="rnn" class="lazyload" data-proofer-ignore></a></p>

<p>从数学上讲，上述公式可以堪称一个动力系统，因此隐层活性值在很多文献中也称为隐状态（hidden state）。</p>

<p>由于循环神经网络具有短期记忆能力，因此其计算能力十分强大，可以近似任意非线性动力系统（程序），相比较而言，前馈神经网络可以模拟任何连续函数。</p>

<h2 id="13-前向传播"><span class="me-2">1.3. 前向传播</span><a href="#13-前向传播" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>如果我们把每个时刻的状态都看作前馈神经网络的一层，循环神经网络可以看作在时间维度上权值共享的神经网络。一个简单的循环神经网络按时间展开后如下图所示</p>

<p><a href="/assets/img/postsimg/20200929/2.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20200929/2.jpg" alt="rnn" class="lazyload" data-proofer-ignore></a></p>

<p>令 $\boldsymbol x_t \in \mathbb R^M$ 为 $t$ 时刻的网络输入向量，则在该时刻的网络隐状态 $\boldsymbol h_t \in \mathbb R^D$ 和网络输出 $\boldsymbol y_t \in \mathbb R^N$ 的更新公式为</p>

\[\begin{aligned}
\boldsymbol h_t &amp;= f(\boldsymbol U \boldsymbol h_{t-1} + \boldsymbol W \boldsymbol x_t + \boldsymbol b)\\
\boldsymbol y_t &amp;= g(\boldsymbol V \boldsymbol h_t + \boldsymbol c)
\end{aligned}\]

<p>其中 $\boldsymbol U \in \mathbb R^{D\times D}$ 是状态-状态权重矩阵，$\boldsymbol W \in \mathbb R^{D\times M}$ 是状态-输入权重矩阵，$\boldsymbol b \in \mathbb R^D, \boldsymbol c \in \mathbb R^N$ 是偏置向量，$\boldsymbol V \in \mathbb R^{N\times D}$ 是状态-输出权重矩阵，$f(\cdot)$ 是激活函数，如 $sigmoid$ 或 $tanh$ 函数，$g(\cdot)$ 也是激活函数，如 $softmax$ 或 $purlin$ 函数。</p>

<p>注意到，第二个方程的具体形式与模型的具体使用方式有关，比如其中的常数项 $\boldsymbol c$ 的有无，激活函数 $g(\cdot)$ 的选取等。</p>

<h2 id="14-反向传播"><span class="me-2">1.4. 反向传播</span><a href="#14-反向传播" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>有了RNN前向传播算法的基础，就容易推导出RNN反向传播算法的流程了。RNN 反向传播算法的思路和 DNN 是一样的，即通过梯度下降法一轮轮的迭代，得到合适的RNN模型参数 $\boldsymbol U,\boldsymbol W,\boldsymbol V,\boldsymbol b,\boldsymbol c$。由于我们是<strong>基于时间反向传播</strong>，所以 RNN 的反向传播有时也叫做 <strong>BPTT</strong> (back-propagation through time)。当然这里的 BPTT 和 DNN 的 BP 也有很大的不同点，即这里所有的 $\boldsymbol U, \boldsymbol W,\boldsymbol V,\boldsymbol b,\boldsymbol c$ 在序列的各个位置是共享的，反向传播时我们更新的是相同的参数。</p>

<p><a href="/assets/img/postsimg/20200929/5.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20200929/5.jpg" alt="rnn" class="lazyload" data-proofer-ignore></a></p>

<p>RNN反向传播过程中，需要计算 $\boldsymbol U,\boldsymbol W,\boldsymbol V,\boldsymbol b,\boldsymbol c$ 等参数的梯度。清晰起见，我们将前向传播过程整理如下</p>

\[\begin{aligned}
\boldsymbol a_t &amp;= \boldsymbol W \boldsymbol h_{t-1} + \boldsymbol U \boldsymbol x_t + \boldsymbol b\\
\boldsymbol h_t &amp;= f(\boldsymbol a_t)\\
\boldsymbol o_t &amp;= \boldsymbol V \boldsymbol h_t + \boldsymbol c\\
\hat{\boldsymbol y_t} &amp;= g(\boldsymbol o_t)
\end{aligned}\]

<p>反向传播的形象的分析如下图所示。途中绿线是正向传播过程，红线是反向传播过程。可以看出，在输出端的 $V,c$ 参数仅与 $t$ 时刻的反向传播通路有关，因此分别求导数后求和即可。而输入端 $U,W,b$ 参数的梯度受到两个反向传播通路的影响，分别是 $t$ 时刻的输出端反向通路，以及 $t+1$ 时刻隐层信息的反向通路。</p>

<p><a href="/assets/img/postsimg/20200929/6.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20200929/6.jpg" alt="rnn" class="lazyload" data-proofer-ignore></a></p>

<p>为了简化描述，这里的损失函数我们为<a href="https://zhuanlan.zhihu.com/p/38241764">交叉熵损失函数</a>，输出的激活函数 $g(\cdot)$ 为 softmax 函数。对于 RNN，由于在序列的每个位置（任意 $t$ 时刻）都有输出 $\hat y_t$，也即都有损失函数，因此最终损失 $L$ 为</p>

<!-- $$
\boldsymbol L = \sum_{t=1}^T \boldsymbol L_t = \sum_{t=1}^T \left[ - (\boldsymbol y_t ln\hat \boldsymbol y_t +(\boldsymbol 1_i-\boldsymbol y_t)ln(\boldsymbol 1_i-\hat \boldsymbol y_t) ) \right]
$$

其中, $\boldsymbol 1_i$ 表示第 $i$ 维为1，其余维为0的向量。 -->

\[\boldsymbol L = \sum_{t=1}^T \boldsymbol L_t = \sum_{t=1}^T \left[ - \boldsymbol y_t ln\hat{\boldsymbol y_t} \right]\]

<p>注意到，对于任意时刻 $t$ 的损失函数 $\boldsymbol L_t$，在 $N$ 分类问题中其与每个维度分量均有关，因此损失函数可以进一步写为</p>

\[\boldsymbol L = \sum_{t=1}^T \boldsymbol L_t = -\sum_{t=1}^T\sum_{j=1}^N y_{tj}{\rm ln} \hat{y_{tj}}\]

<p>上式就是负对数似然函数的形式。</p>

<p>首先计算比较简单的 $V,c$ 的梯度。在输出端的 $V,c$ 参数仅与 $t$ 时刻的反向传播通路有关，因此分别求导数后求和即可，有<sup>[<a href="#ref1">1</a>]</sup></p>

<!-- $$
\begin{aligned}
\frac{\partial L}{\partial \boldsymbol c} &= \sum_{t=1}^T \frac{\partial \boldsymbol L_t}{\partial \boldsymbol c}
= \sum_{t=1}^T \frac{\partial \boldsymbol L_t}{\partial \hat \boldsymbol y_t} \frac{\partial \hat \boldsymbol y_t}{\partial \boldsymbol o_t} \frac{\partial \boldsymbol o_t}{\partial \boldsymbol c}\\
&= \sum_{t=1}^T -(\frac{\boldsymbol y_t}{\hat \boldsymbol y_t}-\frac{\boldsymbol 1_i-\boldsymbol y_t}{\boldsymbol 1_i-\hat \boldsymbol y_t})softmax'\cdot \boldsymbol I\\
&= \sum_{t=1}^T -(\frac{\boldsymbol y_t}{\hat \boldsymbol y_t}-\frac{\boldsymbol 1_i-\boldsymbol y_t}{\boldsymbol 1_i-\hat \boldsymbol y_t})\cdot \hat \boldsymbol y_t(\boldsymbol 1_i -\hat \boldsymbol y_t)\\
&= \sum_{t=1}^T (\hat \boldsymbol y_t-\boldsymbol y_t)\\
\frac{\partial L}{\partial V} &= \sum_{t=1}^T \frac{\partial L_t}{\partial c}
= \sum_{t=1}^T \frac{\partial \boldsymbol L_t}{\partial \hat \boldsymbol y_t} \frac{\partial \hat \boldsymbol y_t}{\partial \boldsymbol o_t} \frac{\partial \boldsymbol o_t}{\partial \boldsymbol V}\\
&= \sum_{t=1}^T (\hat \boldsymbol y_t-\boldsymbol y_t)\boldsymbol h_t
\end{aligned}
$$ -->

\[\begin{aligned}
\frac{\partial L}{\partial \boldsymbol c} &amp;= \sum_{t=1}^T \frac{\partial \boldsymbol L_t}{\partial \boldsymbol c}
= \sum_{t=1}^T \frac{\partial \boldsymbol L_t}{\partial \hat{\boldsymbol y_t}} \frac{\partial \hat{\boldsymbol y_t}}{\partial \boldsymbol o_t} \frac{\partial \boldsymbol o_t}{\partial \boldsymbol c}\\
&amp;= \sum_{t=1}^T -(\frac{\boldsymbol y_t}{\hat{\boldsymbol y_t}})\cdot softmax'\cdot \boldsymbol I\\
\end{aligned}\]

<p>由于 $softmax’$ 需要分情况讨论，当 $j=i$ 时 $softmax’ = \hat y_j(1-\hat y_j)$；当 $j\neq i$ 时 $softmax’ = \hat y_j\hat y_i$，那么有</p>

\[\begin{aligned}
\frac{\partial L}{\partial \boldsymbol c}
&amp;= \sum_{t=1}^T \sum_{j=1}^N -\frac{y_{tj}}{\hat y_{tj}}\cdot
\left\{\begin{matrix}
&amp;\hat y_{tj}(1-\hat y_{tj})\quad &amp;j=i\\ 
&amp;\hat y_{tj}\hat y_{ti} \quad &amp; j\neq i
\end{matrix}\right.\\
&amp;= - \sum_{t=1}^T \sum_{j=1}^N 
\left\{\begin{matrix}
&amp;y_{tj}(1-\hat y_{tj})\quad &amp;j=i\\ 
&amp;y_{tj}\hat y_{ti} \quad &amp; j\neq i
\end{matrix}\right.\\
\end{aligned}\]

<p>由于 $\boldsymbol y_{t} = [y_{t1},y_{t2},…,y_{tj},…,y_{tN}]$ 是一个 one-hot 向量，假设第 $j$ 个分量 $y_{tj} = 1$，可以将第二个累加符号消去（因为其它分量为 0，不影响累加求和）</p>

\[\begin{aligned}
\frac{\partial L}{\partial \boldsymbol c}
&amp;= - \sum_{t=1}^T \left\{\begin{matrix}
&amp;[0,0,...,y_{tj}(1-\hat y_{tj}),...,0]\quad &amp;j=i\\ 
&amp;[0,0,...,y_{tj}\hat y_{ti},...,0] \quad &amp; j\neq i
\end{matrix}\right.\quad (y_{tj} = 1)
\end{aligned}\]

<p>进一步分析</p>

<ul>
  <li>$j=i:\quad y_{tj}(1-\hat y_{tj}) = 1-\hat y_{tj} = y_{tj} - \hat{y_{tj}}$</li>
  <li>$j\neq i:\quad y_{tj}\hat y_{ti} = \hat y_{ti} = y_{tj} - \hat{y_{tj}}$</li>
</ul>

<p>可以发现，二者在形式上可以写成统一的形式 $\boldsymbol y_{tj} - \hat {\boldsymbol y_{tj}}$，那么有</p>

\[\begin{aligned}
\frac{\partial L}{\partial \boldsymbol c} = - \sum_{t=1}^T (\boldsymbol y_{tj} - \hat {\boldsymbol y_{tj}}) =\sum_{t=1}^T (\hat {\boldsymbol y_{tj}} - \boldsymbol y_{tj})
\end{aligned}\]

<p>那么对 $\boldsymbol V$ 的偏导为</p>

\[\begin{aligned}
\frac{\partial L}{\partial V} &amp;= \sum_{t=1}^T \frac{\partial L_t}{\partial c}
= \sum_{t=1}^T \frac{\partial \boldsymbol L_t}{\partial \hat {\boldsymbol y_t}} \frac{\partial \hat {\boldsymbol y_t}}{\partial \boldsymbol o_t} \frac{\partial \boldsymbol o_t}{\partial \boldsymbol V}\\
&amp;= \sum_{t=1}^T (\hat {\boldsymbol y_t}-\boldsymbol y_t)\boldsymbol h_t
\end{aligned}\]

<p>$\boldsymbol U,\boldsymbol W,\boldsymbol b$ 的梯度计算就比较复杂了，误差传播源来自于<strong>两个反向传播通路</strong>的方向，分别是 $t$ 时刻的输出端反向通路，以及 $t+1$ 时刻隐层信息的反向通路。这里假设隐藏层的激活函数 $f(\cdot)$ 为 tanh 函数。</p>

<p>在进一步求解前，首先要考虑矩阵对向量求导的布局。根据布局约定（layout conventions），谁是列向量就是什么布局<sup>[<a href="#ref2">2</a>]</sup>：</p>

<ul>
  <li>分子布局（numerator layout）： 分子为列向量且分母为行向量</li>
  <li>分母布局（denominator layout）：分子为行向量且分母为列向量</li>
</ul>

<p>二者使用完全依据习惯而定，二者结果之间差一个转置。<a href="https://www.zhihu.com/question/352174717">这里</a>讨论了两种布局下的优劣。</p>

<!-- ![layout](/assets/img/postsimg/20200929/2.5.jpg) -->

<p>如果我们采用分母布局（<a href="https://www.pianshen.com/article/5516168061/">点此参考</a>），即分母保持列向量，分子按行向量展开，那么有</p>

\[\begin{aligned}
\frac{\partial \boldsymbol {Ax}}{\partial \boldsymbol x} &amp;= \partial
\begin{bmatrix}
A_{11} &amp;A_{12}&amp;\cdots&amp;A_{1n}\\
A_{21} &amp;A_{22}&amp;\cdots&amp;A_{2n}\\
\vdots &amp;\vdots&amp;\ddots&amp;\vdots\\
A_{m1} &amp;A_{n2}&amp;\cdots&amp;A_{mn}\\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2\\
\vdots\\
x_n\\
\end{bmatrix} / \partial \boldsymbol x\\
&amp;= \begin{bmatrix}
\partial (A_{11}x_1 + A_{12} x_2 + \cdots + A_{1n} x_n) &amp;
\cdots &amp;
\partial (A_{m1}x_1 + A_{m2} x_2 + \cdots + A_{mn} x_n)\\
\end{bmatrix}/\partial \boldsymbol x\quad &lt;row!&gt;\\
&amp;= \begin{bmatrix}
\partial (A_{11}x_1 + A_{12} x_2 + \cdots + A_{1n} x_n)/\partial \boldsymbol x &amp;
\cdots &amp;
\partial (A_{m1}x_1 + A_{m2} x_2 + \cdots + A_{mn} x_n)/\partial \boldsymbol x\\
\end{bmatrix}\\
&amp;= \begin{bmatrix}
\partial (A_{11}x_1 + A_{12} x_2 + \cdots + A_{1n} x_n)/\partial x_1 &amp; \cdots &amp; \partial (A_{m1}x_1 + A_{m2} x_2 + \cdots + A_{mn} x_n)/\partial x_1\\
\partial (A_{11}x_1 + A_{12} x_2 + \cdots + A_{1n} x_n)/\partial x_2 &amp; \cdots &amp; \partial (A_{m1}x_1 + A_{m2} x_2 + \cdots + A_{mn} x_n)/\partial x_2\\
\vdots &amp; \cdots &amp; \vdots\\
\partial (A_{11}x_1 + A_{12} x_2 + \cdots + A_{1n} x_n)/\partial x_n &amp; \cdots &amp; \partial (A_{m1}x_1 + A_{m2} x_2 + \cdots + A_{mn} x_n)/\partial x_n\\
\end{bmatrix}\\
&amp;=\begin{bmatrix}
A_{11} &amp;A_{21}&amp;\cdots&amp;A_{m1}\\
A_{12} &amp;A_{22}&amp;\cdots&amp;A_{m2}\\
\vdots &amp;\vdots&amp;\ddots&amp;\vdots\\
A_{1n} &amp;A_{2n}&amp;\cdots&amp;A_{mn}\\
\end{bmatrix} = \boldsymbol A^T
\end{aligned}\]

<p>需要注意的是，分母布局下，求导的链式法则的顺序是反向的，假设 $\boldsymbol u = \boldsymbol u(\boldsymbol x)$，那么</p>

\[\frac{\partial \boldsymbol f(\boldsymbol g(\boldsymbol u)}{\partial \boldsymbol x}
= \frac{\partial \boldsymbol u}{\partial \boldsymbol x}
\frac{\partial \boldsymbol g}{\partial \boldsymbol u}
\frac{\partial \boldsymbol f}{\partial \boldsymbol g}\]

<p>我们先计算最后时刻 $t=T$ 的隐层梯度（分母布局链式法则方向相反）</p>

\[\frac{\partial \boldsymbol L}{\partial \boldsymbol h_T} = \frac{\partial \boldsymbol o_T}{\partial \boldsymbol h_T} \frac{\partial \boldsymbol L}{\partial \boldsymbol o_T}\]

<p>前面求 $\boldsymbol V, \boldsymbol c$ 的梯度时已经求出</p>

\[\frac{\partial \boldsymbol L}{\partial \boldsymbol o_T} = \frac{\partial \boldsymbol L}{\partial \hat {\boldsymbol y_T}}\frac{\partial \hat {\boldsymbol y_T}}{\partial \boldsymbol o_T} = \hat {\boldsymbol y_T}-\boldsymbol y_T = \nabla_{\boldsymbol o_T}\boldsymbol L\]

<p>假定 $\boldsymbol {Vh}_T$ 的结果是<strong>列</strong>向量，而 $\boldsymbol h_T$ 也是<strong>列</strong>向量，根据分母布局，有</p>

\[\frac{\partial \boldsymbol L}{\partial \boldsymbol h_T} = \frac{\partial \boldsymbol o_T}{\partial \boldsymbol h_T} \nabla_{\boldsymbol o_T}\boldsymbol L = \frac{\partial \boldsymbol {Vh}_T}{\partial \boldsymbol h_T} \nabla_{\boldsymbol o_T}\boldsymbol L= \boldsymbol V^T\nabla_{\boldsymbol o_T}\boldsymbol L\]

<p>对于 $T$ 时刻之前的任意时刻 $t$，根据迭代关系，$\boldsymbol h_t$ 与 $\boldsymbol o_t$ 和 $\boldsymbol h_{t+1}$ 均有关，即</p>

\[\begin{aligned}
\boldsymbol h_{t+1} &amp;= f(\boldsymbol a_{t+1}) =f(\boldsymbol W \boldsymbol h_t + \boldsymbol U \boldsymbol x_{t+1} + \boldsymbol b)\\
\boldsymbol o_t &amp;= \boldsymbol V \boldsymbol h_t + \boldsymbol c\\
\end{aligned}\]

<p>对两个反向通路方向的梯度求和，有（分母布局链式法则方向相反）</p>

\[\begin{aligned}
\frac{\partial \boldsymbol L}{\partial \boldsymbol h_t}&amp;=\frac{\partial \boldsymbol h_{t+1}}{\partial \boldsymbol h_t}\frac{\partial \boldsymbol L}{\partial \boldsymbol h_{t+1}} + \frac{\partial \boldsymbol o_t}{\partial \boldsymbol h_t}\frac{\partial \boldsymbol L_t}{\partial \boldsymbol o_t}\\
&amp;=\frac{\partial \boldsymbol h_{t+1}}{\partial \boldsymbol h_t}\frac{\partial \boldsymbol L}{\partial \boldsymbol h_{t+1}} + \boldsymbol V^T\nabla_{\boldsymbol o_t}\boldsymbol L
\end{aligned}\]

<p>下面考察 $\partial \boldsymbol h_t / \partial \boldsymbol a_t$，因为 $\boldsymbol h_t, \boldsymbol a_t$ 均为列向量，若采用分母布局，将分子 $\boldsymbol h_t$ 看作行向量展开，那么</p>

\[\begin{aligned}
\frac{\partial \boldsymbol h_t }{ \partial \boldsymbol a_t} &amp;= \partial[h_1,h_2,\cdots,h_D]/\partial \boldsymbol a_t\quad&lt;row!&gt;\\
&amp;=\begin{bmatrix}
\partial h_1/\partial a_1&amp;\partial h_2/\partial a_1&amp;\cdots&amp;\partial h_D/\partial a_1\\
\partial h_1/\partial a_2&amp;\partial h_2/\partial a_2&amp;\cdots&amp;\partial h_D/\partial a_2\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\partial h_1/\partial a_D&amp;\partial h_2/\partial a_D&amp;\cdots&amp;\partial h_D/\partial a_D\\
\end{bmatrix}\\
&amp;= diag(f'(a_{t}))
\end{aligned}\]

<p>其中 $diag$ 为对角线矩阵，因为下标不同的项偏导为 0 ，只有对角线元素非 0 。</p>

<p>若 $f(\cdot)$ 为 $tanh$ 函数，有 $tanh’=1-tanh^2$，那么</p>

\[\frac{\partial \boldsymbol h_{t+1}}{\partial \boldsymbol a_{t+1}} = diag(tanh'(a_{t+1})) = diag(1-tanh(a_{t+1})^2) = diag(1-h_{t+1}^2)\]

<p>那么（分母布局链式法则反向）</p>

\[\frac{\partial \boldsymbol h_{t+1}}{\partial \boldsymbol h_t} = \frac{\partial \boldsymbol a_{t+1}}{\partial \boldsymbol h_t} \frac{\partial \boldsymbol h_{t+1}}{\partial \boldsymbol a_{t+1}} = W^T diag(1-h_{t+1}^2)\]

<p>带回隐层梯度公式有</p>

\[\frac{\partial \boldsymbol L}{\partial \boldsymbol h_t} = \boldsymbol W^T diag(1- h_{t+1}^2)\cdot\frac{\partial \boldsymbol L}{\partial \boldsymbol h_{t+1}} + \boldsymbol V^T\nabla_{\boldsymbol o_t}\boldsymbol L\]

<p>稍作整理有</p>

\[\nabla_{\boldsymbol h_t}\boldsymbol L = \boldsymbol W^T diag(1- h_{t+1}^2)\cdot\nabla_{\boldsymbol h_{t+1}}\boldsymbol L + \boldsymbol V^T\nabla_{\boldsymbol o_t}\boldsymbol L\]

<p>可以看出，隐层梯度 $\nabla_{\boldsymbol h_t}\boldsymbol L$ 可以采用递归的方式求解。</p>

<p>下面即可写出 $\boldsymbol W,\boldsymbol U,\boldsymbol b$ 的梯度表达式（分母布局链式法则方向相反）</p>

\[\begin{aligned}
\frac{\partial \boldsymbol L}{\partial \boldsymbol b} &amp;= \sum_t \frac{\partial \boldsymbol a_t}{\partial \boldsymbol b}\frac{\partial \boldsymbol h_t}{\partial \boldsymbol a_t}\frac{\partial \boldsymbol L}{\partial \boldsymbol h_t} = \sum_t \boldsymbol I\cdot diag(1-h_t^2)\nabla_{\boldsymbol h_t}\boldsymbol L\\
\frac{\partial \boldsymbol L}{\partial \boldsymbol W} &amp;= \sum_t \frac{\partial \boldsymbol a_t}{\partial \boldsymbol W}\frac{\partial \boldsymbol h_t}{\partial \boldsymbol a_t}\frac{\partial \boldsymbol L}{\partial \boldsymbol h_t} = \sum_t \frac{\partial \boldsymbol a_t}{\partial \boldsymbol W}\cdot diag(1-h_t^2)\nabla_{\boldsymbol h_t}\boldsymbol L\\
\frac{\partial \boldsymbol L}{\partial \boldsymbol U} &amp;= \sum_t \frac{\partial \boldsymbol a_t}{\partial \boldsymbol U}\frac{\partial \boldsymbol h_t}{\partial \boldsymbol a_t}\frac{\partial \boldsymbol L}{\partial \boldsymbol h_t} = \sum_t \frac{\partial \boldsymbol a_t}{\partial \boldsymbol U}\cdot diag(1-h_t^2)\nabla_{\boldsymbol h_t}\boldsymbol L
\end{aligned}\]

<p>对于 $\boldsymbol W$ 和 $\boldsymbol U$，需要进一步分析 $\boldsymbol a$ 对矩阵 $\boldsymbol W, \boldsymbol U$ 的偏导。因为</p>

\[\boldsymbol a_t = \boldsymbol W \boldsymbol h_{t-1} + \boldsymbol U \boldsymbol x_t + \boldsymbol b\]

<p>以 $\boldsymbol a$ 对 $\boldsymbol W$ 的偏导为例，采用分母布局，即求导过程中的分母 $\boldsymbol W$ 保持为正常矩阵形式，而对分子中的 $\boldsymbol W \boldsymbol h$ 按行向量展开，有</p>

\[\begin{aligned}
&amp;\frac{\partial \boldsymbol a}{\partial \boldsymbol W} = \frac{\partial \boldsymbol W \boldsymbol h}{\partial \boldsymbol W}=\partial
\begin{bmatrix}
w_{11}&amp;w_{12}&amp;\cdots&amp;w_{1D}\\
w_{21}&amp;w_{22}&amp;\cdots&amp;w_{2D}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
w_{D1}&amp;w_{D2}&amp;\cdots&amp;w_{DD}
\end{bmatrix}
\begin{bmatrix}
h_1\\
h_2\\
\vdots\\
h_D
\end{bmatrix}/ \partial \boldsymbol W\\
&amp;=\begin{bmatrix}
w_{11}h_1+w_{12}h_2+\cdots+w_{1D}h_D\\
w_{21}h_1+w_{22}h_2+\cdots+w_{2D}h_D\\
\vdots\\
w_{D1}h_1+w_{D2}h_2+\cdots+w_{DD}h_D\\
\end{bmatrix}^T/ \partial \boldsymbol W \quad &lt;row!&gt;\\
&amp;=\begin{bmatrix}
(w_{11}h_1+w_{12}h_2\cdots+w_{1D}h_D)/\partial w_{11}&amp;(w_{21}h_1+w_{22}h_2\cdots+w_{2D}h_D)/\partial w_{12}&amp;\cdots&amp;(w_{D1}h_1+w_{D2}h_2\cdots+w_{DD}h_D)/\partial w_{1D}\\
(w_{11}h_1+w_{12}h_2\cdots+w_{1D}h_D)/\partial w_{21}&amp;(w_{21}h_1+w_{22}h_2\cdots+w_{2D}h_D)/\partial w_{22}&amp;\cdots&amp;(w_{D1}h_1+w_{D2}h_2\cdots+w_{DD}h_D)/\partial w_{2D}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
(w_{11}h_1+w_{12}h_2\cdots+w_{1D}h_D)/\partial w_{D1}&amp;(w_{21}h_1+w_{22}h_2\cdots+w_{2D}h_D)/\partial w_{D2}&amp;\cdots&amp;(w_{D1}h_1+w_{D2}h_2\cdots+w_{DD}h_D)/\partial w_{DD}
\end{bmatrix}\\
&amp;=\begin{bmatrix}
h_1&amp;0&amp;\cdots&amp;0\\
0&amp;h_2&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\cdots&amp;h_D
\end{bmatrix}=diag (h^T)
\end{aligned}\]

<p>对 $\boldsymbol U$ 的求导同理，最终有</p>

\[\frac{\partial \boldsymbol a_t}{\partial \boldsymbol W} = diag(h_{t-1}^T)
\quad\quad\quad \frac{\partial \boldsymbol a_t}{\partial \boldsymbol U} = diag(x_t^T)\]

<p>其中 $\boldsymbol h_t \in \mathbb R^D,\boldsymbol x_t \in \mathbb R^M$ 是列向量。</p>

<p>带入上面的$\boldsymbol W,\boldsymbol U,\boldsymbol b$ 的梯度表达式，有</p>

\[\begin{aligned}
\frac{\partial \boldsymbol L}{\partial \boldsymbol W} &amp;= \sum_t \frac{\partial \boldsymbol a_t}{\partial \boldsymbol W}\frac{\partial \boldsymbol h_t}{\partial \boldsymbol a_t}\frac{\partial \boldsymbol L}{\partial \boldsymbol h_t} = \sum_t diag (h_{t-1}^T)\cdot diag(1-h_t^2)\nabla_{\boldsymbol h_t}\boldsymbol L = \sum_t \cdot diag(1-h_t^2)\nabla_{\boldsymbol h_t}\boldsymbol L \cdot \boldsymbol h_{t-1}^T\\
\frac{\partial \boldsymbol L}{\partial \boldsymbol U} &amp;= \sum_t \frac{\partial \boldsymbol a_t}{\partial \boldsymbol U}\frac{\partial \boldsymbol h_t}{\partial \boldsymbol a_t}\frac{\partial \boldsymbol L}{\partial \boldsymbol h_t} = \sum_t diag (x^T)\cdot diag(1-h_t^2)\nabla_{\boldsymbol h_t}\boldsymbol L = \sum_t diag(1-h_t^2)\nabla_{\boldsymbol h_t}\boldsymbol L \cdot \boldsymbol x^T
\end{aligned}\]

<p>与参考链接 [<a href="#ref6">6</a>]，[<a href="#ref7">7</a>] 的结果相同。最后将 $\boldsymbol h_{t-1}, \boldsymbol x_t$ 提到末尾的操作应该是成立的，懒得推导了…… 0.0</p>

<h2 id="15-梯度消失"><span class="me-2">1.5. 梯度消失</span><a href="#15-梯度消失" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>RNN 存在时间维度上的梯度消失问题。</p>

<p>为了具体解释梯度消失的原因，首先将前面推导出来的 $t$ 时刻的 $\boldsymbol L$ 对隐层 $\boldsymbol h$ 的梯度递推过程列写如下</p>

\[\nabla_{\boldsymbol h_t}\boldsymbol L = \boldsymbol W^T diag(1- h_{t+1}^2)\cdot\nabla_{\boldsymbol h_{t+1}}\boldsymbol L + \boldsymbol V^T\nabla_{\boldsymbol o_t}\boldsymbol L\]

<p>注意到，$diag(1- h_{t+1}^2) = tanh(\boldsymbol h_{t+1})’$，那么上式可以改写为</p>

\[\nabla_{\boldsymbol h_t}\boldsymbol L = \boldsymbol W^T tanh(\boldsymbol h_{t+1})'\cdot\nabla_{\boldsymbol h_{t+1}}\boldsymbol L + \boldsymbol V^T\nabla_{\boldsymbol o_t}\boldsymbol L\]

<p>那么，对于 $t-1$ 时刻</p>

\[\begin{aligned}
\nabla_{\boldsymbol h_{t-1}}\boldsymbol L &amp;= \boldsymbol W^T tanh(h_{t})'\cdot\nabla_{\boldsymbol h_{t}}\boldsymbol L + \boldsymbol V^T\nabla_{\boldsymbol o_{t-1}}\boldsymbol L\\
&amp;=\boldsymbol W^T tanh(h_{t})'\cdot (\boldsymbol W^T tanh(\boldsymbol h_{t+1})'\nabla_{\boldsymbol h_{t+1}}\boldsymbol L + \boldsymbol V^T\nabla_{\boldsymbol o_t}\boldsymbol L) + \boldsymbol V^T\nabla_{\boldsymbol o_{t-1}}\boldsymbol L\\
&amp;=\boldsymbol W^T tanh(h_{t})'\cdot (\boldsymbol W^T tanh(\boldsymbol h_{t+1})'\nabla_{\boldsymbol h_{t+1}}\boldsymbol L + \boldsymbol V^T\nabla_{\boldsymbol o_t}\boldsymbol L) + \boldsymbol V^T\nabla_{\boldsymbol o_{t-1}}\boldsymbol L\\
&amp;=\boldsymbol W^T tanh(h_{t})'\cdot (\boldsymbol W^T tanh(\boldsymbol h_{t+1})'\cdots(\boldsymbol W^T tanh(\boldsymbol h_T)'\cdots) + \boldsymbol V^T\nabla_{\boldsymbol o_t}\boldsymbol L) + \boldsymbol V^T\nabla_{\boldsymbol o_{t-1}}\boldsymbol L\\
\end{aligned}\]

<p>稍微整理并展开，令 $\boldsymbol W^T tanh’(h) = WT$ 表示包含 $tanh’$ 的项，$\boldsymbol V^T \nabla_{\boldsymbol o}\boldsymbol L = \nabla$ 表示不含高阶次方的项，进行简化描述有（注意此处为简化描述，连乘表示为次方关系，略去了下标）</p>

\[\begin{aligned}
\nabla_{\boldsymbol h_{t-1}}\boldsymbol L &amp;= (WT)^{T-t+1}\nabla_TL+(WT)^{T-t+2}\nabla+(WT)^{T-t+3}\nabla+...+(WT)^{1}\nabla+\nabla
\end{aligned}\]

<p>而对 $tanh$ 及  $tan’h$  进行画图如下</p>

<p><a href="/assets/img/postsimg/20200929/7.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20200929/7.jpg" alt="tanh" class="lazyload" data-proofer-ignore></a></p>

<p>可以看出，由于 $tanh’$ 的值域在 $(0,1)$ 之间，对于训练过程大部分情况下 $tanh’$ 是小于 1 的。若 $\boldsymbol W^T$ 的取值较小导致 $WT = \boldsymbol W^T tanh’(h) &lt;1$，<strong>多项连乘会导致最终的值趋近于0</strong>。因此，随着 $t$ 与 $T$ 的逐渐拉大，隐层梯度中包含的 $WT$ 项的次方越来越高，其取值越来越接近 0 。这会导致较长时间 $t$ 前的梯度项 $\nabla_{\boldsymbol h_{t-1}}\boldsymbol L$ 中的第一项十分接近 0 ，即导致较长时间 $t$ 前的 $\boldsymbol L$ 对参数的梯度项仅与那个时刻的输出有关（上式中的 $\nabla$ 项），而不再包含 $\boldsymbol L$ 的信息。</p>

<p>也就是说，在时间维度上较前时刻的权重无法根据最终的 loss 信息来更新。RNN 中参数的梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。</p>

<p>注意，RNN 中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失。</p>

<p>RNN 梯度消失的本质：由于时间维度共享了参数矩阵，导致计算隐态 $\boldsymbol h_t$ 的梯度时会循环计算矩阵乘法，所以 BPTT 算法求解梯度时出现了参数矩阵的累乘，使得当时间尺度过长时的隐层梯度丢失。</p>

<h2 id="16-梯度爆炸"><span class="me-2">1.6. 梯度爆炸</span><a href="#16-梯度爆炸" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>另一方面，若 $\boldsymbol W^T$ 的取值较大，导致 $\boldsymbol W tanh(\boldsymbol h)$ 的值大于1，那么连乘的每一项均大于1，就会导致梯度爆炸现象。</p>

<p>梯度爆炸会使得学习不稳定， 参数变化太大导致无法获取最优参数。</p>

<h1 id="2-参考文献">2. 参考文献</h1>

<p><span id="ref1">[1]</span>  刘建平Pinard. <a href="https://www.cnblogs.com/pinard/p/6509630.html">循环神经网络(RNN)模型与前向反向传播算法</a>.</p>

<p><span id="ref2">[2]</span>  维基百科. <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions">矩阵微积分-布局约定</a>.</p>

<p><span id="ref3">[3]</span> 仙守. <a href="https://blog.csdn.net/shouhuxianjian/article/details/46669365">数学-矩阵计算（4）两种布局</a>.</p>

<p><span id="ref4">[4]</span> 谓之小一. <a href="https://zhuanlan.zhihu.com/p/136223550">LSTM如何解决RNN带来的梯度消失问题</a>.</p>

<p><span id="ref5">[5]</span> thinkando. <a href="https://www.jianshu.com/p/2da10b181c59">机器学习中的矩阵、向量求导</a>.</p>

<p><span id="ref6">[6]</span> Leo蓝色. <a href="https://www.jianshu.com/p/43b7a927ae34">RNN正向及反向传播</a>.</p>

<p><span id="ref7">[7]</span> 小米粥. <a href="https://zhuanlan.zhihu.com/p/90297737">RNN的反向传播-BPTT</a>.</p>

</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw me-1"></i>
    
      <a href='/categories/academic/'>Academic</a>,
      <a href='/categories/knowledge/'>Knowledge</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw me-1"></i>
      
      <a href="/tags/deep-learning/"
          class="post-tag no-text-decoration" >deep learning</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        本文由作者按照 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         进行授权

      
    </div>

    <!-- Post sharing snippet -->

<div class="share-wrapper">
  <span class="share-label text-muted me-1">分享</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88RNN%EF%BC%89%20-%20SIRLIS&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fdeep-learning-RNN%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88RNN%EF%BC%89%20-%20SIRLIS&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fdeep-learning-RNN%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fdeep-learning-RNN%2F&text=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88RNN%EF%BC%89%20-%20SIRLIS"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <i
      id="copy-link"
      class="fa-fw fas fa-link small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="分享链接"
      data-title-succeed="链接已复制！"
    >
    </i>
  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
      
    </div>
  </div>
  <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
    <div class="access">
      <!-- Get the last 5 posts from lastmod list. -->














  <div id="access-lastmod" class="post">
    <div class="panel-heading">最近更新</div>
    <ul class="post-content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Value-Approximation/">强化学习（值函数近似）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-markov-process/">强化学习（马尔可夫决策过程）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Monte-Carlo/">强化学习（蒙特卡洛法）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Temporal-Differences/">强化学习（时序差分法）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/vscode-c/">VSCode部署C/C++开发环境</a>
        </li>
      
    </ul>
  </div>
  <!-- #access-lastmod -->


      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>

    
      
      



  <div id="toc-wrapper" class="ps-0 pe-4 mb-5">
    <div class="panel-heading ps-3 pt-2 mb-2">文章内容</div>
    <nav id="toc"></nav>
  </div>


    
  </div>
</div>

<!-- tail -->

  <div class="row">
    <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-3 pe-xl-4 mt-5">
      
        
        <!--
  Recommend the other 3 posts according to the tags and categories of the current post,
  if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->








  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  






<!-- Fill with the other newlest posts -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ms-1" data-toc-skip>
      相关文章
    </h3>
    <div class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        
        
        <div class="col">
          <a href="/posts/deep-learning-basic-hp-and-opt/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1592036659"
  data-df="YYYY/MM/DD"
  
>
  2020/06/13
</em>

              <h4 class="pt-0 my-2" data-toc-skip>深度学习基础（基本超参数和优化器）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了深度学习中的基本概念，包括 batch、epoch、iteration、optimizer等，其中优化器包括 BGD、SGD、Adam等，为后深度学习提供基础。






  1. 基本超参数
    
      1.1. epoch
      1.2. batch \&amp;amp; batch_size
      1.3. iteration
    
  
  2. 优化器...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/deep-learning-basic-conv2d/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1592123059"
  data-df="YYYY/MM/DD"
  
>
  2020/06/14
</em>

              <h4 class="pt-0 my-2" data-toc-skip>深度学习基础（PyTorch的CNN组成）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了深度学习中，卷积网络的基本知识，包括2d卷积层、池化层、线性层、softmax 激活函数、交叉熵损失函数等，并结合它们在 Pytorch 中的定义和实现进行说明。






  1. 层
    
      1.1. Conv2d
        
          1.1.1. dilation
          1.1.2. padding
        
     ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/deep-learning-basic-dataloader/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1592295859"
  data-df="YYYY/MM/DD"
  
>
  2020/06/16
</em>

              <h4 class="pt-0 my-2" data-toc-skip>深度学习基础（PyTorch的数据集）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了 Pytorch 中针对计算机视觉方面的基本数据库类Dataset，基本的手写数字数据库MNIST，以及数据库加载函数 DataLoader。






  1. torchvision
  2. Dataset
    
      2.1. 默认类
      2.2. 自定义类
    
  
  3. DataLoader
  4. MNIST
  5. 参考文献


1...
                </p>
              </div>
            </div>
          </a>
        </div>
      
    </div>
    <!-- .card-deck -->
  </div>
  <!-- #related-posts -->


      
        
        <!-- Navigation buttons at the bottom of the post. -->

<div class="post-navigation d-flex justify-content-between">
  
    <a
      href="/posts/deep-learning-FDNN/"
      class="btn btn-outline-primary"
      prompt="上一篇"
    >
      <p>深度学习文章阅读（FDNN）</p>
    </a>
  

  
    <a
      href="/posts/deep-learning-LSTM/"
      class="btn btn-outline-primary"
      prompt="下一篇"
    >
      <p>深度学习基础（LSTM）</p>
    </a>
  
</div>

      
        
        <!--  The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script src="https://utteranc.es/client.js"
        repo="sirlis/sirlis.github.io"
        issue-term="pathname"
        crossorigin="anonymous"
        async>
</script>

<script type="text/javascript">
  $(function() {
    const origin = "https://utteranc.es";
    const iframe = "iframe.utterances-frame";
    const lightTheme = "github-light";
    const darkTheme = "github-dark";
    let initTheme = lightTheme;

    if ($("html[data-mode=dark]").length > 0
        || ($("html[data-mode]").length == 0
            && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
      initTheme = darkTheme;
    }

    addEventListener("message", (event) => {
      let theme;

      /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */
      if (event.origin === origin) {
        /* page initial */
        theme = initTheme;

      } else if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {
        /* global theme mode changed */
        const mode = event.data.message;
        theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme);

      } else {
        return;
      }

      const message = {
        type: "set-theme",
        theme: theme
      };

      const utterances = document.querySelector(iframe).contentWindow;
      utterances.postMessage(message, origin);
    });

  });
</script>



      
    </div>
  </div>


        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 post-content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>
    </div>

    <!-- The Footer -->

<footer>
  <div class="container px-lg-4">
    <div class="d-flex justify-content-center align-items-center text-muted mx-md-3">
      <p>本站采用 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 主题 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
      </p>

      <p>©
        2025
        <a href="https://github.com/sirlis">sirlis</a>.
        
          <span
            data-bs-toggle="tooltip"
            data-bs-placement="top"
            title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。"
          >保留部分权利。</span>
        
      </p>
    </div>
  </div>
</footer>


    <div id="mask"></div>

    <button id="back-to-top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow">
      <i class="fas fa-angle-up"></i>
    </button>

    
      <div
        id="notification"
        class="toast"
        role="alert"
        aria-live="assertive"
        aria-atomic="true"
        data-bs-animation="true"
        data-bs-autohide="false"
      >
        <div class="toast-header">
          <button
            type="button"
            class="btn-close ms-auto"
            data-bs-dismiss="toast"
            aria-label="Close"
          ></button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="px-2 mb-3">发现新版本的内容。</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            更新
          </button>
        </div>
      </div>
    

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.0/dist/jquery.min.js,npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.7/dayjs.min.js,npm/dayjs@1.11.7/locale/zh.min.js,npm/dayjs@1.11.7/plugin/relativeTime.min.js,npm/dayjs@1.11.7/plugin/localizedFormat.min.js,npm/tocbot@4.21.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '<div class="px-1 px-sm-2 px-lg-4 px-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

