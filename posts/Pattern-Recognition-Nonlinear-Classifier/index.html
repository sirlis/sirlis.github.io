<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="zh-CN" 
  
>
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="模式识别（非线性分类器）" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="很多情况下，类别之间的分类边界并不是线性的，一种更好的选择是使用更复杂的非线性函数来描述分类。本文介绍了模式识别常用的非线性分类器，主要包括近邻法分类器（NN、KNN）、支持向量机（SVM）、决策树（DT），最后介绍分类器的集成。" />
<meta property="og:description" content="很多情况下，类别之间的分类边界并不是线性的，一种更好的选择是使用更复杂的非线性函数来描述分类。本文介绍了模式识别常用的非线性分类器，主要包括近邻法分类器（NN、KNN）、支持向量机（SVM）、决策树（DT），最后介绍分类器的集成。" />
<link rel="canonical" href="http://localhost:4000/posts/Pattern-Recognition-Nonlinear-Classifier/" />
<meta property="og:url" content="http://localhost:4000/posts/Pattern-Recognition-Nonlinear-Classifier/" />
<meta property="og:site_name" content="SIRLIS" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-03-30T21:34:19+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="模式识别（非线性分类器）" />
<meta name="twitter:site" content="@none" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-02T21:23:17+08:00","datePublished":"2025-03-30T21:34:19+08:00","description":"很多情况下，类别之间的分类边界并不是线性的，一种更好的选择是使用更复杂的非线性函数来描述分类。本文介绍了模式识别常用的非线性分类器，主要包括近邻法分类器（NN、KNN）、支持向量机（SVM）、决策树（DT），最后介绍分类器的集成。","headline":"模式识别（非线性分类器）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/Pattern-Recognition-Nonlinear-Classifier/"},"url":"http://localhost:4000/posts/Pattern-Recognition-Nonlinear-Classifier/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>模式识别（非线性分类器） | SIRLIS
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SIRLIS">
<meta name="application-name" content="SIRLIS">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.0/dist/tocbot.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/head.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <div class="site-title">
      <a href="/">SIRLIS</a>
    </div>
    <div class="site-subtitle fst-italic">分享科研和生活的日常</div>
  </div>
  <!-- .profile-wrapper -->

  <ul class="nav flex-column flex-grow-1 w-100 ps-0">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home"></i>
        <span>首页</span>
      </a>
    </li>
    <!-- the real tabs -->
    
      <li class="nav-item">
        <a href="/categories/" class="nav-link">
          <i class="fa-fw fas fa-stream"></i>
          

          <span>分类</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/tags/" class="nav-link">
          <i class="fa-fw fas fa-tags"></i>
          

          <span>标签</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/archives/" class="nav-link">
          <i class="fa-fw fas fa-archive"></i>
          

          <span>归档</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/about/" class="nav-link">
          <i class="fa-fw fas fa-info-circle"></i>
          

          <span>关于</span>
        </a>
      </li>
      <!-- .nav-item -->
    
  </ul>
  <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/sirlis"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/none"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['lihongjue','buaa.edu.cn'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</div>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container px-xxl-5">
        <!-- The Top Bar -->

<div id="topbar-wrapper">
  <div
    id="topbar"
    class="container d-flex align-items-center justify-content-between h-100"
  >
    <span id="breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                首页
              </a>
            </span>

          
        
          
        
          
            
              <span>模式识别（非线性分类器）</span>
            

          
        
      
    </span>
    <!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      文章
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="搜索..."
      >
    </span>
    <span id="search-cancel">取消</span>
  </div>
</div>

        











<div class="row">
  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pe-xl-4">
    

    <div class="post px-1 px-md-2">
      

      
        
      
        <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->



  
  

  <!-- CDN URL -->
  

  <!-- Add image path -->
  

  
    
      
      
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  




<!-- return -->




<h1 data-toc-skip>模式识别（非线性分类器）</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      发表于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1743341659"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2025/03/30
</em>

    </span>

    <!-- lastmod date -->
    
    <span>
      更新于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1743600197"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2025/04/02
</em>

    </span>
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      作者

      <em>
      
        <a href="https://github.com/sirlis">sirlis</a>
      
      </em>
    </span>

    <div>
      <!-- read time -->
      <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="10847 字"
>
  <em>60 分钟</em>阅读</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <p>很多情况下，类别之间的分类边界并不是线性的，一种更好的选择是使用更复杂的非线性函数来描述分类。本文介绍了模式识别常用的非线性分类器，主要包括近邻法分类器（NN、KNN）、支持向量机（SVM）、决策树（DT），最后介绍分类器的集成。</p>

<!--more-->

<hr />

<ul>
  <li><a href="#1-引言">1. 引言</a></li>
  <li><a href="#2-近邻法分类器nn--knn">2. 近邻法分类器（NN / KNN）</a>
    <ul>
      <li><a href="#21-最近邻法">2.1. 最近邻法</a></li>
      <li><a href="#22-k-近邻法">2.2. K 近邻法</a></li>
      <li><a href="#23-kd-树">2.3. KD 树</a></li>
      <li><a href="#24-构造-kd-树">2.4. 构造 KD 树</a></li>
    </ul>
  </li>
  <li><a href="#3-支持向量机svm">3. 支持向量机（SVM）</a>
    <ul>
      <li><a href="#31-线性支持向量机">3.1. 线性支持向量机</a>
        <ul>
          <li><a href="#311-硬间隔-svm-优化问题">3.1.1. 硬间隔 SVM 优化问题</a>
            <ul>
              <li><a href="#3111-优化问题描述">3.1.1.1. 优化问题描述</a></li>
              <li><a href="#3112-拉格朗日乘子">3.1.1.2. 拉格朗日乘子</a></li>
              <li><a href="#3113-对偶问题求解">3.1.1.3. 对偶问题求解</a></li>
            </ul>
          </li>
          <li><a href="#312-软间隔-svm-优化问题">3.1.2. 软间隔 SVM 优化问题</a>
            <ul>
              <li><a href="#3121-优化问题描述">3.1.2.1. 优化问题描述</a></li>
              <li><a href="#3122-拉格朗日乘子">3.1.2.2. 拉格朗日乘子</a></li>
              <li><a href="#3123-对偶问题求解">3.1.2.3. 对偶问题求解</a></li>
            </ul>
          </li>
          <li><a href="#313-统计机器学习的一般形式">3.1.3. 统计机器学习的一般形式</a></li>
        </ul>
      </li>
      <li><a href="#32-非线性支持向量机">3.2. 非线性支持向量机</a>
        <ul>
          <li><a href="#321-特征升维分类实例">3.2.1. 特征升维分类实例</a></li>
          <li><a href="#322-核技巧">3.2.2. 核技巧</a>
            <ul>
              <li><a href="#3221-多项式核函数">3.2.2.1. 多项式核函数</a></li>
              <li><a href="#3222-rbf-核函数">3.2.2.2. RBF 核函数</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#4-决策树dt">4. 决策树（DT）</a></li>
  <li><a href="#5-分类器集成">5. 分类器集成</a></li>
  <li><a href="#6-参考文献">6. 参考文献</a></li>
</ul>

<h2 id="1-引言"><span class="me-2">1. 引言</span><a href="#1-引言" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>在介绍线性分类器时，我们介绍过大多数线性分类器在样本线性可分的情况下可以取得很好的效果。但很多情况下，类别之间的分类边界并不是线性的，如果依然使用线性分类器也能勉强使用，但一种更好的选择是使用更复杂的非线性函数来描述分类。</p>

<h2 id="2-近邻法分类器nn--knn"><span class="me-2">2. 近邻法分类器（NN / KNN）</span><a href="#2-近邻法分类器nn--knn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="21-最近邻法"><span class="me-2">2.1. 最近邻法</span><a href="#21-最近邻法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>对于一个新样本，把它逐一与已知样本进行比较，找出距离新样本最近的已知样本，并以该样本的类别作为新样本的类别。其严格的数学描述如下：</p>

<p>假定有 $c$ 个类别的模式识别问题，每类包含样本个数为 $N_i,i=1,2,\cdots,c$。定义两个样本之间的距离度量 $\delta(\boldsymbol{x_i},\boldsymbol{x_j})$（比如可以采用欧式距离 $\delta(\boldsymbol{x_i},\boldsymbol{x_j})=\Vert \boldsymbol{x_i} - \boldsymbol{x_j} \Vert$），则对于任意未知新样本 $\boldsymbol{x}$，可以规定类 $w_i$ 的判别函数为：</p>

\[g_i(\boldsymbol{x}) = \min_k \delta(\boldsymbol{x}, \boldsymbol{x_i}^{(k)}), \; k =1,2,\cdots,N_i\]

<p>其中 $\boldsymbol{x}_k$ 是类别 $w_i$ 中的第 $k$ 个样本。</p>

<p>对于每个类别都求出其判别函数，那么决策规则为</p>

\[\boldsymbol{x}\in w_i\quad\text{if}\quad i=\arg\min_i g_i(\boldsymbol{x}),\; i=1,2,\cdots,c\]

<p>可以看出，最近邻法的思想很简单，新样本离谁最近，就把新样本判为最近的样本所属的类别。但在很多情况下，把决策建立在一个最近的样本上有一定风险，尤其是当数据分布复杂或数据中噪声严重时。如下图所示：</p>

<p><a href="/assets/img/postsimg/20250330/nearest-neibour.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250330/nearest-neibour.jpg" alt="nearest-neibour" class="lazyload" data-proofer-ignore></a></p>

<p>图中，黄色圆点为新样本（已知其为蓝色类别），但其最近邻为红色圆点样本。很明显，该红色样本是一个野值，其深入到了蓝色样本所属类别的区域。如果以最近邻的类别作为新样本的类别，那么显会导致分类错误。</p>

<p>为了更严格地分析最近邻法的错误率，我们需要进行一定的数学推导。当近邻法中训练样本的数量无限多（$N\rightarrow \infty$）时，某待分类的未知样本 $\boldsymbol{x}$ 的最近邻在极限意义上就是其自身，此时最近邻法分类正确的条件为：样本与它最近邻都属于同一类别。即分类正确率为</p>

\[\sum_{i=1}^c P(w_i\vert \boldsymbol{x})P(w_i\vert \boldsymbol{x}) = \sum_{i=1}^c P(w_i\vert \boldsymbol{x})^2\]

<p>对应的错误率为</p>

\[\lim_{N\rightarrow\infty} P_N(e\vert \boldsymbol{x}) = 1 - \sum_{i=1}^c P(w_i\vert \boldsymbol{x})^2\]

<p>则平均错误率为</p>

\[\begin{aligned}
P &amp;= \lim_{N\rightarrow \infty}P(e) =\lim_{N\rightarrow \infty} \int P_N(e\vert \boldsymbol{x})p(\boldsymbol{x})\text{d}x\\
&amp;= \int \lim_{N\rightarrow \infty} P_N(e\vert \boldsymbol{x})p(\boldsymbol{x})\text{d}x\\
&amp;= \int \left(1 - \sum_{i=1}^c P(w_i\vert \boldsymbol{x})^2\right)p(\boldsymbol{x})\text{d}x
\end{aligned}\]

<p>$P$ 又被称为最近邻法的<strong>渐进平均错误率</strong>，是 $P_N(e)$ 在 $N\rightarrow \infty$ 的极限。</p>

<p>为了更进一步了解最近邻法错误率的上界和下界，我们可以将其和理论上具备最优错误率的<u>贝叶斯错误率</u>进行比较。已知 $c$ 分类问题的贝叶斯错误率为</p>

\[P^\star = \int[1- \max_i P(w_i \vert \boldsymbol{x})]p(\boldsymbol{x})\text{d}x,\; i=1,2,\cdots,c\]

<p>记 $m = \arg\max_i P(w_i\vert \boldsymbol{x})$，则</p>

\[P^\star = \int[1- P(w_m\vert \boldsymbol{x})]p(\boldsymbol{x})\text{d}x\]

<p>因此，比较最近邻错误率和贝叶斯错误率的关键在于衡量以下两者的大小关系：</p>

\[P(w_m\vert \boldsymbol{x}) \quad \text{和} \quad \sum_{i=1}^c P(w_i\vert \boldsymbol{x})^2\]

<p>二者作差</p>

\[\begin{aligned}
P(w_m\vert x) - \sum_{i=1}^c P(w_i\vert x)^2 &amp;= P(w_m\vert x) -P(w_m\vert x)^2 - \sum_{i\neq m} P(w_i\vert x)^2\\
&amp;= P(w_m\vert x)(1-P(w_m\vert x)) - \sum_{i\neq m} P(w_i\vert x)^2\\
&amp;= P(w_m\vert x)\sum_{i\neq m} P(w_i\vert x) - \sum_{i\neq m} P(w_i\vert x)^2\\
&amp;= \sum_{i\neq m} P(w_i\vert x)[P(w_m\vert x) - P(w_i\vert x)]\\
&amp;\geq 0
\end{aligned}\]

<p>至此我们确定了<strong>最近邻法的错误率下界为贝叶斯错误率</strong>，即</p>

\[P^\star \leq P\]

<p>为了求得最近邻法的错误率上界，需要借助柯西-施瓦茨不等式。</p>

<blockquote>
  <p>柯西-施瓦茨不等式：对于任意两个实值随机变量 $a$ 和 $b$，有</p>

\[\left( \sum_{i=1}^n a_ib_i \right)^2 \leq \sum_{i=1}^n a_i^2 \sum_{i=1}^n b_i^2\]

  <p>推论为</p>

\[n\sum_{i=1}^n a_i^2 \geq \left( \sum_{i=1}^n a_i \right)^2\]
</blockquote>

<p>对于最近邻错误率有</p>

\[\begin{aligned}
\sum_{i\neq m} P(w_i\vert x)^2 &amp;\geq \frac{1}{c-1}\left(\sum_{i\neq m} P(w_i\vert x)\right)^2\\
&amp;= \frac{1}{c-1}\left(1 - P(w_m\vert x)\right)^2
\end{aligned}\]

<p>左右两边同时补上 $P(w_m\vert x)^2$ 有</p>

\[\begin{aligned}
\sum_{\textcolor{red}{i=1}}^c P(w_i\vert x)^2 &amp;\geq \frac{1}{c-1}\left(1 - P(w_m\vert x)\right)^2 + \textcolor{red}{P(w_m\vert x)^2}\\
\textcolor{green}{1 - }\sum_{i=1}^c P(w_i\vert x)^2 &amp;\textcolor{green}{\leq 1 - }\frac{1}{c-1}\left(1 - P(w_m\vert x)\right)^2 + \textcolor{red}{[1-(1-P(w_m\vert x))]^2}
\end{aligned}\tag{1}\]

<p>将上式右侧展开有</p>

\[\begin{aligned}
&amp;1-\frac{1}{c-1}\left(1 - P(w_m\vert x)\right)^2 - [1-(1-P(w_m\vert x))]^2\\
=&amp; 1 - \frac{1}{c-1}\left(1 - P(w_m\vert x)\right)^2 -[1-2(1-P(w_m\vert x)) + (1-P(w_m\vert x))^2]\\
=&amp; 1 - \frac{1}{c-1}\left(1 - P(w_m\vert x)\right)^2 - 1 + 2(1-P(w_m\vert x)) - (1-P(w_m\vert x))^2\\
=&amp; (-\frac{1}{c-1}-1)\left(1 - P(w_m\vert x)\right)^2 + 2(1-P(w_m\vert x))\\
=&amp; (1-P(w_m\vert x))[2-(1+\frac{1}{c-1})(1-P(w_m\vert x))]\\
=&amp; (1-P(w_m\vert x))\left[2-\frac{c}{c-1}(1-P(w_m\vert x))\right]\\
\end{aligned}\tag{2}\]

<p>将式（2）代回式（1），左右同时积分有</p>

\[\begin{aligned}
1-\sum_{i=1}^c P(w_i\vert x)^2 &amp;\leq (1-P(w_m\vert x))\left[2-\frac{c}{c-1}(1-P(w_m\vert x))\right]\\
\int \left(1-\sum_{i=1}^c P(w_i\vert x)^2\right)p(x)\text{d}x &amp;\leq \int [1-P(w_m\vert x)]p(x)\text{d}x\left[2-\frac{c}{c-1}\int [1-P(w_m\vert x)]\text{d}x\right]\\
\end{aligned}\]

<p>因此我们得到最近邻法的错误率上界为</p>

\[P \leq P^\star \left[2-\frac{c}{c-1}P^\star\right]\]

<p>经过数学推导可知，<strong>最近邻的渐进错误率最坏不会超过两倍的贝叶斯错误率，而最好则有可能接近或达到贝叶斯错误率</strong>。</p>

<p>最后分析等号成立的条件，即最近邻法的错误率等于贝叶斯错误率的条件。</p>

<ul>
  <li>
    <p>对于下界，当且仅当 $P(w_m\vert x) = P(w_i\vert x)$ 时取到等号，此时 $P(w_m\vert x) = P(w_i\vert x) = 1/c$，即所有类别的后验概率相等，不管是最近邻分类器还是贝叶斯分类器都相当于瞎猜，二者因此错误率都一样。</p>
  </li>
  <li>
    <p>对于上界，当且仅当 $P(w_m\vert x) = 1$ 时取到等号，此时 $P(w_m\vert x) = 1$，即后验概率最大的类别的后验概率为 1，表明样本全为某一类，此时贝叶斯分类器和最近邻分类器都能 100% 做出正确决策。</p>
  </li>
</ul>

<h3 id="22-k-近邻法"><span class="me-2">2.2. K 近邻法</span><a href="#22-k-近邻法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>为了解决最近邻法容易受到噪声和野值影响的问题，简单且符合直觉的想法是可以考虑使用 $k$ 近邻法，即找出距离新样本最近的 $k$ 个样本，然后以这 $k$ 个样本中出现次数最多的类别作为新样本的类别。</p>

<p>根据 $k$ 值选取的不同，可做出如下讨论：</p>

<ul>
  <li>$k$ 值越小，模型复杂度越高，容易发生过拟合。极端情况 $k=1$ 退化为最近邻法，如果恰好遇到噪声，就会完全错误；
随着 $k$ 值增大，模型泛化能力也增大，但丢失的信息也增多。$k$ 值的增大就意味着整体的模型变得简单；</li>
  <li>$k=N$，则任意新输入样例的分类就等于训练样例中样本数最多的分类，此时无论输入样本是什么，都只是简单的预测它属于在训练样本中最多的类。</li>
</ul>

<p>对于 $k$ 近邻法，其错误率的上界和下界与最近邻法类似，但是 $k$ 近邻法的错误率上界和下界都比最近邻法的要好。</p>

<p>$k$ 近邻法的特点概括如下：</p>

<ul>
  <li>优点：可以解决几乎所有分类问题，概念简单未经优化，分类器设计容易，且错误率并不高；</li>
  <li>缺点：计算量大，内存开销大，特别依赖快速搜索算法。</li>
</ul>

<h3 id="23-kd-树"><span class="me-2">2.3. KD 树</span><a href="#23-kd-树" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>K-近邻算法是机器学习中最简单的算法之一，如果训练样本过大，则传统的遍历全样本寻找 K-近邻的方式将导致性能的急剧下降。为了优化效率，不同的训练数据存储结构被纳入到实现方式之中。</p>

<p>KD 树（K-Dimension Tree）是一种对 $k$ 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构，主要用于高效地组织和搜索多维空间中的点。</p>

<h3 id="24-构造-kd-树"><span class="me-2">2.4. 构造 KD 树</span><a href="#24-构造-kd-树" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>构造 KD 树相当于不断地用垂直于坐标轴的超平面将 $k$ 维空间切分，构成一系列的 $k$ 维超矩形区域。KD 树的每个节点对应于一个 $k$ 维超矩形区域。通常，依次选择坐标轴对空间划分，选择训练样本点在选定坐标轴上的中位数为切分点，这样得到的 KD 树是是<u>平衡</u>的。</p>

<p>以样本 $a = {(2,3)^\top,(5,4)^\top,(9,6)^\top,(4,7)^\top,(8,1)^\top,(7,2)^\top }$ 为例，构造 KD 树的过程如下：</p>

<ul>
  <li><strong>计算方差</strong>，选择方差最大的维度开始，本例中为 $x$ 轴，之后按照维度顺序进行切分。有时为了简便起见，直接按照维度顺序切分而不计算方差；</li>
  <li><strong>选择 $x$ 轴</strong>，对 $a$ 中的样本按照 $x$ 轴坐标进行排序，得到 $a_x = {(2,3)^\top,(4,7)^\top,(5,4)^\top,(7,2)^\top,(8,1)^\top,(9,6)^\top }$，选择中位数 $m = (5,4)^\top$ 作为切分点，得到左右子树 $a_{x_l} = {(2,3)^\top,(4,7)^\top,(7,2)^\top }$ 和 $a_{x_r} = {(8,1)^\top,(9,6)^\top }$；</li>
  <li><strong>选择 $y$ 轴</strong>，对 $a_{x_l}$ 中的样本按照 $y$ 轴坐标进行排序，得到 $a_{x_l,y} = {(2,3)^\top,(7,2)^\top,(4,7)^\top }$，选择中位数 $m = (7,2)^\top$ 作为切分点，得到左右子树 $a_{x_l,y_l} = {(2,3)^\top }$ 和 $a_{x_l,y_r} = {(4,7)^\top }$；</li>
  <li>如此循环直至所有样本都被划分到叶子节点。</li>
</ul>

<p>得到的划分如下图所示。</p>

<p><a href="/assets/img/postsimg/20250330/kd-tree.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250330/kd-tree.jpg" alt="KD-division" class="lazyload" data-proofer-ignore></a></p>

<p>形成的 KD 树如下图所示。</p>

<p><a href="/assets/img/postsimg/20250330/kd-tree-2.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250330/kd-tree-2.jpg" alt="KD-Tree" class="lazyload" data-proofer-ignore></a></p>

<p>完成 KD 树的构建只是第一步，接下来需要对 KD 树进行搜索。</p>

<p>查询时，从根节点开始，沿着与查询点在当前维度上最接近的方向遍历树。到达叶子节点或分裂超平面时，会检查另一个子空间是否有更近的点，这依赖于当前点到查询点的距离和超平面的距离。这一过程重复，直到找到最近的点或达到预定的搜索深度。</p>

<p>进行最近邻搜索时，KD 树利用其结构减少搜索空间，具体步骤如下：</p>

<ul>
  <li>从根节点出发，根据构造 KD 树时的切分规则（切分维度顺序和左小右大），递归地向下访问 KD 树的节点，直到到达叶子节点；</li>
  <li>跟踪最近点：记录遍历过程中遇到的最近点，并在每个节点处计算是否可能存在更近的点在另一侧的子树中；</li>
  <li>回溯：递归地向上回退，检查每个节点的另一侧是否可能存在更近的点，直到整个树都被搜索完毕。</li>
</ul>

<p>对于上述实例，如果查询点为 $(2,4.5)^\top$，则搜索过程如下：</p>

<ul>
  <li>找寻最近点：
    <ul>
      <li>从根节点开始，根据 $x$ 轴坐标，查询点 $(2,4.5)^\top$ 在 $x$ 轴上小于 $(7,2)^\top$，因此向左子树搜索；</li>
      <li>在左子树中，根据 $y$ 轴坐标，查询点 $(2,4.5)^\top$ 在 $y$ 轴上大于 $(5,4)^\top$，因此向右子树搜索；</li>
      <li>在右子树中，查询点 $(2,4.5)^\top$ 的最近点为 $(4,7)^\top$，计算二者距离为 $3.2$；</li>
    </ul>
  </li>
  <li>回溯：
    <ul>
      <li>回溯上层 $(5,4)^\top$，计算距离为 $3.04$，比 $(4,7)^\top$ 更近，更新最近点为 $(5,4)^\top$；</li>
      <li>以 $3.04$ 做圆；此圆与 $(5,4)^\top$ 所切分的上下两个平面相交，需要检查$(5,4)^\top$ 的另外侧；</li>
      <li>$(5,4)^\top$ 的另一侧子节点为 $(2,3)^\top$ ，与之距离为 $1.5$，更新最近点为 $(2,3)^\top$；</li>
      <li>由于 $(2,3)^\top$ 为叶子节点，回溯至 $(7,2)^\top$，确认与 $(7,2)^\top$ 切分的右子平面无关；</li>
      <li>抵达根节点，回溯结束。最终确定最近点为 $(2,3)^\top$。</li>
    </ul>
  </li>
</ul>

<p><a href="/assets/img/postsimg/20250330/kd-search.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250330/kd-search.jpg" alt="KD-Search" class="lazyload" data-proofer-ignore></a></p>

<p>进一步，给出使用 KD 树进行 K 近邻搜索的伪代码：</p>

<ul>
  <li>输入查询节点 $x$；</li>
  <li>创建 $k$ 个元素的空列表 $L$，用于存储 $x$ 的最近邻点和相应距离；</li>
  <li><strong>（1）</strong> 根据 $x$ 的坐标和当前节点切分规则（左小右大）向下搜索；</li>
  <li><strong>（2）</strong> 到达底部节点，将其标记为访问过，如果 $L$ 中有空位且叶子节点与 $x$ 的距离小于 $L$ 中的最大值，则用叶子节点替换其中距离最大值对应节点；</li>
  <li><strong>（3）</strong> 如果当前节点不是根节点，执行 <strong>（4）</strong>；反之，输出 $L$ 算法结束；</li>
  <li><strong>（4）</strong> 向上回溯一个节点，抵达 $p$。如果 $p$ 未被访问过，将其标记为访问过，并执行后续子步骤；如果访问过，则再次执行 <strong>（4）</strong>；
    <ul>
      <li><strong>（a）</strong> 如果 $L$ 有空位，则将 $p$ 加入 $L$；反之，如果 $L$ 已满 但 $p$ 与 $x$ 的距离小于 $L$ 中的最大值，则用 $p$ 替换 $L$ 中的最大值；</li>
      <li><strong>（b）</strong> 计算 $x$ 与 $p$ 的切分线的距离，记为 $d$；如果 $d$ 小于 $L$ 中的最大值，或者 $L$ 中有空位，则访问 $p$ 的另一侧子节点，回到 <strong>（1）</strong> 执行；反之，如果 $d$ 大于 $L$ 中的最大值且$L$ 已满，则切分线另一边不会有更近的点，返回 <strong>（3）</strong>。</li>
    </ul>
  </li>
</ul>

<p>KD 树的缺点：</p>

<ul>
  <li>维度诅咒：随着维度的增加，KD树的效率急剧下降。这是因为高维空间中，数据点间的距离变得非常相似，难以通过简单的分割有效区分，这种现象被称为“维度诅咒”。</li>
  <li>平衡问题：KD树的性能对分割轴的选择很敏感，如果数据分布不均匀或者选择的分割策略不当，容易导致树的不平衡，进而影响搜索效率。尽管有一些平衡策略，但完全避免不平衡较难。</li>
  <li>维护成本：在动态数据集中，频繁的插入和删除操作可能导致KD树结构失衡，需要花费额外的时间来重新平衡树，这可能抵消掉一部分查询效率的优势。</li>
  <li>不适合高度倾斜或复杂数据分布：当数据分布呈现高度偏斜或具有复杂模式时，简单地按照中位数分割可能不是最优选择，这会导致搜索效率降低。在这种情况下，更复杂的树结构，如球树（Ball Tree）或覆盖树（Cover Tree），可能提供更好的性能。</li>
</ul>

<h2 id="3-支持向量机svm"><span class="me-2">3. 支持向量机（SVM）</span><a href="#3-支持向量机svm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>支持向量机（Support Vector Machines，SVM）被提出于1964年，20世纪90年代后得到快速发展，并衍生出一系列改进和扩展算法。在解决小样本情况下的机器学习问题和高维、非线性问题中表现出较为优异的效果。</p>

<p>SVM 是基于线性可分的最优分类面提出的，最优分类面的定义保证了在样本一定的情况下，两类样本间的距离最大。注意到，截至目前讨论的 SVM 似乎表现为一个线性分类器而不是非线性分类器，但后面会介绍到，SVM 的非线性分类能力是通过引入核函数来实现的。</p>

<h3 id="31-线性支持向量机"><span class="me-2">3.1. 线性支持向量机</span><a href="#31-线性支持向量机" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>前面介绍线性分类器时，我们介绍了感知准则，并提到了线性可分性的概念。假设 $D_0$ 和 $D_1$ 是 $n$ 维欧式空间中的两个点集，$D_0$ 和 $D_1$ 线性可分的条件是存在一个超平面 $H$，使得 $D_0$ 和 $D_1$ 分别位于超平面的两侧。即存在一个向量 $\boldsymbol{w}$ 和一个标量 $b$，使得对于任意的 $\boldsymbol{x}_1\in D_0$ 和 $\boldsymbol{x}_2\in D_1$，都有：</p>

\[\begin{aligned}
\boldsymbol{w}^\top \boldsymbol{x}_1 + b &amp;&gt; 0,\; \boldsymbol{x}_1\in D_0\\
\boldsymbol{w}^\top \boldsymbol{x}_2 + b &amp;&lt; 0,\; \boldsymbol{x}_2\in D_1
\end{aligned}\]

<p>对于线性可分的样本，感知准则可以找到一个超平面将两类样本完全分开，但并不能保证该超平面是最优的。</p>

<p>SVM 可以看作感知准则的一种改进，SVM 的目标是找到一个超平面将两类样本分开，并且使得两类样本到超平面的距离最（标量）大，从而引入了某种最优性。</p>

<h4 id="311-硬间隔-svm-优化问题"><span class="me-2">3.1.1. 硬间隔 SVM 优化问题</span><a href="#311-硬间隔-svm-优化问题" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<h5 id="3111-优化问题描述"><span class="me-2">3.1.1.1. 优化问题描述</span><a href="#3111-优化问题描述" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>以二分类为例，假设样本集为 $S={(\boldsymbol{x}_1,y_1),(\boldsymbol{x}_2,y_2),\cdots,(\boldsymbol{x}_N,y_N)}$，其中 $\boldsymbol{x_i}$ 是样本，$y_i$ 是样本的类别标签。假设样本集线性可分，存在一个超平面 $H$ 将两类样本完全分开。假设超平面 $H$ 的法向量为 $\boldsymbol{w}$，超平面到原点的距离为 $b$，则超平面可以表示为：</p>

\[\boldsymbol{w}^\top \boldsymbol{x} + b = 0\]

<p>那么判别函数可写为</p>

\[\begin{cases}
  \boldsymbol{w}^\top \boldsymbol{x_i} + b &gt; 0, y = 1\\
  \boldsymbol{w}^\top \boldsymbol{x_i} + b &lt; 0, y= -1
\end{cases}\]

<p>合并为简单形式，则线性可分的条件为</p>

\[y_i(\boldsymbol{w}^\top\boldsymbol{x_i}+b) \geq 0, \forall i=1,2,\cdots,N\]

<p>根据前述 SVM 目标，我们希望求解如下优化问题：</p>

\[\begin{aligned}
\max_{\boldsymbol{w},b} &amp;\quad \text{margin} (\boldsymbol{w},b)\\
  s.t. &amp;\quad  y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) &gt; 0, \forall i=1,2,\cdots,N
\end{aligned}\]

<p>上述优化问题中唯一不确定的地方就是间隔（margin）的定义。间隔被定义为样本中离决策面最近的那些样本到决策面的距离。设 $d_i$ 为样本 $\boldsymbol{x_i}$ 到超平面的距离（标量），则有：</p>

\[d_i = \frac{\vert \boldsymbol{w}^\top \boldsymbol{x_i} + b\vert}{\Vert \boldsymbol{w} \Vert}\]

<p>那么<strong>间隔</strong>定义为</p>

\[\text{margin}(\boldsymbol{w},b) = \min_{i} d_i\]

<p>则 SVM 的优化问题变为</p>

\[\begin{aligned}
\max_{\boldsymbol{w},b}  \min_{i} &amp;\quad\frac{\vert \boldsymbol{w}^\top \boldsymbol{x_i} + b\vert}{\Vert \boldsymbol{w} \Vert}\\
  s.t. &amp;\quad  y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) &gt; 0, \forall i=1,2,\cdots,N
\end{aligned}\]

<p>注意到，约束条件表明 $y_i$ 和 $\boldsymbol{w}\boldsymbol{x_i}+b$ 同号且二者相乘恒大于零，那么可安全地将优化目标函数的绝对值打开，得到</p>

\[\begin{aligned}
\max_{\boldsymbol{w},b} \min_{i} &amp;\quad\frac{\textcolor{green}{y_i(\boldsymbol{w}^\top \boldsymbol{x_i} + b)}}{\Vert \boldsymbol{w} \Vert}\\
s.t. &amp;\quad  y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) &gt; 0, \forall i=1,2,\cdots,N
\end{aligned}\]

<p>观察优化目标函数中，不同优化方向的作用对象，可移项如下</p>

\[\begin{aligned}
\max_{\boldsymbol{w},b}&amp;\; \textcolor{blue}{\frac{1}{\Vert \boldsymbol{w} \Vert}} \; \min_{i} \; y_i(\boldsymbol{w}^\top \boldsymbol{x_i} + b)\\
s.t. &amp;\quad  y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) &gt; 0, \forall i=1,2,\cdots,N\\
\end{aligned}\]

<p>既然约束条件要求 $y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) &gt; 0$，那么一定有</p>

\[\exists \gamma &gt; 0,\; \min_i y_i(\boldsymbol{w}^\top\boldsymbol{x_i}+b) =\gamma\]

<p>此处 $\gamma$ 被称为 <strong>函数间隔</strong>。引入函数间隔后，优化问题变为如下形式</p>

\[\begin{aligned}
\max_{\boldsymbol{w},b}&amp;\; \textcolor{blue}{\frac{1}{\Vert \boldsymbol{w} \Vert}} \; \min_{i} \; y_i(\boldsymbol{w}^\top \boldsymbol{x_i} + b)\\
s.t. &amp;\quad  y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) \textcolor{red}{\geq \gamma}, \forall i=1,2,\cdots,N\\
\text{or}\quad s.t.\quad \textcolor{green}{\min_i}&amp;\quad  \textcolor{green}{y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) = \gamma}, \forall i=1,2,\cdots,N
\end{aligned}\]

<p>将条件代入目标函数，消去 $\min_i$ 项，有</p>

\[\begin{aligned}
\max_{\boldsymbol{w},b}&amp;\quad \frac{\textcolor{red}{\gamma}}{\Vert \boldsymbol{w} \Vert}\\
s.t. &amp;\quad  y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) \textcolor{red}{\geq \gamma}, \forall i=1,2,\cdots,N
\end{aligned}\]

<p>注意到当 $\boldsymbol{w},b$ 发生变化（如分别扩大 2 倍）也会导致函数间隔发生变化（同样扩大两倍）。虽然这种操作不会改变最优解，但会产生一个等价的优化问题。为了避免这种情况，不妨令 $\gamma = 1$，优化问题变为</p>

\[\begin{aligned}
\max_{\boldsymbol{w},b}&amp;\quad \frac{1}{\Vert \boldsymbol{w} \Vert}\\
s.t. &amp;\quad  y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) \geq 1, \forall i=1,2,\cdots,N
\end{aligned}\]

<blockquote>
  <p>这一事实给了我们另一个启示，我们应该对法向量 $\boldsymbol{w}$ 加一个约束，使得间隔固定不变，此时的间隔为 <strong>几何间隔</strong>。</p>
</blockquote>

<p>最后，利用如下等价性</p>

\[\max_{\boldsymbol{w},b} \frac{1}{\Vert \boldsymbol{w}\Vert} = \min_{\boldsymbol{w},b} \Vert \boldsymbol{w}\Vert^2 = \frac{1}{2}\min_{\boldsymbol{w},b} \boldsymbol{w}^\top \boldsymbol{w}\]

<p>得到最终优化问题的描述</p>

\[\begin{aligned}
\min_{\boldsymbol{w},b} &amp;\quad \frac{1}{2} \boldsymbol{w}^\top \boldsymbol{w}\\
s.t. &amp;\quad  y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) \geq 1, \forall i=1,2,\cdots,N
\end{aligned}\]

<p>上述优化问题是一个包含 $N$ 个不等式约束的二次凸优化问题。</p>

<p>对于线性可分的样本，所有样本一定能够满足上面的不等式约束，也即所有样本都可以位于间隔的两侧，不存在错分的情况，所以上述优化问题一定有解。我们称上述优化问题为 <strong>硬间隔 SVM 优化问题</strong>。对应的图示如下</p>

<p><a href="/assets/img/postsimg/20250330/hard-margin-svm.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250330/hard-margin-svm.jpg" alt="hard-margin-svm" class="lazyload" data-proofer-ignore></a></p>

<h5 id="3112-拉格朗日乘子"><span class="me-2">3.1.1.2. 拉格朗日乘子</span><a href="#3112-拉格朗日乘子" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>对于一个带约束的优化问题，采用拉格朗日乘子法转化为一个无约束优化问题，拉格朗日函数如下</p>

\[L(\boldsymbol{w},b,\lambda) = \frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w} + \sum_{i=1}^N\lambda_i(1-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b)),\; \lambda_i \geq 0\]

<p>优化问题变为</p>

\[\begin{aligned}
\min_{w,b}\quad &amp; \max_\lambda L(\boldsymbol{w},b,\lambda)\\
s.t. &amp; \quad \lambda_i \geq 0, \forall i=1,2,\cdots,N\\
\end{aligned}\]

<blockquote>
  <p>拉格朗日乘子法变换后的优化问题与原始优化问题的等价性可如下简单说明：</p>

  <p>定义 $\delta = 1-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b)$，SVM 优化问题的约束条件变为</p>

\[\delta_i \leq 0\]

  <p>整个 $(\boldsymbol{w},b)$ 空间可划分为两个部分，不满足约束 （$\delta &gt; 0$） 和 满足约束 （$\delta \leq 0$）。</p>

  <ul>
    <li>对于 $\delta &gt; 0$，约束被违反，可以取 $\lambda \to \infty$ 使得优化目标</li>
  </ul>

\[\max_\lambda L(\boldsymbol{w},b,\lambda) =  \frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w} + \infty = +\infty\]

  <ul>
    <li>对于 $\delta \leq 0$，约束被满足，可以取$\lambda = 0$ 使得优化目标取到最大值</li>
  </ul>

\[\max_\lambda L(\boldsymbol{w},b,\lambda) =  \frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w} + 0 = \frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w}\]

  <p>综上，拉格朗日乘子法的优化问题</p>

\[\min_{w,b}\max_\lambda L(\boldsymbol{w},b,\lambda) = \min_{w,b} [+\infty,\;\frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w}] = \min_{w,b} \frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w}\]

  <p>与原始优化问题等价，其中外层 $\min_{w,b}$ 会自动忽略不满足约束的解。</p>
</blockquote>

<h5 id="3113-对偶问题求解"><span class="me-2">3.1.1.3. 对偶问题求解</span><a href="#3113-对偶问题求解" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>上述优化问题的 <strong>对偶问题</strong> 为（交换 $\max$ 与 $\min$）</p>

\[\begin{aligned}
\max_\lambda \min_{w,b} &amp;\quad L(\boldsymbol{w},b,\lambda)\\
s.t. &amp;\quad \lambda_i \geq 0, \forall i=1,2,\cdots,N
\end{aligned}\]

<blockquote>
  <p>关于为何 SVM 要引入原问题的对偶问题，是因为：</p>

  <ul>
    <li>支持向量的稀疏性：</li>
  </ul>

  <p>在对偶问题中，拉格朗日乘子 $\lambda_i$ 只有在支持向量上才会非零，而支持向量的数量通常远小于总样本数。这意味着在分类时，只需要计算支持向量与查询点的内积，大大减少了计算量。</p>

  <ul>
    <li>高维空间的计算效率：</li>
  </ul>

  <p>如果直接求解原问题，分类时需要计算 $w^\top x$，其中 $w$ 是一个高维向量（维度为 $d$）。而在对偶问题中，分类只需要计算支持向量与查询点的内积 $\sum \lambda_i y_i \langle \boldsymbol{x_i}, x \rangle$，这可以通过核函数高效完成，避免直接处理高维向量。</p>

  <ul>
    <li>核函数的引入：</li>
  </ul>

  <p>对偶问题的形式天然适合引入核函数，将数据映射到高维空间进行非线性分类，而无需显式计算高维空间中的坐标。这是 SVM 能够处理非线性问题的关键。</p>
</blockquote>

<p>注意原问题和对偶问题存在如下关系</p>

\[\min_{w,b}\max_\lambda L(\boldsymbol{w},b,\lambda) \geq \max_\lambda \min_{w,b} L(\boldsymbol{w},b,\lambda)\]

<p>在优化理论中，原始问题（Primal Problem）和对偶问题（Dual Problem）之间存在一定的关系，而 KKT（Karush-Kuhn-Tucker）条件 是连接两者的桥梁。对于支持向量机（SVM）的优化问题，原始问题和对偶问题之间满足 <u>弱对偶性（Weak Duality）</u>，而在一定条件下（如凸优化+Slater条件），它们还满足 <u>强对偶性（Strong Duality）</u>，即原始问题的最优值等于对偶问题的最优值。此时，KKT条件成为取等号的充要条件。</p>

<blockquote>
  <p>KKT 条件，$\forall i=1,2,\cdots,N$，有</p>

\[\begin{aligned}
\frac{\partial L}{\partial w} = 0,\frac{\partial L}{\partial b} = 0,\frac{\partial L}{\partial \lambda} = 0\quad \text{梯度可行性}\\
\lambda_i(1-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b))=0\quad \text{互补松弛性}\\
\lambda_i \geq 0\quad \text{对偶可行性}\\
1-y_i(\boldsymbol{w}^\top x + b)\leq 0\quad \text{原始可行性}\\
\end{aligned}\]
</blockquote>

<p>求解此对偶问题，根据梯度可行性条件，<strong>目标函数首先对 $b$ 求偏导令其为零</strong>，有</p>

\[\frac{\partial L}{\partial b} = 0\Rightarrow \sum_{i=1}^N\lambda_i y_i = 0\\\]

<p>将其代入目标函数进行化简，有</p>

\[\begin{aligned}
L(\boldsymbol{w},b,\lambda) &amp;= \frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w} + \sum_{i=1}^N\lambda_i(1-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b))\\
&amp;= \frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w} + \sum_{i=1}^N \lambda_i - \sum_{i=1}^N \lambda_i y_i w^\top \boldsymbol{x_i}
\end{aligned}\]

<p>简化后的<strong>目标函数对 $w$ 求偏导令其为零</strong>，得到最优参数 $w^\star$ 的形式</p>

\[\frac{\partial L}{\partial w} = w - \sum_{i=1}^N \lambda_i y_i \boldsymbol{x_i} = 0\Rightarrow w= \sum_{i=1}^N \lambda_i y_i \boldsymbol{x_i}\]

<p>将其代入目标函数进行进一步化简，有</p>

\[\begin{aligned}
L(\boldsymbol{w},b,\lambda) =&amp; \frac{1}{2}(\sum_{i=1}^N\lambda_i y_i \boldsymbol{x_i})^\top (\sum_{i=j}^N\lambda_j y_j \boldsymbol{x_j})+ \sum_{i=1}^N \lambda_i\\
&amp; - \sum_{i=1}^N \lambda_i y_i (\sum_{i=j}^N \lambda_j y_j \boldsymbol{x_j})^\top \boldsymbol{x_i}\\
=&amp;\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \lambda_i \lambda_j y_i y_j \boldsymbol{x_i}^\top \boldsymbol{x_j} + \sum_{i=1}^N \lambda_i\\
&amp; - \sum_{i=1}^N\sum_{j=1}^N \lambda_i \lambda_j y_i y_j \boldsymbol{x_j}^\top \boldsymbol{x_i}\\
=&amp; -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \lambda_i \lambda_j y_i y_j \boldsymbol{x_i}^\top \boldsymbol{x_j} + \sum_{i=1}^N \lambda_i
\end{aligned}\]

<p>下面继续确定最优参数 $b^\star$ 的形式。根据 KKT 条件中的互补松弛条件、对偶可行条件、原始可行条件进行分析</p>

\[\begin{aligned}
\lambda_i(1-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b))=0\\
\lambda_i \geq 0\\
1-y_i(\boldsymbol{w}^\top \boldsymbol{x}_i + b)\leq 0\\
\end{aligned}\]

<p>对于第 $k$ 个样本分析如下</p>

<ul>
  <li>
    <p>若样本点 $(\boldsymbol{x}_k, y_k)$ 使得 $y_i(\boldsymbol{w}^\top \boldsymbol{x} + b) = 1$，这个样本点就是距离分类面最近的那些样本点，被称为 <strong>支持向量（Support Vector）</strong>。此时对 $\lambda_i$ 没有额外约束。因此我们通过 <u>选择任意一个支持向量</u> 可以求出 $b^\star$</p>

\[\begin{aligned}
y_k(\boldsymbol{w}^\top \boldsymbol{x}_k + b) &amp;= 1\\
y_k^2(\boldsymbol{w}^\top \boldsymbol{x}_k + b) &amp;= y_k
\end{aligned}\]

    <p>考虑到 $y_k^2 = 1$，则可以得到最优参数 $b^\star$ 的形式</p>

\[b^\star = y_k-w^\top \boldsymbol{x}_k= y_k-\sum_{i=1}^N \lambda_i y_i \boldsymbol{x}_i^\top \boldsymbol{x}_k\]

    <p>考虑到样本带噪声的情况，我们也可以通过对所有支持向量求出 $b$ 后取均值得到 $b^\star$。</p>
  </li>
  <li>
    <p>第二种情况，该样使得 $y_i(\boldsymbol{w}^\top x + b) &gt; 1$，那么必须要求$\lambda_i = 0$。也就是说，只有当样本是支撑向量时对应的 $\lambda_i \neq 0$，否则 $\lambda = 0$。这将极大简化 $\boldsymbol{w}^\star,b^\star$ 的计算。</p>
  </li>
</ul>

<p>至此，我们已经确定了内层优化 $\min_{w,b}$ 的最优解，剩余的优化问题如下</p>

\[\begin{aligned}
\max_\lambda &amp;\quad -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \lambda_i \lambda_j y_i y_j \boldsymbol{x_i}^\top \boldsymbol{x_j} + \sum_{i=1}^N \lambda_i\\
s.t. &amp;\quad \lambda_i \geq 0,\;\sum_{i=1}^N\lambda_i y_i = 0,\; \forall i=1,2,\cdots,N
\end{aligned}\]

<p>这个问题是针对 $N$ 个拉格朗日乘子 $\lambda_i$ 进行优化，我们需要求一个 $N$ 元函数的受不等式约束的极值问题。当 $N$ 很大的时候，这个问题就很难求解。我们需要借助 <strong>序列最小化算法（Sequential Minimal Optimization，SMO）</strong>，将问题简化为，每次只变动两个参数，记作 $\alpha_i,\alpha_j$，与此同时固定其余所有的参数，问题就能转变为一个一元二次函数求极值问题，最终可以求解得到所有的 $\lambda_i$，具体求解略。</p>

<blockquote>
  <p>之所以选择变动两个参数，是因为我们有限制条件</p>

\[\sum_{i=1}^N\lambda_i y_i = 0\]

  <p>如果仅变动一个参数，那么这个参数实际上可以由其他参数唯一确定。</p>

  <p>关于 SMO 算法的详细讨论，可以参考：https://zhuanlan.zhihu.com/p/367578887</p>
</blockquote>

<p>最后将优化问题的解列写如下</p>

\[\begin{aligned}
w^\star &amp;= \sum_{i=1}^N \lambda_i y_i \boldsymbol{x_i}\\
b^\star &amp;= y_k-\sum_{i=1}^N \lambda_i y_i \boldsymbol{x_i}^\top \boldsymbol{x}_k\quad \text{support vector}\; (\boldsymbol{x}_k,y_k)
\end{aligned}\]

<p>得到 <strong>SVM 的判别函数</strong> 如下</p>

\[f(x) = \text{sign}({\boldsymbol{w}^\star}^\top x + b^\star)\]

<p>并且知道 $\boldsymbol{w}^\star,b^\star$ 仅是 <u>支持向量</u> 的线性组合（因为 $\lambda_i$ 已只有在样本为支持向量时才不为零）。所以 SVM 训练完成后，大部分的训练样本都不需要保留，最终的分类模型仅与支持向量有关。</p>

<h4 id="312-软间隔-svm-优化问题"><span class="me-2">3.1.2. 软间隔 SVM 优化问题</span><a href="#312-软间隔-svm-优化问题" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<h5 id="3121-优化问题描述"><span class="me-2">3.1.2.1. 优化问题描述</span><a href="#3121-优化问题描述" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>前面一直假定训练样本在样本空间或特征空间线性可分，即存在一个超平面将不同类的样本完全划分开。但现实中，很难确定使得训练样本在特征空间中线性可分。缓解该问题的办法是允许支持向量机出错，可以通过在目标函数中额外添加一项 $\text{loss}$ 来实现这一点，优化问题如下</p>

\[\begin{aligned}
\min_{\boldsymbol{w},b} &amp;\quad \frac{1}{2} \boldsymbol{w}^\top \boldsymbol{w} + \text{loss}\\
s.t. &amp;\quad  y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) \geq 1, \;\forall i=1,2,\cdots,N
\end{aligned}\]

<p>$\text{loss}$ 选取可以有以下几种形式，第一种是引入 $0/1$ 损失函数作为惩罚项，每当有一个样本不满足约束则计数加 $1$，最后寻找参数 $(\boldsymbol{w},b)$ 使得包含计数的损失函数最小</p>

\[\begin{aligned}
  \text{loss} &amp;= C\sum_{i=1}^N l_{0/1}\{1-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b)\}, \;\forall i=1,2,\cdots,N\\
  l_{0/1} &amp;= \begin{cases}
    0,\;\text{if}\;1-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) \leq 0 \\
    1,\;\text{if}\;1-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) &gt; 0 \;\text{违背约束}
  \end{cases}
\end{aligned}\]

<p>但 $0/1$ 损失函数非凸、非连续，不容易优化，需要采用其他替代损失。几种可行的替代损失如下图所示</p>

<p><a href="/assets/img/postsimg/20250330/hinge-loss.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250330/hinge-loss.jpg" alt="hinge-loss" class="lazyload" data-proofer-ignore></a></p>

<blockquote>
  <p>求解替代损失函数得到的解是否仍为原问题的解，在理论上称为替代损失的 “一致性” 问题。替代损失一般选取为原始损失损失函数的上界，这样替代损失函数得到优化时，原始损失函数也同样得到优化。此外，替代函数一般选为凸函数，便于优化。</p>
</blockquote>

<p>上图中列出的几种常见替代损失都已被证明是一致的，可以替代 $0/1$ 损失函数。SVM 求解选取 <strong>hinge loss</strong> 作为替代损失，同时引入 $C$ 作为惩罚参数</p>

\[\text{loss} = C\sum_{i=1}^N\max\{ 0, 1-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) \}\]

<p>可以看出，hinge loss 在约束满足时取值为 0，在约束不满足时取值即为约束的正值，两个区间的边界处的函数值是连续的。引入这个惩罚项后，假设有样本不满足约束，我们通过优化带上述惩罚项的目标函数，也希望尽可能使得不满足约束的样本带来的影响（对约束的违背程度）尽可能小。</p>

<p>虽然 hinge loss 函数取值已经连续了，但由于存在 $\max$ 算符，在实际使用时仍然不太方便。这里把 hinge loss 单独定义为一个变量 $\xi_i$，并且要求 $\xi_i \geq 0$，即</p>

\[\xi_i =  \max\{0,1-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b)\}\]

<p>可进行如下讨论</p>

<ul>
  <li>当 $\xi_i = 0$ 时， 样本满足 $y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) \geq 1$；</li>
  <li>当 $\xi_i &gt; 0$ 时， 样本满足 $y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) = 1 - \xi_i$；</li>
</ul>

<p>因此惩罚项可以移除 $\max$ 写成统一的形式，约束不等式也可以写为统一的形式。则优化问题可以写成如下的 <strong>软间隔 SVM 优化问题</strong></p>

\[\begin{aligned}
\min_{\boldsymbol{w},b,\xi_i} &amp;\quad \frac{1}{2} \boldsymbol{w}^\top \boldsymbol{w} \textcolor{red}{+ C\sum_{i=1}^N\xi_i}\\
s.t. &amp;\quad  y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b) \geq \textcolor{red}{1-\xi_i},\; \xi_i \geq 0,\; \forall i=1,2,\cdots,N
\end{aligned}\]

<p>其中 $\xi_i$ 又称为<strong>松弛变量</strong>，引入松弛变量相当于我们放宽了对样本点的约束，原本要求其必须在硬间隔之外，现在只要都保证在软间隔之外即可。</p>

<p>最小化目标函数包含两层含义：第一项使硬间隔尽量大，第二项使误分类点的个数尽量小，$C$ 是调和二者的系数。并且只要错分一个样本点，我们都将付出 $C\xi_i$ 的代价。极端情况下，当 $C\to \infty$，表明此时我们无法容忍误分类样本点，只要有一丁点误分就会导致目标函数变得很大，相当于 $\xi_i\to 0$，软间隔 SVM 问题退化成为硬间隔 SVM 问题。</p>

<p>软间隔 SVM 优化问题的图示如下所示</p>

<p><a href="/assets/img/postsimg/20250330/soft-margin-svm.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250330/soft-margin-svm.jpg" alt="soft-margin-svm" class="lazyload" data-proofer-ignore></a></p>

<h5 id="3122-拉格朗日乘子"><span class="me-2">3.1.2.2. 拉格朗日乘子</span><a href="#3122-拉格朗日乘子" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>同样采用拉格朗日乘子法，上述软间隔 SVM 优化问题可以写成如下形式</p>

\[\begin{aligned}
&amp;\min_{w,b,\xi_i} \max_{\lambda, \mu}L(\boldsymbol{w},b,\xi,\lambda, \mu) = \\
&amp;\frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w} + C\sum_{i=1}^N \xi_i \textcolor{red}{+ \sum_{i=1}^N \lambda_i(1-\xi_i-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b)) - \sum_{i=1}^N \mu_i\xi_i}\\
&amp;s.t. \quad \xi_i \geq 0, \lambda_i \geq 0,\mu_i \geq 0, \quad \forall i=1,2,\cdots,N
\end{aligned}\]

<h5 id="3123-对偶问题求解"><span class="me-2">3.1.2.3. 对偶问题求解</span><a href="#3123-对偶问题求解" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>同样将原问题转化为对偶问题</p>

\[\begin{aligned}
&amp;\textcolor{green}{\max_{\lambda, \mu} \min_{w,b,\xi_i}} L(\boldsymbol{w},b,\xi,\lambda, \mu) = \\
&amp;\frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w} + C\sum_{i=1}^N \xi_i + \sum_{i=1}^N \lambda_i(1-\xi_i-y_i(\boldsymbol{w}^\top \boldsymbol{x_i}+b)) - \sum_{i=1}^N \mu_i\xi_i\\
&amp;s.t. \quad \xi_i \geq 0, \lambda_i \geq 0,\mu_i \geq 0, \quad \forall i=1,2,\cdots,N
\end{aligned}\]

<p>根据 KKT 条件中的梯度可行性条件，可以得到如下的等式：</p>

\[\begin{cases}
  \nabla_w L(\boldsymbol{w},b,\xi,\lambda, \mu) = \boldsymbol{w} - \sum_{i=1}^N \lambda_i y_i \boldsymbol{x_i} = 0\\
  \nabla_b L(\boldsymbol{w},b,\xi,\lambda, \mu) = - \sum_{i=1}^N \lambda_i y_i = 0, \quad \forall i=1,2,\cdots,N\\
  \nabla_{\xi_i} L(\boldsymbol{w},b,\xi,\lambda, \mu) = C - \lambda_i - \mu_i = 0\\
\end{cases}\]

<p>可以得到</p>

\[\begin{cases}
  \boldsymbol{w} = \sum_{i=1}^N \lambda_i y_i \boldsymbol{x_i}\\
  \sum_{i=1}^N \lambda_i y_i=0, \quad \forall i=1,2,\cdots,N\\
  C-\lambda_i-\mu_i=0
\end{cases}\]

<p>又根据 KKT 条件中的互补松弛性等条件，对于 $ \forall i=1,2,\cdots,N$</p>

\[\begin{aligned}
\lambda_i \geq 0,\mu_i \geq 0 &amp;\quad \text{对偶可行性}\\
1-\xi_i-y_i(\boldsymbol{w}^T\boldsymbol{x_i}+b) \leq 0,\;\xi_i\geq 0 &amp;\quad \text{原始可行性}\\
\lambda_i(1-\xi_i-y_i(\boldsymbol{w}^T\boldsymbol{x_i}+b))=0 &amp;\quad \text{互补松弛性}\\
\mu_i\xi_i=0 &amp;\quad \text{互补松弛性}
\end{aligned}\]

<p>对于第 $i$ 个样本分析如下</p>

<ul>
  <li>$ \lambda_i = 0$：样本不是支持向量；</li>
  <li>梯度可行条件条件：$\lambda_i+\mu_i=C$，因此 $ \mu \geq 0 \Rightarrow \lambda_i \leq C$
    <ul>
      <li>$0&lt;\lambda_i &lt; C \Rightarrow \mu_i &gt; 0 \Rightarrow \xi_i = 0$：样本是支持向量；</li>
      <li>$\lambda_i = C \Rightarrow \mu_i = 0$：$\xi_i$ 取值任意，样本不是支持向量；
        <ul>
          <li>$0\leq \xi_i &lt; 1$：样本落在最大间隔内部；</li>
          <li>$\xi_i = 1$：样本落在分类超平面上；</li>
          <li>$\xi_i &gt; 1$：样本被错误分类；</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>代入原始目标函数可得化简的目标函数（隐去 $\mu_i$）为</p>

\[\begin{aligned}
\max_\lambda &amp;\quad -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \lambda_i \lambda_j y_i y_j \boldsymbol{x_i}^\top \boldsymbol{x_j} + \sum_{i=1}^N \lambda_i\\
s.t. &amp;\quad \sum_{i=1}^N \lambda_i y_i=0,\; \textcolor{green}{0\leq \lambda_i \leq C},\; i=1,2,\cdots,N\\
\end{aligned}\]

<p>上述标绿约束也是 软间隔 SVM 与 硬间隔 SVM 优化问题的唯一区别。利用 KKT 条件和 SMO 算法继续求解拉格朗日系数 $\lambda_i$，此处不在赘述。</p>

<p>最后我们也可以解得优化问题的最优解为</p>

\[\begin{aligned}
w^\star &amp;= \sum_{i=1}^N \lambda_i y_i \boldsymbol{x_i}\\
b^\star &amp;= y_k-\sum_{i=1}^N \lambda_i y_i \boldsymbol{x_i}^\top \boldsymbol{x}_k\quad \text{support vector}\; (\boldsymbol{x}_k,y_k)
\end{aligned}\]

<p>得到 <strong>SVM 的判别函数</strong> 如下</p>

\[f(x) = \text{sign}({\boldsymbol{w}^\star}^\top x + b^\star)\]

<p>其形式和 硬间隔 SVM 的最优解形式是一样的。</p>

<h4 id="313-统计机器学习的一般形式"><span class="me-2">3.1.3. 统计机器学习的一般形式</span><a href="#313-统计机器学习的一般形式" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>我们用一个统一的目标函数同时刻画 硬间隔 SVM 和 软间隔 SVM 的目标函数，即：</p>

\[\min_f \quad \underbrace{\Omega(f)}_{结构风险} + \underbrace{C\sum_{i=1}^N l(f(\boldsymbol{x}_i),y_i)}_{经验风险}\]

<p>其中</p>

<ul>
  <li><strong>结构风险</strong>（Structural Risk）：描述模型本身的某些性质，通常称为正则化项，使用正则化项的方法称之为 “罚函数法”；</li>
  <li><strong>经验风险</strong>（Experience Risk）：描述模型在训练数据集上的拟合程度。</li>
</ul>

<p>相比于其他基于数据的机器学习，SVM 这类基于统计的机器学习通过对不希望的结果（模型选择）施加惩罚，使得优化过程趋向于希望目标。</p>

<p>从贝叶斯估计的角度，则可认为结构风险这一正则化项是提供了模型的先验概率（对模型的喜好和态度），或者称为归纳偏好。</p>

<h3 id="32-非线性支持向量机"><span class="me-2">3.2. 非线性支持向量机</span><a href="#32-非线性支持向量机" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>对于非线性可分问题，我们本着简化问题的思想，自然是希望将其转化为熟悉的线性可分问题进行处理。</p>

<p>Cover 定理指出：将复杂的模式分类问题非线性地投射到高维空间将比投射到低维空间更可能是线性可分的。极端情况下，在维度超过数据数量时，数据一定线性可分（试想如果我们把每个数据点都映射到不同的坐标轴上，那么可不就是线性可分的了么）。</p>

<p>因此，我们对非线性可分的数据，可以将数据映射至高维空间，然后再用我们熟悉的线性支持向量机分类，至此，剩下的问题就是怎么映射至高维。</p>

<blockquote>
  <p>通过坐标变换而不进行升维在一些特殊场景下也能实现分类，如</p>

  <p><a href="/assets/img/postsimg/20250330/coordinate-tranformation.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250330/coordinate-tranformation.jpg" alt="coordinate-tranformation" class="lazyload" data-proofer-ignore></a></p>

  <p>使用极坐标 $\phi(x):[x_1,x_2]^\top \to [r,\theta]^\top,\quad \mathbb{R}^2\to \mathbb{R}^2$</p>

  <p>但这种映射方式适用面较窄，并不常用。</p>
</blockquote>

<h4 id="321-特征升维分类实例"><span class="me-2">3.2.1. 特征升维分类实例</span><a href="#321-特征升维分类实例" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>考虑如下四个样本，需要将其进行分类，可以发现没有任何一个线性分类面能够完成此任务</p>

\[\begin{aligned}
\text{class 1}:\;&amp;(0,3),(3,0)\\
\text{class 2}:\;&amp;(2,1),(1,2)
\end{aligned}\]

<p><a href="/assets/img/postsimg/20250330/a-simple-sample.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250330/a-simple-sample.jpg" alt="a-simple-example" class="lazyload" data-proofer-ignore></a></p>

<p>我们尝试通过升维操作来解决分类问题。那么第三个维度可以选什么呢？我们尝试如下三种</p>

\[(x,y,\textcolor{red}{x+y}),(x,y,\textcolor{green}{xy}),(x,y,\textcolor{blue}{x^2})\]

<p>分别求得三种情况下的第三维度计算结果</p>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th> </th>
      <th>$\textcolor{red}{(0,3)}$</th>
      <th>$\textcolor{green}{(1,2)}$</th>
      <th>$\textcolor{green}{(2,1)}$</th>
      <th>$\textcolor{red}{(3,0)}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$x+y$</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <td>$xy$</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>$x^2$</td>
      <td>0</td>
      <td>1</td>
      <td>4</td>
      <td>9</td>
    </tr>
  </tbody>
</table></div>

<p>很明显，使用第三个维度为 $xy$ 时能够区分两类不同的样本。结果如图所示</p>

<p><a href="/assets/img/postsimg/20250330/a-simple-sample-trick.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250330/a-simple-sample-trick.jpg" alt="a-simple-sample-trick" class="lazyload" data-proofer-ignore></a></p>

<p>上述过程实际上是一个朴素的特征映射过程，其仍然存在以下两个问题：</p>

<ul>
  <li>特征映射需要凭借经验设计（如设计第三个维度为 $xy$）；</li>
  <li>特征映射引入了额外的计算成本（后面展开介绍）。</li>
</ul>

<h4 id="322-核技巧"><span class="me-2">3.2.2. 核技巧</span><a href="#322-核技巧" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>观察线性 SVM 的优化目标函数</p>

\[\max_\lambda \quad -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \lambda_i \lambda_j y_i y_j \boldsymbol{x_i}^\top \boldsymbol{x_j} + \sum_{i=1}^N \lambda_i\]

<p>可以看出其主要通过<strong>计算样本的内积来优化得到分类超平面</strong>。由于升维可以使得原本线性不可分的样本变得线性可分，其关键在于如何对样本进行升维后求内积。</p>

<p>假设我们用一个映射函数 $\phi(\cdot)$ 将原始样本升维，那么优化函数变为</p>

\[\max_\lambda \quad -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \lambda_i \lambda_j y_i y_j \phi(\boldsymbol{x_i})^\top \phi(\boldsymbol{x_j}) + \sum_{i=1}^N \lambda_i\]

<p>假设样本在映射函数 $\phi(\cdot)$ 的作用下从二维升至六维，如下</p>

\[\boldsymbol{x} = [a, b]^\top\mathop{\longrightarrow}\limits^{\phi}\phi(\boldsymbol{x})=[1,\sqrt{2}a,\sqrt{2}b,a^2,b^2,\sqrt{2}ab]^\top\]

<p>那么<font color="red">先映射后再求内积</font>可以得到</p>

\[\phi(\boldsymbol{x}_1)\cdot \phi(\boldsymbol{x}_2) = 1+2a_1a_2+2b_1b_2+a_1^2a_2^2+b_1^2b_2^2+2a_1a_2b_1b_2\]

<p>可以看出，</p>

<ul>
  <li>设计六维特征已经十分繁杂，更高维度特征设计我们无从下手；</li>
  <li>随着维度的增加，求内积操作变得十分复杂。</li>
</ul>

<p>下面通过引入核函数的方式解决上述两个问题。</p>

<h5 id="3221-多项式核函数"><span class="me-2">3.2.2.1. 多项式核函数</span><a href="#3221-多项式核函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>我们转换思想，尝试 <font color="red">先求内积再进行映射</font>，定义如下关于内积的函数：</p>

\[\begin{aligned}
\mathcal{K}(\boldsymbol{x}_1,\boldsymbol{x}_2) &amp;= (1+\boldsymbol{x}_1^\top\cdot\boldsymbol{x}_2)^2\\
&amp;=(1+a_1a_2+b_1b_2)^2\\
&amp;=1+2a_1a_2+2b_1b_2+a_1^2a_2^2+b_1^2b_2^2+2a_1a_2b_1b_2
\end{aligned}\]

<p>可以发现，其结果正好等于前述映射到六维后的样本内积结果。</p>

<p>上述操作使用的函数被称为核函数，通过引入核函数来简化内积计算的这种操作被称为<strong>核技巧</strong>，具体而言，这里采用的是 <strong>多项式核函数</strong></p>

\[\mathcal{K}_{poly}^{(c,d)}(\boldsymbol{x}_1,\boldsymbol{x}_2) = (c+\boldsymbol{x}_1^\top\cdot\boldsymbol{x}_2)^d\]

<p>当 $c=1,d=2$ 时，就可以将原本的样本升至前述六维。如果换一组参数，可以将原样本升至不同的维度。因此，<u>多项式核函数确定了一组参数后，也就确定了转换后的样本特征维度。</u></p>

<p>多项式核函数中的常数 $c$ 十分重要，其决定了内积结果的表达始终包含了从低次项到高次项的数据组合，体现出维度的多样性。当 $c=0$ 时，点积展开式将只包含高次项。进一步，我们可以通过将多个不同次的多项式核函数相加组合，是结果同时具有高低次项，以此丰富维度的多样性</p>

\[\begin{aligned}
&amp;\textcolor{red}{c=1,d=2:}\\
&amp;\mathcal{K}(\boldsymbol{x}_1,\boldsymbol{x}_2) =\textcolor{green}{1+2a_1a_2+2b_1b_2}+a_1^2a_2^2+b_1^2b_2^2+2a_1a_2b_1b_2\\
&amp;\textcolor{red}{c=0,d=2:}\\
&amp;\mathcal{K}(\boldsymbol{x}_1,\boldsymbol{x}_2) =a_1^2a_2^2+b_1^2b_2^2+2a_1a_2b_1b_2\\
&amp;\textcolor{red}{(c=0,d=2)+(c=0,d=2):}\\
&amp;\mathcal{K}(\boldsymbol{x}_1,\boldsymbol{x}_2) =\textcolor{green}{2a_1a_2+2b_1b_2}+a_1^2a_2^2+b_1^2b_2^2+2a_1a_2b_1b_2\\
\end{aligned}\]

<p>多项式核函数通过构建特征的高次组合，适用于存在较明显非线性但复杂度适中的数据。多项式阶数的选择直接影响模型的复杂度和拟合能力，过高可能导致过拟合，过低可能无法捕捉数据的非线性特性。</p>

<p>虽然引入核函数可以在某种程度上简化高维特征的设计问题（通过调节参数 $c$ 和 $d$ 即可直接得到不同的高维特征），并且可以降低计算量。即我们可以首先计算出特征在原始低维空间的内积，然后直接代入核函数公式中，得到其在高维特种中的内积。</p>

<p>但是多项式核函数仍然存在以下局限性：</p>

<ul>
  <li>阶数 $d$ 的选择困难：
    <ul>
      <li>$d$ 太小可能导致模型欠拟合，无法捕捉复杂模式；</li>
      <li>$d$ 太大容易过拟合，且计算复杂度高（特征空间维度爆炸增长）。</li>
    </ul>
  </li>
  <li>
    <p>数值问题：当 $d$ 较大时，$\boldsymbol{x}_i^\top\boldsymbol{x}_j$ 的值可能非常大或非常小，导致数值不稳定。</p>
  </li>
  <li>
    <p>对特征尺度敏感：多项式核的性能受特征尺度影响较大，通常需要标准化数据，而高斯核对尺度敏感度较低（因已包含距离度量）。</p>
  </li>
  <li>灵活性不足：多项式核只能捕捉固定阶数的多项式关系，而高斯核可以近似任意复杂函数。</li>
</ul>

<p>下面介绍的 RBF 核函数能够解决上述绝大部分局限，使用更为广泛。</p>

<h5 id="3222-rbf-核函数"><span class="me-2">3.2.2.2. RBF 核函数</span><a href="#3222-rbf-核函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>根据 Cover 定理，维度越高越有可能使得样本变得线性可分。极端情况下，如果希望样本特征升至无限维，那么原本的先映射再计算内积的方法就失效了，因为我们无法对映射后的无限维特征进行求内积操作。因此我们只能使用核技巧来计算。</p>

<p>根据前面的介绍，多项式核函数在 $d$ 太大时有很多问题，这里我们给出另一种核函数，被称为 <strong>RBF 核函数</strong>（Radial Basis Function）、径向基核函数 或 高斯核函数。其定义如下</p>

\[\begin{aligned}
RBF(\boldsymbol{x}_i, \boldsymbol{x}_j) &amp; = \exp\left(-\frac{\Vert \boldsymbol{x}_i-\boldsymbol{x}_j\Vert^2}{2\sigma^2}\right) \\
\end{aligned}\]

<p>其反应了两个样本向量的相似度信息，其参数 $\sigma$ 是核函数的参数，用来调控对样本向量之间的相似度和距离之间的敏感程度。</p>

<p>简便起见，令 $\sigma = 1$，对 RBF 核函数展开如下</p>

\[\begin{aligned}
RBF(\boldsymbol{x}_i, \boldsymbol{x}_j) &amp;= \exp\left(-\frac{\Vert \boldsymbol{x}_i-\boldsymbol{x}_j\Vert^2}{2}\right) \\
&amp;=\exp\left(-\frac{1}{2}(\boldsymbol{x}_i-\boldsymbol{x}_j)^\top(\boldsymbol{x}_i-\boldsymbol{x}_j)\right) \\
&amp;=\exp\left(-\frac{1}{2}[\boldsymbol{x}_i^\top\boldsymbol{x}_i+\boldsymbol{x}_j^\top\boldsymbol{x}_j - 2\boldsymbol{x}_i^\top\boldsymbol{x}_j]\right)\\
&amp;= \exp\left(-\frac{1}{2}[\boldsymbol{x}_i^\top\boldsymbol{x}_i+\boldsymbol{x}_j^\top\boldsymbol{x}_j]\right)\exp\left(\boldsymbol{x}_i^\top\boldsymbol{x}_j\right)\\
&amp;=C\cdot\exp\left(\boldsymbol{x}_i^\top\boldsymbol{x}_j\right)
\end{aligned}\]

<p>可以看出，RBF 核函数是关于两向量点积的指数函数，对其进行泰勒展开可以得到</p>

\[RBF(\boldsymbol{x}_i,\boldsymbol{x}_j) = C\cdot\sum_{n=0}^{\infty}\frac{1}{n!}\left(\boldsymbol{x}_i^\top\boldsymbol{x}_j\right)^n = C\cdot\sum_{n=0}^{\infty}\frac{1}{n!}\left(\mathcal{K}_{poly}^{(0,n)}\right)\]

<blockquote>
\[\exp(x) = \sum_{n=0}^{\infty}\frac{1}{n!}x^n\]
</blockquote>

<p>因此，<u>RBF 核函数可以看作无数个不带常数 $c$ 的多项式核函数从低次到高次的加权和</u>，<strong>可以表达高低次项的无限多样性</strong>。这样，我们就实现了在不实质踏入无限维度的情况下，得到无限维度下向量相似度的结果。</p>

<p>RBF 核函数是最常用的非线性核函数之一，尤其适用于高维、复杂非线性数据。其参数 $\gamma = \frac{1}{2\sigma^2}$ 决定了核函数的宽度，对模型的复杂度和分类效果有显著影响。RBF核因其局部性、平滑性和无限维映射等特性，常能在保持较低模型复杂度的同时获得良好的分类性能。</p>

<h2 id="4-决策树dt"><span class="me-2">4. 决策树（DT）</span><a href="#4-决策树dt" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h2 id="5-分类器集成"><span class="me-2">5. 分类器集成</span><a href="#5-分类器集成" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h2 id="6-参考文献"><span class="me-2">6. 参考文献</span><a href="#6-参考文献" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>[1] 周志华. 机器学习. 北京：清华大学出版社, 2016.</p>

<p>[2] T. M. Cover and P. E. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13:21–27, January 1967.</p>

<p>[3] <a href="https://www.bilibili.com/video/BV1Hs411w7ci/">blilbili. SVM白板推导(六)-支持向量机SVM（Support Vector Machine）</a>.</p>

<p>[4] <a href="https://zhuanlan.zhihu.com/p/335017540">知乎. 机器学习-白板推导系列(六)-支持向量机SVM（Support Vector Machine）笔记</a>.</p>

</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw me-1"></i>
    
      <a href='/categories/academic/'>Academic</a>,
      <a href='/categories/knowledge/'>Knowledge</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw me-1"></i>
      
      <a href="/tags/pattern-recognition/"
          class="post-tag no-text-decoration" >pattern recognition</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        本文由作者按照 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         进行授权

      
    </div>

    <!-- Post sharing snippet -->

<div class="share-wrapper">
  <span class="share-label text-muted me-1">分享</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%EF%BC%88%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%89%20-%20SIRLIS&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FPattern-Recognition-Nonlinear-Classifier%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%EF%BC%88%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%89%20-%20SIRLIS&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FPattern-Recognition-Nonlinear-Classifier%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FPattern-Recognition-Nonlinear-Classifier%2F&text=%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%EF%BC%88%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%89%20-%20SIRLIS"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <i
      id="copy-link"
      class="fa-fw fas fa-link small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="分享链接"
      data-title-succeed="链接已复制！"
    >
    </i>
  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
      
    </div>
  </div>
  <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
    <div class="access">
      <!-- Get the last 5 posts from lastmod list. -->














  <div id="access-lastmod" class="post">
    <div class="panel-heading">最近更新</div>
    <ul class="post-content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Nonlinear-Classifier/">模式识别（非线性分类器）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Bayes/">模式识别（贝叶斯决策）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Temporal-Differences/">强化学习（时序差分法）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-markov-process/">强化学习（马尔可夫决策过程）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Monte-Carlo/">强化学习（蒙特卡洛法）</a>
        </li>
      
    </ul>
  </div>
  <!-- #access-lastmod -->


      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>

    
      
      



  <div id="toc-wrapper" class="ps-0 pe-4 mb-5">
    <div class="panel-heading ps-3 pt-2 mb-2">文章内容</div>
    <nav id="toc"></nav>
  </div>


    
  </div>
</div>

<!-- tail -->

  <div class="row">
    <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-3 pe-xl-4 mt-5">
      
        
        <!--
  Recommend the other 3 posts according to the tags and categories of the current post,
  if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->








  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  






<!-- Fill with the other newlest posts -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ms-1" data-toc-skip>
      相关文章
    </h3>
    <div class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        
        
        <div class="col">
          <a href="/posts/Pattern-Recognition-Linear-Classifier/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1709447119"
  data-df="YYYY/MM/DD"
  
>
  2024/03/03
</em>

              <h4 class="pt-0 my-2" data-toc-skip>模式识别（线性分类器）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了模式识别的线性分类器，包括线性分类器的基础概念、垂直平分准则以及垂直平分分类器、Fisher 投影准则、感知准则、最小错分样本准则、最小平方误差准则、最小二乘估计。






  1. 引言
  2. 线性分类器基础
    
      2.1. 线性判别函数
      2.2. 广义线性判别函数
      2.3. 二分类与多分类
      2.4. 线性分类器设计的基...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/Pattern-Recognition-Bayes/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1710249739"
  data-df="YYYY/MM/DD"
  
>
  2024/03/12
</em>

              <h4 class="pt-0 my-2" data-toc-skip>模式识别（贝叶斯决策）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了模式识别的贝叶斯决策，包括贝叶斯公式、最小错误贝叶斯决策、最小风险贝叶斯决策。






  1. 决策理论与方法
    
      1.1. 基于先验概率的决策
      1.2. 基于贝叶斯公式（后验概率）的决策
    
  
  2. 最小错误贝叶斯决策
    
      2.1. 直观举例
      2.2. 类概率密度
      2.3. 错误率分析
 ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/Pattern-Recognition-LDA&PCA/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1714615399"
  data-df="YYYY/MM/DD"
  
>
  2024/05/02
</em>

              <h4 class="pt-0 my-2" data-toc-skip>模式识别（LDA和PCA）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了模式识别种常用的两种特征选择与特征降维方法，即线性判别分析（Linear Discriminant Analysis，LDA）和主成分分析（Principal Component Analysis，PCA）。






  1. 特征降维
  2. 线性判别分析（LDA）
    
      2.1. Fisher 投影准则
      2.2. 瑞利商与广义瑞利商
     ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
    </div>
    <!-- .card-deck -->
  </div>
  <!-- #related-posts -->


      
        
        <!-- Navigation buttons at the bottom of the post. -->

<div class="post-navigation d-flex justify-content-between">
  
    <a
      href="/posts/deep-learning-basic-math/"
      class="btn btn-outline-primary"
      prompt="上一篇"
    >
      <p>深度学习（基础数学知识）</p>
    </a>
  

  
    <div
      class="btn btn-outline-primary disabled"
      prompt="下一篇"
    >
      <p>-</p>
    </div>
  
</div>

      
        
        <!--  The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script src="https://utteranc.es/client.js"
        repo="sirlis/sirlis.github.io"
        issue-term="pathname"
        crossorigin="anonymous"
        async>
</script>

<script type="text/javascript">
  $(function() {
    const origin = "https://utteranc.es";
    const iframe = "iframe.utterances-frame";
    const lightTheme = "github-light";
    const darkTheme = "github-dark";
    let initTheme = lightTheme;

    if ($("html[data-mode=dark]").length > 0
        || ($("html[data-mode]").length == 0
            && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
      initTheme = darkTheme;
    }

    addEventListener("message", (event) => {
      let theme;

      /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */
      if (event.origin === origin) {
        /* page initial */
        theme = initTheme;

      } else if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {
        /* global theme mode changed */
        const mode = event.data.message;
        theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme);

      } else {
        return;
      }

      const message = {
        type: "set-theme",
        theme: theme
      };

      const utterances = document.querySelector(iframe).contentWindow;
      utterances.postMessage(message, origin);
    });

  });
</script>



      
    </div>
  </div>


        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 post-content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>
    </div>

    <!-- The Footer -->

<footer>
  <div class="container px-lg-4">
    <div class="d-flex justify-content-center align-items-center text-muted mx-md-3">
      <p>本站采用 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 主题 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
      </p>

      <p>©
        2025
        <a href="https://github.com/sirlis">sirlis</a>.
        
          <span
            data-bs-toggle="tooltip"
            data-bs-placement="top"
            title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。"
          >保留部分权利。</span>
        
      </p>
    </div>
  </div>
</footer>


    <div id="mask"></div>

    <button id="back-to-top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow">
      <i class="fas fa-angle-up"></i>
    </button>

    
      <div
        id="notification"
        class="toast"
        role="alert"
        aria-live="assertive"
        aria-atomic="true"
        data-bs-animation="true"
        data-bs-autohide="false"
      >
        <div class="toast-header">
          <button
            type="button"
            class="btn-close ms-auto"
            data-bs-dismiss="toast"
            aria-label="Close"
          ></button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="px-2 mb-3">发现新版本的内容。</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            更新
          </button>
        </div>
      </div>
    

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.0/dist/jquery.min.js,npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.7/dayjs.min.js,npm/dayjs@1.11.7/locale/zh.min.js,npm/dayjs@1.11.7/plugin/relativeTime.min.js,npm/dayjs@1.11.7/plugin/localizedFormat.min.js,npm/tocbot@4.21.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '<div class="px-1 px-sm-2 px-lg-4 px-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

