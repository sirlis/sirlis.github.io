<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="zh-CN" 
  
>
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="强化学习（时序差分法）" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="本文首先引入了随机近似理论，然后通过比较动态规划和蒙特卡洛，引出结合二者优势的时序差分法。通过分析可知，时序差分法是随机近似理论的一个特例。随后详细介绍了同轨策略下的时序差分控制（SARSA）、离轨策略下的时序差分控制（Q-Learning）和期望SARSA。最后介绍了基于价值的深度强化学习方法：Deep Q-Network（DQN）。" />
<meta property="og:description" content="本文首先引入了随机近似理论，然后通过比较动态规划和蒙特卡洛，引出结合二者优势的时序差分法。通过分析可知，时序差分法是随机近似理论的一个特例。随后详细介绍了同轨策略下的时序差分控制（SARSA）、离轨策略下的时序差分控制（Q-Learning）和期望SARSA。最后介绍了基于价值的深度强化学习方法：Deep Q-Network（DQN）。" />
<link rel="canonical" href="http://localhost:4000/posts/reinforcement-learning-Temporal-Differences/" />
<meta property="og:url" content="http://localhost:4000/posts/reinforcement-learning-Temporal-Differences/" />
<meta property="og:site_name" content="SIRLIS" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-18T14:59:19+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="强化学习（时序差分法）" />
<meta name="twitter:site" content="@none" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-09T00:13:33+08:00","datePublished":"2022-12-18T14:59:19+08:00","description":"本文首先引入了随机近似理论，然后通过比较动态规划和蒙特卡洛，引出结合二者优势的时序差分法。通过分析可知，时序差分法是随机近似理论的一个特例。随后详细介绍了同轨策略下的时序差分控制（SARSA）、离轨策略下的时序差分控制（Q-Learning）和期望SARSA。最后介绍了基于价值的深度强化学习方法：Deep Q-Network（DQN）。","headline":"强化学习（时序差分法）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/reinforcement-learning-Temporal-Differences/"},"url":"http://localhost:4000/posts/reinforcement-learning-Temporal-Differences/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>强化学习（时序差分法） | SIRLIS
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SIRLIS">
<meta name="application-name" content="SIRLIS">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.0/dist/tocbot.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/head.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <div class="site-title">
      <a href="/">SIRLIS</a>
    </div>
    <div class="site-subtitle fst-italic">分享科研和生活的日常</div>
  </div>
  <!-- .profile-wrapper -->

  <ul class="nav flex-column flex-grow-1 w-100 ps-0">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home"></i>
        <span>首页</span>
      </a>
    </li>
    <!-- the real tabs -->
    
      <li class="nav-item">
        <a href="/categories/" class="nav-link">
          <i class="fa-fw fas fa-stream"></i>
          

          <span>分类</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/tags/" class="nav-link">
          <i class="fa-fw fas fa-tags"></i>
          

          <span>标签</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/archives/" class="nav-link">
          <i class="fa-fw fas fa-archive"></i>
          

          <span>归档</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/about/" class="nav-link">
          <i class="fa-fw fas fa-info-circle"></i>
          

          <span>关于</span>
        </a>
      </li>
      <!-- .nav-item -->
    
  </ul>
  <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/sirlis"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/none"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['lihongjue','buaa.edu.cn'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</div>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container px-xxl-5">
        <!-- The Top Bar -->

<div id="topbar-wrapper">
  <div
    id="topbar"
    class="container d-flex align-items-center justify-content-between h-100"
  >
    <span id="breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                首页
              </a>
            </span>

          
        
          
        
          
            
              <span>强化学习（时序差分法）</span>
            

          
        
      
    </span>
    <!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      文章
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="搜索..."
      >
    </span>
    <span id="search-cancel">取消</span>
  </div>
</div>

        











<div class="row">
  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pe-xl-4">
    

    <div class="post px-1 px-md-2">
      

      
        
      
        <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->



  
  

  <!-- CDN URL -->
  

  <!-- Add image path -->
  

  
    
      
      
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  




<!-- return -->




<h1 data-toc-skip>强化学习（时序差分法）</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      发表于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1671346759"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2022/12/18
</em>

    </span>

    <!-- lastmod date -->
    
    <span>
      更新于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1744128813"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2025/04/09
</em>

    </span>
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      作者

      <em>
      
        <a href="https://github.com/sirlis">sirlis</a>
      
      </em>
    </span>

    <div>
      <!-- read time -->
      <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="10510 字"
>
  <em>58 分钟</em>阅读</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <p>本文首先引入了随机近似理论，然后通过比较动态规划和蒙特卡洛，引出结合二者优势的时序差分法。通过分析可知，时序差分法是随机近似理论的一个特例。随后详细介绍了同轨策略下的时序差分控制（SARSA）、离轨策略下的时序差分控制（Q-Learning）和期望SARSA。最后介绍了基于价值的深度强化学习方法：Deep Q-Network（DQN）。</p>

<!--more-->

<hr />

<ul>
  <li><a href="#1-引言">1. 引言</a></li>
  <li><a href="#2-随机近似理论">2. 随机近似理论</a>
    <ul>
      <li><a href="#21-求期望的增量更新式">2.1. 求期望的增量更新式</a></li>
      <li><a href="#22-robbins-monrorm-算法">2.2. Robbins-Monro（RM） 算法</a>
        <ul>
          <li><a href="#221-算法推导">2.2.1. 算法推导</a></li>
          <li><a href="#222-收敛性分析">2.2.2. 收敛性分析</a></li>
          <li><a href="#223-回顾增量更新">2.2.3. 回顾增量更新</a></li>
        </ul>
      </li>
      <li><a href="#23-随机梯度下降sgd">2.3. 随机梯度下降（SGD）</a>
        <ul>
          <li><a href="#231-sgd-算法推导">2.3.1. SGD 算法推导</a></li>
          <li><a href="#232-sgd-收敛性分析">2.3.2. SGD 收敛性分析</a></li>
          <li><a href="#233-回顾增量更新">2.3.3. 回顾增量更新</a></li>
          <li><a href="#234-sgd-的确定性形式">2.3.4. SGD 的确定性形式</a></li>
          <li><a href="#235-bgdmbgd-和-sgd">2.3.5. BGD、MBGD 和 SGD</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#3-时序差分法">3. 时序差分法</a>
    <ul>
      <li><a href="#31-时序差分思想temporal-difference">3.1. 时序差分思想（Temporal Difference）</a>
        <ul>
          <li><a href="#311-从蒙特卡洛的角度分析">3.1.1. 从蒙特卡洛的角度分析</a></li>
          <li><a href="#312-从-rm-算法的角度分析">3.1.2. 从 RM 算法的角度分析</a></li>
          <li><a href="#313-从贝尔曼期望方程的角度分析">3.1.3. 从贝尔曼期望方程的角度分析</a></li>
          <li><a href="#314-td-与-mc-的比较">3.1.4. TD 与 MC 的比较</a></li>
        </ul>
      </li>
      <li><a href="#32-同策略时序差分sarsa">3.2. 同策略时序差分（SARSA）</a>
        <ul>
          <li><a href="#321-sarsa">3.2.1. SARSA</a></li>
          <li><a href="#322-expected-sarsa">3.2.2. Expected SARSA</a></li>
          <li><a href="#323-n-step-sarsa">3.2.3. n-step SARSA</a></li>
        </ul>
      </li>
      <li><a href="#33-异策略时序差分q-learning">3.3. 异策略时序差分（Q-Learning）</a>
        <ul>
          <li><a href="#331-q-learning">3.3.1. Q-Learning</a></li>
          <li><a href="#332-同策略与异策略">3.3.2. 同策略与异策略</a></li>
          <li><a href="#333-不同行为策略的探索性">3.3.3. 不同行为策略的探索性</a></li>
        </ul>
      </li>
      <li><a href="#34-总结">3.4. 总结</a>
        <ul>
          <li><a href="#341-表述形式总结">3.4.1. 表述形式总结</a></li>
          <li><a href="#342-求解方程总结">3.4.2. 求解方程总结</a></li>
          <li><a href="#343-行为表现总结">3.4.3. 行为表现总结</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#4-参考文献">4. 参考文献</a></li>
</ul>

<h2 id="1-引言"><span class="me-2">1. 引言</span><a href="#1-引言" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>回顾强化学习的目标：价值估计（预测问题）和策略寻优（控制问题）。在前面的的介绍中，我们分别介绍了两种基于价值的方法，动态规划法和蒙特卡洛法：</p>

<ul>
  <li><strong>动态规划法</strong>（DP）：是基于模型的方法，包含策略评估和策略改进两步，策略评估用来进行价值估计（即预测问题），策略改进用来进行策略优化（控制问题）。</li>
  <li><strong>蒙特卡洛法</strong>（MC）：是无模型的方法，我们无法得到具体模型（动态特性）时，通过采样完整序列后，通过计算 $G_t$ 的均值来近似价值函数。</li>
</ul>

<p>本节介绍第三种基于价值的方法：时序差分法（TD）。首先回顾一下价值函数的等式：</p>

\[\begin{aligned}
v_\pi(s) &amp;= \mathbb{E}_\pi[G_t\vert S_t=s] &amp; {MC}\\
&amp;= \mathbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})\vert S_t=s] &amp; {TD}\\
&amp;= \sum_a\pi(a\vert s) \sum_{s^\prime,r}p(r^\prime,r \vert s,a)(r+\gamma v_\pi(s^\prime)) &amp; {DP}\
\end{aligned}\]

<p>可以看出，基于价值的方法可以根据价值函数的等式不同来划分，其中：</p>
<ul>
  <li>动态规划（DP）：是一种自举的方法。更新 $v_{k+1}$ 时采用上一步的 $v_k$ 进行组装，<strong>缺点：环境动态特性必须已知</strong>；</li>
  <li>蒙特卡洛（MC）：是一种采样的方法。依据大数定律，让样本均值逼近期望，<strong>缺点：必须完整采集一回合</strong>；</li>
  <li>时序差分（TD）：本章节介绍的方法。</li>
</ul>

<h2 id="2-随机近似理论"><span class="me-2">2. 随机近似理论</span><a href="#2-随机近似理论" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="21-求期望的增量更新式"><span class="me-2">2.1. 求期望的增量更新式</span><a href="#21-求期望的增量更新式" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>在蒙特卡洛中介绍增量更新时，我们给出了根据每次采样得到的新 $G_t$ 求取 $G(S_t)$ 均值的增量更新形式如下：</p>

\[\begin{cases}
N &amp;\leftarrow N+1\\
G(S_t) &amp;\leftarrow G(S_t) - \frac{1}{N} (G(S_t) - G_t)
\end{cases}\]

<p>将 $\frac{1}{N(S_t)}$ 替换为 $\alpha$，将待求量 $G(S_t)$ 写为 $w$，将即时获取的量记作 $x_k$，我们可以将其写为更加简洁的一般的形式</p>

\[w_{k+1} \leftarrow w_k - \textcolor{red}{\alpha} (w_k - x_k)\]

<p>其中，$\alpha$ 为步长参数，用来控制更新的速度。随着迭代次数 $k$ 的增加，$w_k$ 逐渐逼近 $x_k$ 的期望 $\mathbb{E}[X]$。</p>

<p>这种算法的优势在于它是渐进式的。一旦收到一个样本，就可以立即获得平均值估计值。然后，平均估算值就可以立即用于后续其他计算。在第 $k$ 步的时候，我不需要把前面所有的样本全部加起来再求平均，只需要通过上式一步的计算就可以得到一个新的平均数。</p>

<p>但是，在更新开始的时候因为数据量比较小，$w_k$ 难以非常精确的逼近 $\mathbb{E}[X]$。即由于样本不足，平均值估计在开始时并不准确。不过，有总比没有好，总比一直等到最后才能有一个数来得到一个平均数要强。在这个过程中就算不精确，也可以先凑合用着。随着样本的增多，数据越来越大，只要保证逐渐能够逼近期望就行。</p>

<p>随之而来的问题是：</p>

<ul>
  <li>上述计算是否仍能保证收敛至 $\mathbb{E}[X]$？；</li>
  <li>上述计算属于什么类型的算法？</li>
</ul>

<p>这里可以告诉大家，上述计算是 <strong>随机近似理论</strong>（Stochastic Approximation, SA） 的一个特殊形式，也是 <strong>随机梯度下降</strong>（Stochastic Gradient Descent, SGD） 的一个特殊形式。我们将在分别后面详细介绍。</p>

<h3 id="22-robbins-monrorm-算法"><span class="me-2">2.2. Robbins-Monro（RM） 算法</span><a href="#22-robbins-monrorm-算法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="221-算法推导"><span class="me-2">2.2.1. 算法推导</span><a href="#221-算法推导" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>随机近似理论是解决寻根（方程求解）或优化问题的一大类随机迭代算法的总称。与许多其他寻根（方程求解）算法（如基于梯度的方法，梯度下降或梯度上升）相比，其的强大之处在于它不需要知道目标函数的表达式，也不需要知道目标函数梯度的表达式。</p>

<p>其中，<strong>Robbins-Monro（RM）算法</strong>，是随机近似理论中的先驱性工作，它由 John Robbins 和 Paul Monro 在 1951 年提出。著名的随机梯度下降算法则是 RM 算法的一个特殊形式。该算法也可用来分析上述提到的均值估计的收敛性。</p>

<p><strong>【问题描述】</strong>：假设需要求解 $g(w) = 0$ 的根，其中 $w\in\mathbb{R}, g:\mathbb{R}\rightarrow \mathbb{R}$。注意到：</p>

<ul>
  <li>这个简单的寻根问题是许多其他复杂问题的最终形式，如假设 $J(w)$ 是一个待优化的目标函数，我们需要求解 $J(w)$ 的极值，问题就转化为 $g(w) = \nabla_w J(w)=0$（必要条件）；</li>
  <li>对于 $g(w)=c$ 这类问题的寻根也可转化为 $g(w) - c = 0$ 的寻根问题。</li>
</ul>

<p>如果函数 $g$ 的表达式已知，那么采取很多数值方法就可以求解。但如果函数表达式未知，比如是一个神经网络，那么问题就变成我输入什么样的 $w$ 能够使神经网络的输出为 0 ？</p>

<p><strong>【RM 算法】</strong>：是一种迭代式算法，的求解方法为</p>

\[w_{k+1} = w_k-\alpha \tilde{g}(w_k,\eta_k), \quad k=1,2,\ldots\]

<p>其中</p>

<ul>
  <li>$w_k$ 是第 $k$ 步的根的估计值；</li>
  <li>$\tilde{g}(w_k,\eta_k) = g(w_k) + \eta_k$ 为第 $k$ 步的含噪观测；</li>
  <li>$\alpha$ 为正的步长参数。</li>
</ul>

<p>注意，上述求解过程不涉及 $g(w)$ 的表达式，他是一个黑箱。与之相对，算法的求解依赖数据，即</p>

<ul>
  <li>输入序列：${w_k}$</li>
  <li>测量到的含噪输出序列：${\tilde{g}(w_k,\eta_k)}$</li>
</ul>

<blockquote>
  <p>例：求解 $g(w) = \tanh(w-1)$
真实根易知为 $w=1$
采用 RM 算法，假设 $w_1=3,a_k=\frac{1}{k},\eta_k=0$，不含噪便于简化分析。那么迭代式如下</p>

\[w_{k+1} = w_k-\frac{1}{k} g(w_k)\]

  <p>迭代结果如图所示，可以发现确实能收敛到真实根</p>

  <p><a href="/assets/img/postsimg/20221210/rm-example.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221210/rm-example.jpg" alt="RM算法迭代结果" class="lazyload" data-proofer-ignore></a></p>
</blockquote>

<h4 id="222-收敛性分析"><span class="me-2">2.2.2. 收敛性分析</span><a href="#222-收敛性分析" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>下面给出收敛性证明的严格数学推导。如果满足以下三条约束，那么，RM 算法中 $w_k$ 可依概率（with probability 1, w.p.1）收敛至方程 $g(w^\star) = 0$ 的根 $w^\star$。</p>

<p>（1）梯度约束：</p>

\[0&lt;c_1\leq \nabla_w g(w)\leq c_2\]

<ul>
  <li>
    <p>对于梯度约束，其要求梯度为某一个正值区间内，此时 $g(w)$ <strong>单调递增</strong>， $g(w)=0$ 的根存在且唯一（注意，类似 $g(w) = e^w$ 是不满足此条件的，因为其导数在 $w\rightarrow -\infty$ 时趋于零，对于任意 $c_1&gt;0$ 总能找到其导数小于 $c_1$ 的情况）；</p>
  </li>
  <li>
    <p>单调递增是一个严格约束，对于神经网络而言，假设 $J(w)$ 是一个待优化的目标函数，我们需要求解 $J(w)$ 的极值，问题就转化为 $g(w) = \nabla_w J(w)=0$，则梯度要求为 $\nabla_w^2 J(w) &gt; 0$ 即 Hessian 矩阵是正定矩阵，对应原始函数是凹函数（不同约定下也可能定义为凸函数，本质上是单调且存在极值解的）；</p>
  </li>
</ul>

<p>（2）系数约束</p>

\[\sum_{k=1}^{\infty} \alpha_k=\infty,\;\sum_{k=1}^{\infty}\alpha_k^2&lt;\infty\]

<ul>
  <li>$\sum_{k=1}^{\infty} \alpha_k^2&lt;\infty$ 要求在 $k\rightarrow \infty$ 时系数 $a_k\rightarrow 0$ 即<strong>收敛至零</strong>；
    <ul>
      <li>对于 RM 算法迭代式进行移项：$w_{k+1} - w_k = -\alpha_k\tilde{g}(w_k,\eta_k)$</li>
      <li>只有 $a_k\rightarrow 0$ 才能保证 $a_k\tilde{g}(w_k,\eta_k) \rightarrow 0$</li>
      <li>即 $w_{k+1} - w_k \rightarrow 0$</li>
    </ul>
  </li>
  <li>$\sum_{k=1}^{\infty} \alpha_k = \infty$ 要求系数 $\alpha_k\rightarrow 0$ 的<strong>收敛速度不能太快</strong>；
    <ul>
      <li>对于 RM 算法迭代式进行移项累加后有：$w_{\infty} - w_1 = -\sum_{k=1}^\infty \alpha_k\tilde{g}(w_k,\eta_k)$</li>
      <li>假设 $w_{\infty}=w^\star$，那么若 $\sum_{k=1}^{\infty} \alpha_k &lt; \infty$ 表明上式右项是有界的（若 $g(w)$ 恰好也有界）</li>
      <li>如果初始猜测 $w_1$ 距离根 $w^\star$ 特别远，$w_{\infty} - w_1$ 就会超出上面的界限，产生矛盾</li>
      <li>因此要求 $\sum_{k=1}^{\infty} \alpha_k = \infty$，保证我们可以任意选取 $w_1$</li>
    </ul>
  </li>
  <li>最常见的可选系数序列是<strong>调和级数</strong>：$\alpha_k = \frac{1}{k}$</li>
</ul>

<p>（3）误差约束：</p>

\[\mathbb{E}[\eta_k\vert\mathcal{H}_k]=0,\;\mathbb{E}[\eta_k^2\vert\mathcal{H}_k]&lt; \infty,\quad \mathcal{H}_k=\{w_k, w_{k-1},\cdots\}\]

<ul>
  <li>
    <p>要求误差的均值是0，且方差是有界的；</p>
  </li>
  <li>
    <p>常见的观测误差是一系列独立同分布（iid）的随机噪声序列中进行采样，但不要求噪声是高斯的。</p>
  </li>
</ul>

<blockquote>
  <p>即使不满足上述三条约束中的某一条，RM 算法也可能有效，但是不能保证一定收敛，如：</p>
  <ul>
    <li>对于 $g(w) = w^3-5$ 不满足第一条梯度约束，但如果初始猜测比较好，算法依然能够（局部）收敛；</li>
    <li>在强化学习算法中经常令 $a_k$ 等于一个很小的值（学习率）而不是调和级数，显然不满足第二条系数约束，但是算法依然有效。</li>
  </ul>
</blockquote>

<p>RM 算法收敛性证明需要用到更加古老的 Dvoretzky 收敛定理。其表述如下：</p>

<p>考虑一个随机过程 $w_{k+1} = (1-\alpha_k)w_k+\beta_k\eta_k$，其中</p>

\[\{\alpha_k\}_{k=1}^{\infty},\{\beta_k\}_{k=1}^{\infty},\{\eta\}_{k=1}^{\infty}\]

<p>都是随机序列，且 $\alpha_k\geq 0, \beta_k \geq 0$，那么当满足以下约束时，$w_k$ 依概率收敛至 0。</p>

<p>（1）系数约束：</p>

\[\sum_{k=1}^{\infty} \alpha_k = \infty,\; \sum_{k=1}^{\infty} \alpha_k^2 &lt; \infty,\; \text{and} \sum_{k=1}^{\infty} \beta_k^2 &lt; \infty\; \text{uniformly w.p.1}\]

<p>（2）误差约束：</p>

\[\mathbb{E}[\eta_k\vert \mathcal{H}_k]=0,\;\mathbb{E}[\eta_k^2\vert \mathcal{H}_k] \leq C\; \text{w.p.1},\quad \mathcal{H}_k=\{w_k, w_{k-1},\cdots, \eta_{k-1},\cdots, \alpha_{k-1},\cdots,\eta_{k-1}\}\]

<p>证明略，可参考如下链接。</p>

<blockquote>
  <p>Stochastic Approximation 随机近似方法的详解之（三）Dvoretzky’s convergence theorem
网页链接：https://blog.csdn.net/weixin_37726222/article/details/129306871</p>
</blockquote>

<h4 id="223-回顾增量更新"><span class="me-2">2.2.3. 回顾增量更新</span><a href="#223-回顾增量更新" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>考虑一个简单的均值估计问题，我们想要从一组独立统分不采样得到的样本 ${x}_i\in X$ 中计算均值</p>

\[w = \mathbb{E}[X]\]

<p>将原始问题转变为如下寻根问题，定义函数</p>

\[g(w) \doteq w - \mathbb{E}[X]\]

<p>我们的目标是求解 $g(w) = 0$，如果可以求解，相当于变相求出了期望。</p>

<p>由于期望是未知的，我们只能够获取到状态量的具体测量 $x$，那么定义对函数 $g(w)$ 的含噪观测为</p>

\[\tilde g(w,x) \doteq w - x\]

<p>注意到，上式可以进行如下变换</p>

\[\begin{aligned}
\tilde g(w,x) &amp;\doteq w - x \\
&amp;= w-x+\mathbb{E}[X]-\mathbb{E}[X]\\
&amp;= (w-\mathbb{E}[X])+(\mathbb{E}[X]-x)\\
&amp;= g(w) + \eta\\
&amp;= \tilde{g}(w,\eta)
\end{aligned}\]

<p>根据 RM 算法，求解方程 $g(w)=0$ 的根可以通过如下迭代式进行</p>

\[w_{k+1} = w_k - \alpha \tilde g(w_k,\eta_k) = w_k - \alpha_k (w_k - x_k)\]

<p>观察发现其正好就是均值估计的增量更新算法迭代式。所以<strong>均值估计的增量更新算法是一种 RM 算法的特殊形式</strong>，自然收敛。</p>

<h3 id="23-随机梯度下降sgd"><span class="me-2">2.3. 随机梯度下降（SGD）</span><a href="#23-随机梯度下降sgd" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="231-sgd-算法推导"><span class="me-2">2.3.1. SGD 算法推导</span><a href="#231-sgd-算法推导" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>考虑一个如下的一般优化问题</p>

\[\min_w J(w) = \mathbb{E}[f(w,X)]\]

<p>其中 $f(w,X)$ 的输出为标量，$X$ 是输入随机变量，$w$ 是待优化参数。求期望符号是针对 $X$ 的，即希望找到某个参数使得期望最小。</p>

<p>使用 <strong>梯度下降法</strong>（Gradient Descent, <strong>GD</strong>）进行求解</p>

\[w_{k+1} = w_k - \alpha \nabla_w \mathbb{E}[f(w,X)] = w_k - \alpha \mathbb{E}{[\nabla_w f(w,X)]}\]

<p>其中 $\alpha$ 是控制步长的参数。注意到求期望的本质就是求和，因此求梯度符号可以移到期望符号内。</p>

<p>上述求解过程的难点在于对梯度求期望，如果我们没有映射函数的显示表达式，那么我们无法直接求解。因此我们需要借助数据，采用批量梯度下降法或随机梯度下降法进行求解。</p>

<p>使用 <strong>批量梯度下降法</strong>（Batch Gradient Descent, <strong>BGD</strong>），梯度的期望可以近似表示如下</p>

\[\begin{aligned}
\mathbb{E}{[\nabla_w f(w,X)]} &amp;\approx \frac{1}{N}\sum_{i=1}^N \nabla_w f(w_k,x_i)\\
w_{k+1} &amp;= w_k-\alpha_k \frac{1}{N}\sum_{i=1}^N \nabla_w f(w_k,x_i)
\end{aligned}\]

<p>上述求解过程仍然唇在一个不足，即需要采样 $N$ 次获得梯度数据才能迭代计算一步。我们进一步采用随机梯度下降法解决这个不足。</p>

<p>使用 <strong>随机梯度下降法</strong>（Stochastic Gradient Descent, <strong>SGD</strong>），直接移除 $N$ 个梯度样本的求和，直接使用单个样本的梯度进行迭代</p>

\[w_{k+1} = w_k-\alpha_k  \nabla_w f(w_k,x_k)\]

<p>可以看出，随机梯度下降是批量梯度下降取 $N=1$ 的一种特殊情况，相当于将真实梯度的期望替换为单个样本（随机）梯度。</p>

<h4 id="232-sgd-收敛性分析"><span class="me-2">2.3.2. SGD 收敛性分析</span><a href="#232-sgd-收敛性分析" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>对于如下的一般优化问题</p>

\[\min_w J(w) = \mathbb{E}[f(w,X)]\]

<p>我们称 $g(w) = \mathbb{E}[\nabla_w f(w,X)]$ 为 true gradient，称其某次采样 $\nabla_w f(w_k,x_k)$ 为 stochastic gradient。易知，原始<strong>优化问题</strong>可以转化为一个<strong>梯度等于零</strong> $g(w) = 0$ 这个方程求根问题，因为后者是前者能取到最优解的必要条件。</p>

<p>由于我们不知道 $\nabla_w f(w,X)$ 的表达式，我们可以测量到其含噪近似（本质就是 stochastic gradient）</p>

\[\begin{aligned}
  \tilde{g}(w,\eta) &amp;= \nabla_w f(w,x)\\
  &amp;=\textcolor{blue}{\mathbb{E}[\nabla_w f(w,X)]} + \textcolor{red}{\nabla_w f(w,x) - \mathbb{E}[\nabla_w f(w,X)]}\\
&amp;= \textcolor{blue}{g(w)} + \textcolor{red}{\eta}
\end{aligned}\]

<p>根据 RM 算法，求根问题的迭代解法如下</p>

\[w_{k+1} = w_k - \alpha_k \tilde{g}(w_k,\eta_k)\]

<p>而前面说了 $\tilde{g}(w,\eta)$ 就是随机梯度，那么上式就是 SGD 的迭代表达式，<strong>因此 SGD 是一个特殊的 RM 算法，当满足三条约束时可收敛。</strong></p>

<p>定义其第 $k$ 步迭代时的相对误差为</p>

\[\delta_k = \frac{\vert \nabla_w f(w_k,x_k) - \mathbb{E}[\nabla_w f(w_k,X)]\vert}{\vert \mathbb{E}[\nabla_w f(w_k,X)] \vert}\]

<p>又因为 $\mathbb{E}[\nabla_w f(w^\star,X)]=0$，代入得到</p>

\[\delta_k = \frac{\vert\nabla_w f(w_k,x_k) - \mathbb{E}[\nabla_w f(w_k,X)]\vert}{\vert\mathbb{E}[\nabla_w f(w_k,X)]-\mathbb{E}[\nabla_w f(w^\star,X)] \vert} = \frac{\vert\nabla_w f(w_k,x_k) - \mathbb{E}[\nabla_w f(w_k,X)]\vert}{\vert\mathbb{E}[\nabla_w^2 f(\tilde{w}_k,X)(w_k-w^\star)] \vert}\]

<p>其中，分母部分的变换利用了中值定理，使得 $w_k-w^\star$ 项出现。</p>

<blockquote>
  <p>中值定理：$f(x_1) - f(x_2) = f^\prime(x_3)(x_1 - x_2)$ 其中 $x_3\in[x_1, x_2]$</p>
</blockquote>

<p>假设分母的二阶梯度 $\nabla_w^2f(w_k,X)\geq c &gt;0$，其中 $c$ 是一个正的常值。那么分母可进一步化简得</p>

\[\begin{aligned}
  \vert\mathbb{E}[\nabla_w^2 f(\tilde{w}_k,X)(w_k-w^\star)] \vert &amp;= \vert\mathbb{E}[\nabla_w^2 f(\tilde{w}_k,X)]\cdot (w_k-w^\star) \vert\\
  &amp;= \vert\mathbb{E}[\nabla_w^2 f(\tilde{w}_k,X)]\vert \cdot \vert w_k-w^\star \vert\\
  &amp;\geq c \cdot \vert w_k-w^\star \vert
\end{aligned}\]

<p>带如相对误差的表达式有</p>

\[\delta_k \leq \frac{\overbrace{\vert\nabla_w f(w_k,x_k)}^{\text{stochastic gradient}} - \overbrace{\mathbb{E}[\nabla_w f(w_k,X)]\vert}^{\text{true gradient}}}{\underbrace{c \cdot \vert w_k-w^\star \vert}_{\text{distance to optimal solutiion}}}\]

<p>上式给出了 SGD 算法有趣的收敛模式：</p>

<ul>
  <li>如果 $w_k$ 距离最优解 $w^\star$ 越远，那么相对误差 $\delta_k$ 越小，即 SGD 算法的收敛速度越快；</li>
  <li>如果 $w_k$ 距离最优解越近，那么相对误差 $\delta_k$ 越大，即 SGD 算法的收敛速度越慢。</li>
</ul>

<p>这意味着，初始时刻 SGD 算法与 GD 算法很类似，随机梯度与真实梯度之间的相对误差较小，SGD 算法会快速收敛到最优解附近；随着迭代次数的增加，$w_k$ 收敛至最优解 $w^\star$ 时 SGD 的收敛速度反而变慢，收敛过程的随机性增加。</p>

<h4 id="233-回顾增量更新"><span class="me-2">2.3.3. 回顾增量更新</span><a href="#233-回顾增量更新" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>考虑如下优化问题</p>

\[\min_w J(w) = \mathbb{E}[f(w,X)]=\mathbb{E}\left[\frac{1}{2}\vert\vert w-X \vert\vert^2\right]\]

<p>令梯度 $\nabla_w J(w)$ 等于零，考虑到 $w$ 不是随机变量，显然有 $\mathbb{E}[w-x]=0 \Rightarrow w^\star = \mathbb{E}[X]$。</p>

<p>如果假设不知道函数的显示表达式，使用 GD 和 SGD 来求解，有</p>

\[\begin{aligned}
GD: \quad &amp;w_{k+1} = w_k - \alpha \nabla_w \mathbb{E}[f(w,X)] = w_k - \alpha \mathbb{E}[w_k-X]\\
SGD: \quad &amp;w_{k+1} = w_k - \alpha \nabla_w f(w,x_i) = w_k - \alpha [w_k-x_k]
\end{aligned}\]

<p>观察发现第二式其正好就是均值估计的增量更新迭代式，所以<strong>均值估计的增量更新算法是 SGD 算法的特殊形式</strong>，自然收敛。</p>

<h4 id="234-sgd-的确定性形式"><span class="me-2">2.3.4. SGD 的确定性形式</span><a href="#234-sgd-的确定性形式" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>在前述 SGD 定义中，我们涉及到随机变量 $X$ 及其期望 $\mathbb{E}[X]$，而我们在实际任务中会经常遇到另一种很类似但是不涉及随机变量的确定形式。考虑如下一般的优化问题</p>

\[\min_w J(w) = \frac{1}{N}\sum_{i=1}^N f(w,x_i),\quad x_i\in\{x_i\}_{i=1}^N\]

<p>其中，$x_i$ 是一组确定的样本，不再是随机变量的采样。</p>

<p>又假设样本数据集中没取出一个数代价很大（很慢或者很耗费成本），我们每次只能取出一个数进行计算，那么我们可以将迭代求解的形式写为</p>

\[w_{k+1} = w_k - \alpha \nabla_w f(w_k,x_k)\]

<p>上述过程和 SGD 形式几乎完全一样，只是 $x_i$ 从随机变量变为了确定的样本。此时有如下几个问题思考：</p>

<ul>
  <li>上面的迭代式是否是 SGD？</li>
  <li>每次采样一个样本时需要对其进行大小排列后采样，还是随机采样？</li>
</ul>

<p>实际上，我们可以人为引入一个定义在样本数据集 ${x_i}_{i=1}^N$ 上的随机变量 $X$ 且其概率分布为均匀分布（$p(X=x_i)=\frac{1}{N}$），此时确定性优化问题就转变为一个随机优化问题</p>

\[\min_w J(w) =\frac{1}{N}\sum_{i=1}^N f(w,x_i) =  \mathbb{E}[f(w,X)]\]

<p>其中，</p>

<ul>
  <li>后一个等式是严格成立的而不是近似，因此上述确定性形式的问题就是 SGD 算法；</li>
  <li>随机变量 $X$ 需要从样本集中均匀随机抽取。</li>
</ul>

<h4 id="235-bgdmbgd-和-sgd"><span class="me-2">2.3.5. BGD、MBGD 和 SGD</span><a href="#235-bgdmbgd-和-sgd" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>在 SGD 的算法推导中我们按照 GD、BGD 到 SGD 的过程逐步推导。现在我们进一步分析其中 BGD 与 SGD 和一种新的 Mini-batch Gradient Descent（MBGD）算法的关系。</p>

<p>考虑如下一般形式的优化问题</p>

\[\min_w J(w) = \mathbb{E}[f(w,X)]\]

<p>使用 BGD、SGD 和 MBGD 算法求解上述问题的迭代式分别如下：</p>

\[\begin{aligned}
\text{BGD}:\quad w_{k+1} &amp;= w_k - \alpha \nabla_w \frac{1}{n} \sum_{i=1}^n f(w_k,x_i)\\
\text{MBGD}:\quad w_{k+1} &amp;= w_k - \alpha \nabla_w \frac{1}{m} \sum_{j\in\mathcal{I}_k} f(w_k,x_k)\\
\text{SGD}:\quad w_{k+1} &amp;= w_k - \alpha \nabla_w f(w_k,x_k)
\end{aligned}\]

<p>其中，MBGD 中的 $\mathcal{I}_k$ 是样本空间的一个子集，是从样本空间中独立随机采样 $m$ 次得到的样本集合。</p>

<ul>
  <li>相比 SGD，MBGD 因为用到了更多的样本数据，从而使其随机性更小。当 $m=1$ 时 MBGD 就退化成了 SGD；</li>
  <li>相比 BGD，MBGD 由于采样的样本数目更少，因此其效率更高；</li>
  <li>注意，当 $m=n$ 时 MBGD 并不等同于 BGD，因为 MBGD 的数据是从原始样本集中均匀随机采样的（某样本可能没被采样到也可能被使用多次），而 BGD 是对所有样本数据都仅使用一次。</li>
</ul>

<p>下面分别采用上述三种方法，给出对 $ x,y\in [-10, +10] $ 区间的 100 个样本（均值为 $(0,0)$ 的随机撒点）求均值的结果示意图。其中左图是三种方法的迭代收敛路径，有图为三种方法迭代过程中预测值与真实均值的距离变化情况。需要关注的是不同方法的收敛速度。</p>

<p><a href="/assets/img/postsimg/20221210/bgd-mbgd-sgd-comparison.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221210/bgd-mbgd-sgd-comparison.jpg" alt="bgd-mbgd-sgd-comparison.jpg" class="lazyload" data-proofer-ignore></a></p>

<h2 id="3-时序差分法"><span class="me-2">3. 时序差分法</span><a href="#3-时序差分法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="31-时序差分思想temporal-difference"><span class="me-2">3.1. 时序差分思想（Temporal Difference）</span><a href="#31-时序差分思想temporal-difference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="311-从蒙特卡洛的角度分析"><span class="me-2">3.1.1. 从蒙特卡洛的角度分析</span><a href="#311-从蒙特卡洛的角度分析" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>根据前面的章节我们知道，即使采用增量更新技巧，蒙特卡洛方法也必须要等整个回合采样结束之后才能计算得到这一次的回报 $G_{t}$。那么有没有一种方法能够无须完整采样一个回合就能更新价值函数呢？我们以状态价值函数为例，观察回报的定义式：</p>

\[\begin{aligned}
G_t \doteq \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} &amp;= R_{t+1}+\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2}\\
&amp;= R_{t+1}+\gamma V_{t+1}    
\end{aligned}\]

<p>然后带入蒙特卡洛的价值函数增量更新策略，即</p>

\[\begin{aligned}
\text{MC}:\quad &amp; v(S_t) \leftarrow v(S_t)+\alpha({\color{red}G_t}-v(S_t)) \\
\text{TD}:\quad &amp; v(S_t) \leftarrow v(S_t)+\alpha({\color{red}R_{t+1}+\gamma v(S_{t+1})}-v(S_t))
\end{aligned}\]

<p>上述第二行递推式就是本章所要介绍的 <strong>时序差分法</strong>（Temporal Difference Learning, <strong>TD</strong>）。</p>

<p>可以发现，时序差分法只需要采样一次，即可利用下一时刻下一状态的状态价值函数，来更新我们当前时刻当前状态的状态价值函数，注意这里涉及到了两种关于时间的概念（当前时刻 vs 下一时刻、当前状态 vs 下一状态），为避免混淆，详细展开如下：</p>

<ul>
  <li>当前时刻 $t$，每一个当前状态 $S_t$ 对应状态价值函数 $V(s_t)$；</li>
  <li>下一时刻 $t+1$，每一个下一时刻状态 $S_{t+1}$ 对应状态价值函数 $V(s_{t+1})$；</li>
  <li>在当前时刻从某个具体状态 $S_t=s_t$ 出发，下一时刻转移至 $S_{t+1}=s_{t+1}$ 后，使用 $v_{\textcolor{red}{t}}(\textcolor{blue}{s_{t+1}})$ 来计算 $v_{\textcolor{red}{t+1}}(\textcolor{blue}{s_t})$；</li>
  <li>其他不涉及状态转移的状态，他们的价值函数保持不变，即 $v_{t+1}(s)=v_{t}(s),\; s\neq s_t$</li>
</ul>

<p>综上，经过一些移项，时序差分法的迭代公式为</p>

\[\begin{cases}
v_{t+1}(s_t) = v_t(s_t)-\alpha_t(s_t)\big[v_t(s_t)-[r_{t+1}+\gamma v_t(s_{t+1})]\big]\\
v_{t+1}(s) = v_t(s),\; s\neq s_t
\end{cases}\tag{1}\]

<p>我们将时序差分法迭代式的各个部分做如下标注</p>

\[\underbrace{v_{t+1}(s_t)}_{\text{new estimate}} = \underbrace{v_t(s_t)}_{\text{old estimate}}-\alpha_t(s_t)\big[\overbrace{v_t(s_t)-[\underbrace{r_{t+1}+\gamma v_t(s_{t+1})}_{\text{TD target }\bar{v}_t}}^{\text{TD error }\delta_t}]\big]\]

<ul>
  <li><strong>TD Target</strong></li>
</ul>

<p>时序差分法的迭代式中，迭代目的就是将 $v(s_t)$ 朝着 $\bar{v}_t$ 的方向改进，所以称其为 <strong>TD Target</strong>。</p>

<p>将迭代式改写如下</p>

\[\begin{aligned}
  &amp;v_{t+1}(s_t) = v_t(s_t)-\alpha_t(s_t)[v_t(s_t) - \bar{v}_t]\\
  \Rightarrow \; &amp;v_{t+1}(s_t) - \textcolor{red}{\bar{v}_t} = v_t(s_t)-\textcolor{red}{\bar{v}_t}-\alpha_t(s_t)[v_t(s_t) - \bar{v}_t]\\
  \Rightarrow \; &amp;v_{t+1}(s_t) - \textcolor{red}{\bar{v}_t}= [1-\alpha_t(s_t)] \cdot [v_t(s_t) - \textcolor{red}{\bar{v}_t}]\\
  \Rightarrow \; &amp;\vert v_{t+1}(s_t) - \textcolor{red}{\bar{v}_t}\vert = \vert 1-\alpha_t(s_t)\vert\cdot \vert v_t(s_t) - \textcolor{red}{\bar{v}_t}\vert\\
\end{aligned}\]

<p>由于 $0&lt;1-\alpha_t(s_t)&lt;1$，所以有</p>

\[\vert v_{t+1}(s_t) - \textcolor{red}{\bar{v}_t}\vert \leq \vert v_t(s_t) - \textcolor{red}{\bar{v}_t}\vert\]

<p>也即 $v_{t+1}(s_t)$ 距离 TD Target 更近！</p>

<ul>
  <li><strong>TD Error</strong></li>
</ul>

<p>TD Error 定义为</p>

\[\delta_t = v_t(s_t)-[r_{t+1}+\gamma v_t(s_{t+1})]\]

<p>其反应了两个连续时刻前后状态的价值函数的差分（difference）。</p>

<p>更进一步，我们需要分析该差分随着迭代的变化。假设在当前策略 $\pi$ 下我们已经迭代收敛，价值函数已经收敛至 $v_\pi(s)$，将 TD Error 定义式的下标进行改写，有</p>

\[\delta_{\pi,t} = v_\pi(s_t) - [r_{t+1}+\gamma v_\pi(s_{t+1})]\]

<p>分别对上式左右两边求期望，有</p>

\[\mathbb{E}[\delta_{\pi,t}] = v_\pi(s_t) - \mathbb{E}[r_{t+1} + \gamma v_\pi(s_{t+1})] = 0\]

<p>右侧等号成立的原因是因为，对于收敛的状态价值函数，其定义式就是</p>

\[v_{\pi}(s_t) = \mathbb{E}[r_{t+1} + \gamma v_\pi(s_{t+1})]\]

<p>因此，TD Error 衡量了 $v_t$ 和 $v_\pi$ 的偏差，同时其也反应了一步采样后从序列 $(s_t, r_{t+1}, s_{t+1})$ 中得到的新的信息，这个新的信息与当前旧的估计之间存在偏差，就可以利用这个偏差来更新 $v_t$。</p>

<h4 id="312-从-rm-算法的角度分析"><span class="me-2">3.1.2. 从 RM 算法的角度分析</span><a href="#312-从-rm-算法的角度分析" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>可以证明时序差分算法迭代计算的状态价值函数 $v(s)$ 最终收敛至 $v_{\pi}(s)$，这需要用到我们前述介绍的 RM 算法，具体分析如下：</p>

<p>考虑如下的期望估计问题</p>

\[w = \mathbb{E}[R+\gamma v(X)]\]

<p>其中 $R,X$ 都是随机变量，$\gamma$ 是一个常数（折扣因子），$v(\cdot)$ 是一个函数（状态价值函数）。</p>

<p>假设我们能够采样得到样本 ${x}\in X,{r}\in R$，可定义函数 $g(w)$ 和其含噪观测 $g(w,\eta)$ 如下</p>

\[\begin{aligned}
g(w) &amp;= w-\mathbb{E}[R+\gamma v(X)]\\
\tilde{g}(w,\eta) &amp;= w-[r+\gamma v(x)]\\
&amp;=(w-\mathbb{E}[R+\gamma v(x)])+(\mathbb{E}[R+\gamma v(x)])-[r+\gamma v(x)]\\
&amp;\doteq g(w) + \eta
\end{aligned}\]

<p>上述问题变为方程 $g(w)=0$ 的寻根问题，对应的 RM 算法求解如下</p>

\[w_{k+1} = w_{k} - \alpha_k \tilde{g}(w_k,\eta_k) = w_k - \alpha_k[w_k-(r_k+\gamma v(x_k))]\tag{2}\]

<p>对比式（1）与式（2）可以发现形式完全一样，因此 TD 算法是收敛的。</p>

<h4 id="313-从贝尔曼期望方程的角度分析"><span class="me-2">3.1.3. 从贝尔曼期望方程的角度分析</span><a href="#313-从贝尔曼期望方程的角度分析" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>虽然前面我们通过一个特殊形式的期望估计问题推导出 TD 算法是一种特殊形式的 RM 算法，并得到了收敛性保证，但我们并没有明确其一定能够收敛至 $v_\pi(s)$。下面我们从贝尔曼期望方程的角度分析 TD 算法的收敛性，只有通过贝尔曼期望方程才能严格证明这一点。</p>

<p>对于状态价值函数，贝尔曼期望方程如下</p>

\[v_\pi(s) = \mathbb{E}[R+\gamma v_\pi(S^\prime)\vert s], \;s\in S\]

<p>使用 RM 算法求解贝尔曼期望方程，需要转化为 $g(v(s))=0$ 的求根问题 ，其中函数 $g(v(s))$ 如下</p>

\[g(v(s)) = v(s) - \mathbb{E}[R+\gamma v_\pi(S^\prime)\vert s]\]

<p>我们仅能采样得到 $s,r, s^\prime$，则含噪观测 $g(v(s),\eta)$ 如下</p>

\[\begin{aligned}
\tilde{g}(v(s),\eta) &amp;= v(s) - [r+\gamma v_\pi(s^\prime)]\\
&amp;= v(s) - \mathbb{E}[R+\gamma v_\pi(s^\prime)\vert s] + \mathbb{E}[R+\gamma v_\pi(s^\prime)\vert s] - [r+\gamma v_\pi(s^\prime)]\\
&amp;= g(v(s)) + \eta
\end{aligned}\]

<p>那么 RM 算法的迭代式如下</p>

\[\begin{aligned}
v_{k+1}(s) &amp;= v_k(s) - \alpha_k \tilde{g}(v_k(s),\eta_k)\\
&amp;= v_k(s) - \alpha_k [v_k(s) - [\textcolor{blue}{r_k}+\gamma \textcolor{blue}{v}_{\textcolor{red}{\pi}}(\textcolor{blue}{s^\prime_k})]]
\end{aligned}\]

<p>虽然上述迭代式已经和 TD 算法的迭代式很相似，但是需要注意以下两点不同：</p>

<ul>
  <li>在每一步迭代中，我们需要频繁<strong>采样多次同样</strong>的序列 $(s,r, s^\prime)$ 才能完成对状态 $v(s)$ 的更新（蓝色标记）；</li>
  <li>在每一步迭代中，注意式中要求 $v_\pi(s^\prime)$ 是<strong>已知的</strong>，但实际上是不知道的！（红色标记）。</li>
</ul>

<p>对于第一个假设，我们实际采样一个回合后，对于其中每一个片段 $(r_t, r_{t+1},s_{t+1})$ 替代 $(s,r, s^\prime)$，采到谁我们就更新谁的价值函数。</p>

<p>对于第二个假设，我们就用当前时刻的 $v_k(s^\prime_k)$ 替换掉 $v_\pi(s^\prime)$（开放式思考，替换后能保证收敛么？）。</p>

<p>TD 算法的严格收敛证明依赖 RM 算法，此处略。</p>

<p>最后我们可以发现，TD 算法就是用于求解贝尔曼期望方程的一种特殊的 RM 算法。</p>

<h4 id="314-td-与-mc-的比较"><span class="me-2">3.1.4. TD 与 MC 的比较</span><a href="#314-td-与-mc-的比较" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<ul>
  <li><strong>在线与离线</strong>：
    <ul>
      <li>TD 方法：在线；</li>
      <li>MC 方法：离线；</li>
    </ul>
  </li>
  <li><strong>无限采样与有限采样</strong>：
    <ul>
      <li>TD 方法：支持无限采样；</li>
      <li>MC 方法：仅支持有限采样；</li>
    </ul>
  </li>
  <li><strong>自举与否</strong>：
    <ul>
      <li>TD 方法：自举；</li>
      <li>MC 方法：非自举；</li>
    </ul>
  </li>
  <li><strong>方差与偏差</strong>：
    <ul>
      <li>TD 方法：高偏差，低方差。因为 TD 方法只需要一次采样，其中涉及到的随机变量少，随机性少，因此方差小。但是由于其依赖初值估计，如果估计不准会带来很大的偏差，虽然最终随着采样次数的增多也能收敛到正确的值。</li>
      <li>MC 方法：低偏差，高方差。因为 MC 方法不涉及任何初值估计，其均值的估计是对期望的无偏估计，因此在迭代过程中偏差比较小，但因为他每一次采样都是“天马行空”的，因此不同的采样可能差距非常大，方差就会很大。</li>
    </ul>
  </li>
</ul>

<h3 id="32-同策略时序差分sarsa"><span class="me-2">3.2. 同策略时序差分（SARSA）</span><a href="#32-同策略时序差分sarsa" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="321-sarsa"><span class="me-2">3.2.1. SARSA</span><a href="#321-sarsa" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>前述 TD 算法针对状态价值函数进行估计，将其推广至状态-动作价值函数估计后，得到本节介绍的时序差分方法的一个具体算法，即 <strong>SARSA</strong> 算法：</p>

\[q(s_t,a_t)  \leftarrow q(s_t,a_t)-\alpha\{q(s_t,a_t)-[r_{t+1}+\gamma q(s_{t+1},a_{t+1})]\}\\\]

<p>注意到，上述迭代式需要与进行两次动作采样才能得到足够的经验数据进行一步迭代：</p>

\[s_t\mathop{\longrightarrow}\limits^{\pi}a_t\mathop{\longrightarrow}\limits^{\text{model}}r_t,s_{t+1}\mathop{\longrightarrow}\limits^{\pi}a_{t+1}\]

<p>观察经验数据的一般形式 $(S_t,A_t, R_{t+1}, S_{t+1}, A_{t+1})$，取首字母放在一起命名本方法。</p>

<ul>
  <li><strong>行为策略</strong></li>
</ul>

<p>经验获取过程需要进行两次动作采样，对于 SARSA 而言，两次动作采样时使用的策略是相同的。延续我们之前在 Monte Carlo 方法中讨论的结果，策略一般选取为 $\varepsilon$-greedy 策略，兼顾探索和利用。由于这个策略的第一次采样需要实际执行产生环境交互，因此称其为 「行为策略」。具体而言：</p>

<ul>
  <li>第一步根据策略进行动作采样后实际执行动作，即 $s_t\mathop{\longrightarrow}\limits^{\pi}a_t\mathop{\longrightarrow}\limits^{\text{model}}r_t,s_{t+1}$ 过程；</li>
  <li>第二步根据策略进行动作采样只是为了能够得到具体的 $Q(S_{t+1},A_{t+1})$ 用于迭代式的求解，并不实际执行采样得到的动作 $A_{t+1}$。</li>
</ul>

<p>所以严格来说，行为策略指的是第一次实际执行动作所采用的策略。</p>

<p>上述迭代式仍然是一个<font color="red">策略评估</font>过程，即在 <strong>给定策略 $\pi$ 的条件下，给出状态-动作价值函数 $q_\pi(s,a)$ 的估计</strong>。将其与特定的策略改进方法结合可以得到完整的 TD 形式的方法。</p>

<ul>
  <li><strong>目标策略</strong></li>
</ul>

<p>我们定义策略改进时被改进的策略叫 「目标策略」，那么对于 SARSA 而言，目标策略和行为策略是同一个策略。也就是说，SARSA 算法的自然流程表明，<font color="red">策略改进</font>是针对前面的 $\varepsilon$-greedy 行为策略进行改进。</p>

<blockquote>
  <p>有人可能会问，为什么目标策略和行为策略是相同的？能不同么？
大家可以尝试脑补一下：</p>
  <ul>
    <li>上一轮迭代已有旧的 $Q$ 函数；</li>
    <li>在策略评估阶段，基于 $\varepsilon$-greedy 行为策略采集经验更新 $Q$ 函数；</li>
    <li>在策略改进阶段，如果更新一个新的目标策略，可选的无非就是 greedy 策略；</li>
    <li>在下一时刻策略评估阶段，再次基于 $\varepsilon$-greedy 行为策略采集经验，这时需要用到新的 $Q$ 函数按照 $\varepsilon$ 概率来选择动作，不就相当于隐式更新了 $\varepsilon$-greedy 行为策略自身么？额外更新的 greedy 策略没有任何意义；</li>
  </ul>

  <p>所以，并不是人们强行设定 SARSA 在策略改进时，必须针对 行为策略=目标策略 来改进，而是整个算法自然的流程导致的结果如此。</p>
</blockquote>

<p>对 SARSA 算法的行为策略和目标策略总结如下：</p>

<ul>
  <li>行为策略：
    <ul>
      <li>第一次动作采样： $\varepsilon$-greedy 策略</li>
      <li>第二次动作采样： 相同的 $\varepsilon$-greedy 策略</li>
    </ul>
  </li>
  <li>目标策略：
    <ul>
      <li>相同的 $\varepsilon$-greedy 策略</li>
    </ul>
  </li>
</ul>

<p>最终形成 SARSA 方法的伪代码如下：</p>

<hr />
<ul>
  <li>Initialize $q(s,a)$ arbitrarily, choose start state $s$, given a <u>behaviour</u> policy $\pi_t$ <font color="red">(e.g., $\varepsilon$-greedy)</font></li>
  <li>Repeat (for each episode):
    <ul>
      <li>Repeat (if $s$ is not terminal state):
        <ul>
          <li><em>collect experience</em>:
            <ul>
              <li>choose $a$ at $s$ following $\pi_t$</li>
              <li>take action $a$, get $r$, transfer to $s^\prime$</li>
              <li>
                <font color="red">choose $a^\prime$ at $s^\prime$ following $\pi_t$</font>
              </li>
            </ul>
          </li>
          <li><em>policy evaluation</em>:
            <ul>
              <li>${q(s,a)\leftarrow q(s,a)-\alpha[q(s,a) - (r+\gamma q(s^\prime,a^\prime))]}$</li>
            </ul>
          </li>
          <li><em>policy improvement</em>:
            <ul>
              <li>(update same $\varepsilon$-greedy as <u>target</u> policy)</li>
              <li>
                <font color="red">$\pi_{t+1}(a\vert s) = 1-\frac{\varepsilon}{\vert \mathcal{A} \vert}(\mathcal{A}-1)$ if $a=\arg\max_a q(s,a)$</font>
              </li>
              <li>
                <font color="red">$\pi_{t+1}(a\vert s) = \frac{\varepsilon}{\vert \mathcal{A} \vert}$ otherwise</font>
              </li>
              <li>${s\leftarrow}s^\prime; a\leftarrow a^\prime$</li>
            </ul>
          </li>
          <li>$t \leftarrow t+1$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<p>使用 SARSA 方法在 Grid World 中进行实验。设定初始状态为左上角格子，相关参数设置为 $r_{target}=0, r_{forbidden}=r_{boundary} = -10, r_{other}=-1,\gamma = 0.9,\alpha = 0.1$，迭代 500 步如下图所示：</p>

<p><a href="/assets/img/postsimg/20221210/sarsa-result.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221210/sarsa-result.jpg" alt="sarsa-result" class="lazyload" data-proofer-ignore></a></p>

<ul>
  <li>左子图表明，
    <ul>
      <li>经过 500 步迭代，从左上角格子出发已经找到了对应的较优的路径；</li>
      <li>除了左上角格子外，其他格子对应的策略明显不是最优的，需要经过更多步探索才能收敛到最优策略。</li>
    </ul>
  </li>
  <li>右子图表明，
    <ul>
      <li>随着迭代次数的增加，每个 episode 探索的步长逐渐降低，对应的总奖励逐渐升高；</li>
      <li>总奖励值依然为负数，因为采用 $\varepsilon$-greedy 策略会引入大量非最优探索，导致状态价值的估计偏低。</li>
    </ul>
  </li>
</ul>

<h4 id="322-expected-sarsa"><span class="me-2">3.2.2. Expected SARSA</span><a href="#322-expected-sarsa" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>我们可以对 SARSA 的策略评估部分进行改进，不再进行采样得到动作 $a^\prime$，而是使用 $q$ 的期望（也就是 $v$），此时的迭代式即为 <strong>期望 SARSA</strong></p>

\[\begin{aligned}
q(s,a) &amp;\leftarrow q(s,a)-\alpha\{q(s,a)-(r+\gamma \mathbb{E}_\pi[q(s^\prime,A)])\}\\
q(s,a) &amp;\leftarrow q(s,a)-\alpha[q(s,a)-(r+\gamma V(s^\prime))]\\
\end{aligned}\]

<ul>
  <li>优势：偏差更小（因为只需要 $(s,a,r,s^\prime)$，不需要做第二次动作采样得到 $a^\prime$，随机性更小）</li>
  <li>劣势：计算量大幅增加（因为需要计算多个 $q$ 值后取平均）。</li>
</ul>

<p>期望 SARSA 本质上是求解如下贝尔曼期望方程</p>

\[q_\pi(s,a) = \mathbb{E}[R+\gamma v_\pi(s^\prime)\vert s,a]\]

<h4 id="323-n-step-sarsa"><span class="me-2">3.2.3. n-step SARSA</span><a href="#323-n-step-sarsa" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>SARSA 的策略评估部分是一种一步更新的方法，即每次更新只使用一步的采样数据。我们可以将 SARSA 的策略评估部分泛化到 $n$ 步更新，即每次更新使用 $n$ 步的采样数据。这本质上是将 SARSA 和 MC 方法进行融合。考虑回报的不同展开形式</p>

\[\begin{aligned}
  \text{SARSA} \; \leftarrow \; G_t^{(1)} &amp;= R_{t+1}+\gamma Q(S_{t+1},A_{t+1})\\
  G_t^{(2)} &amp;= R_{t+1}+\gamma R_{t+2}+\gamma^2 Q(S_{t+2},A_{t+2})\\
  \vdots\\
  \text{n-step SARSA} \; \leftarrow \; G_t^{(n)} &amp;= R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1} R_{t+n}+\gamma^n Q(S_{t+n},A_{t+n})\\
  \vdots\\
  \text{MC} \; \leftarrow \; G_t^{(\infty)} &amp;= R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1} R_{t+n}+\cdots
\end{aligned}\]

<p>将上述不同的回报展开形式代入 $q(s,a)$ 关于回报的定义式，然后再代入期望 SARSAR 迭代式，可以得到 $\boldsymbol{n}$<strong>-step SARSA</strong> 这种介于 SARSA 和 MC 方法之间的迭代式</p>

\[q_\pi(s,a)\leftarrow q_\pi(s,a)-\alpha[q_\pi(s,a) - G_t^{(n)}]\]

<ul>
  <li>当 $n=1$ 时，$n$-step SARSA 等价于 SARSA（很显然）；</li>
  <li>当 $n=\infty$ 时，$n$-step SARSA 等价于 MC 方法（请尝试推导，需要额外定义 $\alpha=1$）。</li>
</ul>

<h3 id="33-异策略时序差分q-learning"><span class="me-2">3.3. 异策略时序差分（Q-Learning）</span><a href="#33-异策略时序差分q-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="331-q-learning"><span class="me-2">3.3.1. Q-Learning</span><a href="#331-q-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>在 SARSAR 算法的行为策略是 $\varepsilon$-greedy 策略，但在第二次动作采样时，我们只是为了计算后续 $Q$ 值而并不需要与环境进行交互。此时继续采用 $\varepsilon$-greedy 策略并不会对探索产生影响。</p>

<p>实际上，在下一时刻状态 $s^\prime$ 下，<u>明显存在一个理论更优的确定性策略</u>（也即贪婪策略）可供我们选择动作</p>

\[a^\star=\pi(s^\prime) = \arg\max_a\; q(s^\prime, a)\]

<p>此时我们不需要进行第二次动作采样，而是直接找到下一时刻状态下已知的最大 $q(s^\prime, a)$，就使用这个值对应的 $a$ 来更新 $q(s,a)$。这种方法即为 <strong>Q-Learning</strong>。对应的<font color="red">策略评估</font>迭代式如下</p>

\[q(s,a) \leftarrow q(s,a)-\alpha\{q(s,a)-[r+\gamma q(s^\prime,a^\star)]\}\]

<p>当然我们也可以避免定义中间的 $a^\star$，直接使用 $\max_a q(s^\prime,a)$ 来简化策略评估迭代式</p>

\[q(s,a) \leftarrow q(s,a)-\alpha\{q(s,a)-[r+\gamma \max_a q(s^\prime,a)]\}\]

<p>经过上述操作，迭代式就不再是求关于某个策略 $\pi$ 的状态-动作价值函数值 $q_\pi(s,a)$，而是直接求 <strong><font color="green">最优</font>状态-动作价值函数 $q_\star(s,a)$ 的的估计</strong>，最终收敛后直接对应的贪婪策略就是最优策略。因此，Q-Learning 在<font color="red">策略改进</font>时更新的 「目标策略」 自然就是 贪婪策略。</p>

<p>注意到，上述内容只是围绕第二次动作采样展开的，并没有对第一次动作采样使用的 「行为策略」 做出任何约束，我们可以沿用 SARSA 的 $\varepsilon$-greedy 策略，也可以使用一些别的策略，如均匀随机采样策略，甚至直接使用人类决策来进行采样都可以。</p>

<p>为便于与 SARSA 算法对比，将 Q-Learning 的目标策略和行为策略总结如下：</p>

<ul>
  <li>行为策略：
    <ul>
      <li>第一次动作采样： $\varepsilon$-greedy 策略、uniform 策略、人类等；</li>
      <li>第二次动作选取： greedy 策略</li>
    </ul>
  </li>
  <li>目标策略：
    <ul>
      <li>greedy 策略</li>
    </ul>
  </li>
</ul>

<p>Q-Learning 的伪代码如下：</p>

<hr />
<ul>
  <li>Initialize $Q(s,a)$ arbitrarily, choose start state $s$, given a <u>behaviour</u> policy $\textcolor{red}{\pi_b}$ <font color="red">(e.g., $\varepsilon$-greedy, uniform, human, etc)</font></li>
  <li>Repeat (for each episode):
    <ul>
      <li>Repeat (if $s$ is not terminal state):
        <ul>
          <li><em>collect experience</em>:
            <ul>
              <li>choose $a$ at $s$ following $\textcolor{red}{\pi_b}$</li>
              <li>take action $a$, get $r$, transfer to $s^\prime$</li>
              <li>
                <font color="red">choose $a^\prime= \arg\max_a q(s^\prime, a)$</font>
              </li>
            </ul>
          </li>
          <li><em>policy evaluation</em>:
            <ul>
              <li>${q(s,a)\leftarrow q(s,a)-\alpha[q(s,a) - (r+\gamma q(s^\prime,a^\prime))]}$</li>
            </ul>
          </li>
          <li><em>policy improvement</em>:
            <ul>
              <li>(update greedy policy as <u>target</u> policy)</li>
              <li>
                <font color="red">$\pi_{t+1}(a\vert s) = 1$ if $a=\arg\max_a q(s,a)$</font>
              </li>
              <li>
                <font color="red">$\pi_{t+1}(a\vert s) = 0$ otherwise</font>
              </li>
              <li>${s\leftarrow}s^\prime; a\leftarrow a^\prime$</li>
            </ul>
          </li>
          <li>$t \leftarrow t+1$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<p>标红的部分即为 Q-Learning 与 SARSA 算法伪代码中的区别所在。注意到，伪代码中 policy improvement 实际上是不用写出来的，因为其目标策略是贪婪策略，当 $q(s,a)$更新时自然会按照最大的 Q 值进行策略更新，不需要额外显式说明。</p>

<h4 id="332-同策略与异策略"><span class="me-2">3.3.2. 同策略与异策略</span><a href="#332-同策略与异策略" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>为了更加清晰地区分 SARSA 和 Q-learning，我们对这两种典型的 TD 算法中提到的目标策略和行为策略再次明确定义如下：</p>

<ul>
  <li>行为策略：用于和环境交互产生样本的策略；</li>
  <li>目标策略：基于价值函数一直保持更新直至最优的策略。</li>
</ul>

<p>当行为策略和目标策略相同时，称其为 <strong>同策略（on-policy）时序差分方法</strong>，不论是和环境交互还是被持续改进，都是同一个策略。当行为策略和目标策略不同时，就称其为 <strong>异策略（off-policy）时序差分方法</strong>。</p>

<p>为了便于比较，我们再次将 SARSA 和 Q-Learning 的策略列表对比如下：</p>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th style="text-align: center">SARSA</th>
      <th style="text-align: center">Q-Learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><font color="blue">行为策略</font></td>
      <td style="text-align: center"><font color="blue">行为策略</font></td>
    </tr>
    <tr>
      <td style="text-align: center">第一次动作采样： $\varepsilon$-greedy 策略</td>
      <td style="text-align: center">第一次动作采样： $\varepsilon$-greedy 策略、uniform 策略、人类等</td>
    </tr>
    <tr>
      <td style="text-align: center">第二次动作采样： $\varepsilon$-greedy 策略</td>
      <td style="text-align: center">第二次动作采样： greedy 策略</td>
    </tr>
    <tr>
      <td style="text-align: center"><font color="red">目标策略</font></td>
      <td style="text-align: center"><font color="red">目标策略</font></td>
    </tr>
    <tr>
      <td style="text-align: center">$\varepsilon$-greedy 策略</td>
      <td style="text-align: center">greedy 策略</td>
    </tr>
  </tbody>
</table></div>

<p>可以看出，SARSA 是一种同策略（on-policy）时序差分方法，因为其采样和更新的均为同一个策略（都是 $\varepsilon$-greedy 策略），而 Q-Learning 是一种异策略（off-policy）时序差分方法，因为其行为策略不是 greedy 策略，但更新的策略是 greedy（贪婪）策略，二者不同。</p>

<p>使用 off-policy 的好处在于，我们可以使用与目标策略不同的更加激进更加具备探索性的行为策略来与环境交互获得经验。行为策略越激进，那么能够探索到的状态动作对就越多，相应价值函数更新的频次就更高，探索效率就自然更高。注意，<strong>在 off-policy中，只要求行为策略与目标策略不同。行为策略本身可以随意选取</strong>，比如当目标策略是贪婪策略时，行为策略可以是 $\varepsilon$-greedy 策略，也可以是均匀随机策略（探索性更强），甚至是人在回路决策策略。</p>

<p>最后给出两个延伸问题供大家思考：</p>

<ul>
  <li>思考1：MC 方法是同策略还是异策略？<font color="blue">（同策略）</font></li>
  <li>思考2：怎么把 Q-Learning 改造成 on-policy 方法？<font color="blue">（第一次动作采样使用$\varepsilon$-greedy，第二次动作选取使用greedy，目标策略使用 $\varepsilon$-greedy）</font></li>
</ul>

<h4 id="333-不同行为策略的探索性"><span class="me-2">3.3.3. 不同行为策略的探索性</span><a href="#333-不同行为策略的探索性" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>下面给出两种不同行为策略下 Q-Learning 探索 Grid World 的示例，场景设置与前述介绍 SARSA 时相同，唯一区别在于此处我们想找到所有状态下的最优策略，而不是某个初始状态的最优轨迹。</p>

<p>首先给出最优策略和其对应的状态价值函数作为参考，如下图所示</p>

<p><a href="/assets/img/postsimg/20221210/q-learning-results-1.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221210/q-learning-results-1.jpg" alt="q-learning-result-optimal" class="lazyload" data-proofer-ignore></a></p>

<ul>
  <li><strong>行为策略是均匀随机策略</strong></li>
</ul>

<p>设置行为策略是均匀随机策略，每个状态的五个动作 <strong>选取概率为 25%</strong>，采用异策略的 Q-Learning 方法迭代 100 万步，得到的结果如下图所示</p>

<p><a href="/assets/img/postsimg/20221210/q-learning-results-2.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221210/q-learning-results-2.jpg" alt="q-learning-result-2" class="lazyload" data-proofer-ignore></a></p>

<p>可以看出，均匀随机策略的探索性比较强，100 万步迭代后每个状态动作对都被多次采样到了。最终得到的最优策略估计（贪婪策略）如下图所示，下图右侧给出了状态价值函数与最优状态价值函数的差随迭代次数变化的曲线</p>

<p><a href="/assets/img/postsimg/20221210/q-learning-results-3.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221210/q-learning-results-3.jpg" alt="q-learning-result-random" class="lazyload" data-proofer-ignore></a></p>

<p>可以看出，最终得到的最优策略估计与真实的最优策略非常接近。</p>

<ul>
  <li><strong>行为策略是有倾向的随机策略</strong></li>
</ul>

<p>接着，我们采用另外一个行为策略，设置所有状态下 <strong>向右走的概率提高至 50%</strong>，剩下 50% 概率由其他动作平分。很明显，这个策略的探索性不如上一个全部动作都均匀随机选择的策略，因为他多了一个向右的倾向性。得到的结果如下图所示</p>

<p><a href="/assets/img/postsimg/20221210/q-learning-results-4.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221210/q-learning-results-4.jpg" alt="q-learning-result-special" class="lazyload" data-proofer-ignore></a></p>

<p>可以看出，经过 100 万步迭代，仍然有很多状态动作对从没有被探索到，得到的状态价值函数估计与真实的最优值相差较大。继续提高向右走的概率，探索性会变得更弱，Q-Learning 方法的收敛速度更慢。</p>

<h3 id="34-总结"><span class="me-2">3.4. 总结</span><a href="#34-总结" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="341-表述形式总结"><span class="me-2">3.4.1. 表述形式总结</span><a href="#341-表述形式总结" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>所有的 TD 算法可以表述为一个统一的形式</p>

\[q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha [q_t(s_t,a_t) - \textcolor{red}{\bar{q}_t}]\]

<p>不同 TD 算法的差异都可以体现在对 TD Target（$\bar{q}_t$）的不同选择上</p>

\[\begin{aligned}
\text{SARSA:}\quad\bar{q}_t &amp;= r_{t+1}+\gamma q_t(s_{t+1},a_{t+1})\\
n\text{-step SARSA:}\quad \bar{q}_t &amp;= r_{t+1} + r_{t+2}+\cdots +\gamma^n q_t(s_{t+n},a_{t+n})\\
\text{Expected SARSA:}\quad\bar{q}_t &amp;= r_{t+1}+\gamma \sum_a \pi(a\vert s_{t+1})q(s_{t+1},a)\\
\text{Q-Learning:}\quad\bar{q}_t &amp;= r_{t+1}+\gamma \max_a q(s_{t+1},a)\\
\text{MC:}\quad\bar{q}_t &amp;= r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^{T-t-1} r_T,\text{ where }\alpha = 1
\end{aligned}\]

<h4 id="342-求解方程总结"><span class="me-2">3.4.2. 求解方程总结</span><a href="#342-求解方程总结" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>从求解贝尔曼方程的角度也可总结如下</p>

\[\begin{aligned}
\text{SARSA:}\quad \text{BE:} \quad&amp;q_\pi(s,a) = \mathbb{E}[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})\vert St+s, A_t=a]\\
n\text{-step SARSA:}\quad \text{BE:} \quad&amp;q_\pi(s,a) = \mathbb{E}[R_{t+1}+\gamma^2R_{t+2}+\cdots+\gamma^n q_\pi(S_{t+n},A_{t+n})\vert St+s, A_t=a]\\
\text{Expected SARSA:}\quad \text{BE:} \quad&amp;q_\pi(s,a) = \mathbb{E}[R_{t+1}+\gamma \mathbb{E}_{A_{t+1}}[q_\pi(S_{t+1},A_{t+1})]\vert St=s, A_t=a]\\
\text{Q-Learning:}\quad \textcolor{red}{\text{BOE:}} \quad&amp;q_\pi(s,a) = \mathbb{E}[R_{t+1}+\gamma \max_a q_\pi(S_{t+1},a)\vert St=s, A_t=a]\\
\text{MC:}\quad \text{BE:} \quad&amp;q_\pi(s,a) = \mathbb{E}[R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-t-1} R_T\vert St=s, A_t=a]
\end{aligned}\]

<h4 id="343-行为表现总结"><span class="me-2">3.4.3. 行为表现总结</span><a href="#343-行为表现总结" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<ul>
  <li>
    <p><strong>Q-Learning</strong>：包含 greedy 策略中取 max 操作，也就是不考虑可能走到很大负奖励的状态-动作，只考虑会不会最终获得最大奖励的状态-动作。如果某状态下某动作可以获得很大的奖励，那这条路就牛逼，所以 Q-Learning更勇猛，不害怕错，更激进;</p>
  </li>
  <li>
    <p><strong>SARSA</strong>：只要某状态周围有错（很大的负奖励），那么就有机会（$\varepsilon/\vert\mathcal{A}\vert$）获得这个不好的奖励，那么整条路反馈都会评分很差，之后会尽量避开。最终导致Sarsa会对犯错更敏感，会远离犯错的状态-动作，更保守。</p>
  </li>
</ul>

<h2 id="4-参考文献"><span class="me-2">4. 参考文献</span><a href="#4-参考文献" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>[1] shuhuai008. <a href="https://www.bilibili.com/video/BV1BS4y1r7cm">bilibili【强化学习】(SARSA) 时序差分-同轨策略TD控制</a></p>

<p>[2] 莫烦. <a href="https://zhuanlan.zhihu.com/p/24860793">什么是 Sarsa (强化学习)</a></p>

</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw me-1"></i>
    
      <a href='/categories/academic/'>Academic</a>,
      <a href='/categories/knowledge/'>Knowledge</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw me-1"></i>
      
      <a href="/tags/python/"
          class="post-tag no-text-decoration" >python</a>
      
      <a href="/tags/reinforcement-learning/"
          class="post-tag no-text-decoration" >reinforcement learning</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        本文由作者按照 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         进行授权

      
    </div>

    <!-- Post sharing snippet -->

<div class="share-wrapper">
  <span class="share-label text-muted me-1">分享</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%B3%95%EF%BC%89%20-%20SIRLIS&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Temporal-Differences%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%B3%95%EF%BC%89%20-%20SIRLIS&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Temporal-Differences%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Temporal-Differences%2F&text=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%B3%95%EF%BC%89%20-%20SIRLIS"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <i
      id="copy-link"
      class="fa-fw fas fa-link small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="分享链接"
      data-title-succeed="链接已复制！"
    >
    </i>
  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
      
    </div>
  </div>
  <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
    <div class="access">
      <!-- Get the last 5 posts from lastmod list. -->














  <div id="access-lastmod" class="post">
    <div class="panel-heading">最近更新</div>
    <ul class="post-content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Policy-Gradient/">强化学习（策略梯度法）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Nonlinear-Classifier/">模式识别（非线性分类器）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Temporal-Differences/">强化学习（时序差分法）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Value-Approximation/">强化学习（值函数近似）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Bayes/">模式识别（贝叶斯决策）</a>
        </li>
      
    </ul>
  </div>
  <!-- #access-lastmod -->


      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>

    
      
      



  <div id="toc-wrapper" class="ps-0 pe-4 mb-5">
    <div class="panel-heading ps-3 pt-2 mb-2">文章内容</div>
    <nav id="toc"></nav>
  </div>


    
  </div>
</div>

<!-- tail -->

  <div class="row">
    <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-3 pe-xl-4 mt-5">
      
        
        <!--
  Recommend the other 3 posts according to the tags and categories of the current post,
  if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->








  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  






<!-- Fill with the other newlest posts -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ms-1" data-toc-skip>
      相关文章
    </h3>
    <div class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-markov-process/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1667968579"
  data-df="YYYY/MM/DD"
  
>
  2022/11/09
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（马尔可夫决策过程）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的基本概念和模型，主要包括马尔可夫过程、马尔可夫奖励过程和马尔可夫决策过程。






  1. 强化学习
    
      1.1. 状态空间
        
          1.1.1. 状态
          1.1.2. 观测
        
      
      1.2. 动作空间
      1.3. 策略
    
  
  2. 马尔可夫...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Dynamic-Programming/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1669433059"
  data-df="YYYY/MM/DD"
  
>
  2022/11/26
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（动态规划）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的动态规划法（Dynamic Programming，DP），采用动态规划的思想，分别介绍策略迭代和价值迭代方法。






  1. 强化学习问题的求解
  2. 动态规划
    
      2.1. 策略迭代
        
          2.1.1. 策略评估
          2.1.2. 策略改进
          2.1.3. 算法流程
   ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Monte-Carlo/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1669712839"
  data-df="YYYY/MM/DD"
  
>
  2022/11/29
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（蒙特卡洛法）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的 model-free 方法——蒙特卡洛法。






  1. 引言
  2. 蒙特卡洛法
    
      2.1. 大数定律与蒙特卡洛思想
      2.2. 蒙特卡洛基础算法
        
          2.2.1. 蒙特卡洛采样
          2.2.2. 蒙特卡洛价值估计
          2.2.3. 算法流程
        
 ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
    </div>
    <!-- .card-deck -->
  </div>
  <!-- #related-posts -->


      
        
        <!-- Navigation buttons at the bottom of the post. -->

<div class="post-navigation d-flex justify-content-between">
  
    <a
      href="/posts/reinforcement-learning-Monte-Carlo/"
      class="btn btn-outline-primary"
      prompt="上一篇"
    >
      <p>强化学习（蒙特卡洛法）</p>
    </a>
  

  
    <a
      href="/posts/reinforcement-learning-Value-Approximation/"
      class="btn btn-outline-primary"
      prompt="下一篇"
    >
      <p>强化学习（值函数近似）</p>
    </a>
  
</div>

      
        
        <!--  The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script src="https://utteranc.es/client.js"
        repo="sirlis/sirlis.github.io"
        issue-term="pathname"
        crossorigin="anonymous"
        async>
</script>

<script type="text/javascript">
  $(function() {
    const origin = "https://utteranc.es";
    const iframe = "iframe.utterances-frame";
    const lightTheme = "github-light";
    const darkTheme = "github-dark";
    let initTheme = lightTheme;

    if ($("html[data-mode=dark]").length > 0
        || ($("html[data-mode]").length == 0
            && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
      initTheme = darkTheme;
    }

    addEventListener("message", (event) => {
      let theme;

      /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */
      if (event.origin === origin) {
        /* page initial */
        theme = initTheme;

      } else if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {
        /* global theme mode changed */
        const mode = event.data.message;
        theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme);

      } else {
        return;
      }

      const message = {
        type: "set-theme",
        theme: theme
      };

      const utterances = document.querySelector(iframe).contentWindow;
      utterances.postMessage(message, origin);
    });

  });
</script>



      
    </div>
  </div>


        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 post-content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>
    </div>

    <!-- The Footer -->

<footer>
  <div class="container px-lg-4">
    <div class="d-flex justify-content-center align-items-center text-muted mx-md-3">
      <p>本站采用 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 主题 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
      </p>

      <p>©
        2025
        <a href="https://github.com/sirlis">sirlis</a>.
        
          <span
            data-bs-toggle="tooltip"
            data-bs-placement="top"
            title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。"
          >保留部分权利。</span>
        
      </p>
    </div>
  </div>
</footer>


    <div id="mask"></div>

    <button id="back-to-top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow">
      <i class="fas fa-angle-up"></i>
    </button>

    
      <div
        id="notification"
        class="toast"
        role="alert"
        aria-live="assertive"
        aria-atomic="true"
        data-bs-animation="true"
        data-bs-autohide="false"
      >
        <div class="toast-header">
          <button
            type="button"
            class="btn-close ms-auto"
            data-bs-dismiss="toast"
            aria-label="Close"
          ></button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="px-2 mb-3">发现新版本的内容。</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            更新
          </button>
        </div>
      </div>
    

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.0/dist/jquery.min.js,npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.7/dayjs.min.js,npm/dayjs@1.11.7/locale/zh.min.js,npm/dayjs@1.11.7/plugin/relativeTime.min.js,npm/dayjs@1.11.7/plugin/localizedFormat.min.js,npm/tocbot@4.21.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '<div class="px-1 px-sm-2 px-lg-4 px-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

