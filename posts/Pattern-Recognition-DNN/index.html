<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="zh-CN" 
  
>
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="模式识别（经典神经网络）" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="本文介绍了经典的人工神经网络，以及基于神经网络的分类器。人工神经网络（Artificial Neural Network，ANN）是模拟生物神经网络系统的一种计算模型，由大量计算节点（神经元）层层连接构成。通过改变神经元间的连接权重，ANN 可以对大量数据中的复杂映射关系进行建模。对于给定的任务，通过迭代训练进行计算节点权重参数更新，ANN 最终能够实现任务目标。" />
<meta property="og:description" content="本文介绍了经典的人工神经网络，以及基于神经网络的分类器。人工神经网络（Artificial Neural Network，ANN）是模拟生物神经网络系统的一种计算模型，由大量计算节点（神经元）层层连接构成。通过改变神经元间的连接权重，ANN 可以对大量数据中的复杂映射关系进行建模。对于给定的任务，通过迭代训练进行计算节点权重参数更新，ANN 最终能够实现任务目标。" />
<link rel="canonical" href="http://localhost:4000/posts/Pattern-Recognition-DNN/" />
<meta property="og:url" content="http://localhost:4000/posts/Pattern-Recognition-DNN/" />
<meta property="og:site_name" content="SIRLIS" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-05-27T14:32:49+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="模式识别（经典神经网络）" />
<meta name="twitter:site" content="@none" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-18T10:35:05+08:00","datePublished":"2025-05-27T14:32:49+08:00","description":"本文介绍了经典的人工神经网络，以及基于神经网络的分类器。人工神经网络（Artificial Neural Network，ANN）是模拟生物神经网络系统的一种计算模型，由大量计算节点（神经元）层层连接构成。通过改变神经元间的连接权重，ANN 可以对大量数据中的复杂映射关系进行建模。对于给定的任务，通过迭代训练进行计算节点权重参数更新，ANN 最终能够实现任务目标。","headline":"模式识别（经典神经网络）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/Pattern-Recognition-DNN/"},"url":"http://localhost:4000/posts/Pattern-Recognition-DNN/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>模式识别（经典神经网络） | SIRLIS
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SIRLIS">
<meta name="application-name" content="SIRLIS">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.0/dist/tocbot.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/head.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <div class="site-title">
      <a href="/">SIRLIS</a>
    </div>
    <div class="site-subtitle fst-italic">分享科研和生活的日常</div>
  </div>
  <!-- .profile-wrapper -->

  <ul class="nav flex-column flex-grow-1 w-100 ps-0">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home"></i>
        <span>首页</span>
      </a>
    </li>
    <!-- the real tabs -->
    
      <li class="nav-item">
        <a href="/categories/" class="nav-link">
          <i class="fa-fw fas fa-stream"></i>
          

          <span>分类</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/tags/" class="nav-link">
          <i class="fa-fw fas fa-tags"></i>
          

          <span>标签</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/archives/" class="nav-link">
          <i class="fa-fw fas fa-archive"></i>
          

          <span>归档</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/about/" class="nav-link">
          <i class="fa-fw fas fa-info-circle"></i>
          

          <span>关于</span>
        </a>
      </li>
      <!-- .nav-item -->
    
  </ul>
  <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/sirlis"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/none"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['lihongjue','buaa.edu.cn'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</div>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container px-xxl-5">
        <!-- The Top Bar -->

<div id="topbar-wrapper">
  <div
    id="topbar"
    class="container d-flex align-items-center justify-content-between h-100"
  >
    <span id="breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                首页
              </a>
            </span>

          
        
          
        
          
            
              <span>模式识别（经典神经网络）</span>
            

          
        
      
    </span>
    <!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      文章
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="搜索..."
      >
    </span>
    <span id="search-cancel">取消</span>
  </div>
</div>

        











<div class="row">
  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pe-xl-4">
    

    <div class="post px-1 px-md-2">
      

      
        
      
        <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->



  
  

  <!-- CDN URL -->
  

  <!-- Add image path -->
  

  
    
      
      
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  




<!-- return -->




<h1 data-toc-skip>模式识别（经典神经网络）</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      发表于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1748327569"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2025/05/27
</em>

    </span>

    <!-- lastmod date -->
    
    <span>
      更新于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1760754905"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2025/10/18
</em>

    </span>
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      作者

      <em>
      
        <a href="https://github.com/sirlis">sirlis</a>
      
      </em>
    </span>

    <div>
      <!-- read time -->
      <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="10798 字"
>
  <em>59 分钟</em>阅读</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <p>本文介绍了经典的人工神经网络，以及基于神经网络的分类器。人工神经网络（Artificial Neural Network，ANN）是模拟生物神经网络系统的一种计算模型，由大量计算节点（神经元）层层连接构成。通过改变神经元间的连接权重，ANN 可以对大量数据中的复杂映射关系进行建模。对于给定的任务，通过迭代训练进行计算节点权重参数更新，ANN 最终能够实现任务目标。</p>

<!--more-->

<hr />
<ul>
  <li><a href="#1-神经元">1. 神经元</a>
    <ul>
      <li><a href="#11-人工神经元">1.1. 人工神经元</a></li>
      <li><a href="#12-激活函数">1.2. 激活函数</a>
        <ul>
          <li><a href="#121-阶跃函数">1.2.1. 阶跃函数</a></li>
          <li><a href="#122-sigmoid-函数">1.2.2. Sigmoid 函数</a></li>
          <li><a href="#123-tanh-函数">1.2.3. Tanh 函数</a></li>
          <li><a href="#124-relu-系列函数">1.2.4. ReLU 系列函数</a>
            <ul>
              <li><a href="#1241-relu-函数">1.2.4.1. ReLU 函数</a></li>
              <li><a href="#1242-leakyrelu-函数">1.2.4.2. LeakyReLU 函数</a></li>
              <li><a href="#1243-elu-函数">1.2.4.3. ELU 函数</a></li>
            </ul>
          </li>
          <li><a href="#125-swish-函数">1.2.5. Swish 函数</a></li>
          <li><a href="#126-恒等映射函数">1.2.6. 恒等映射函数</a></li>
          <li><a href="#127-softmax-函数">1.2.7. softmax 函数</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#2-神经网络">2. 神经网络</a>
    <ul>
      <li><a href="#21-感知机">2.1. 感知机</a></li>
      <li><a href="#22-多层感知机">2.2. 多层感知机</a>
        <ul>
          <li><a href="#221-网络结构">2.2.1. 网络结构</a></li>
          <li><a href="#222-信号前向推理">2.2.2. 信号前向推理</a></li>
          <li><a href="#223-误差反向传播">2.2.3. 误差反向传播</a>
            <ul>
              <li><a href="#2231-损失函数">2.2.3.1. 损失函数</a></li>
              <li><a href="#2232-反向传播">2.2.3.2. 反向传播</a></li>
              <li><a href="#2233-梯度消失">2.2.3.3. 梯度消失</a></li>
              <li><a href="#2234-梯度爆炸">2.2.3.4. 梯度爆炸</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#23-自组织神经网络">2.3. 自组织神经网络</a>
        <ul>
          <li><a href="#231-自组织竞争神经网络">2.3.1. 自组织竞争神经网络</a></li>
          <li><a href="#232-自组织映射神经网络">2.3.2. 自组织映射神经网络</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#3-参考文献">3. 参考文献</a></li>
</ul>

<h2 id="1-神经元"><span class="me-2">1. 神经元</span><a href="#1-神经元" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="11-人工神经元"><span class="me-2">1.1. 人工神经元</span><a href="#11-人工神经元" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>神经元是神经系统的基本组成单位，一个神经元就是一个神经细胞。人类大脑包含大约 1000 亿个神经元，这些神经元相互联系在一起，组成一个复杂的网络。一个生物神经元如下图所示：</p>

<p><a href="/assets/img/postsimg/20250527/neuron.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/neuron.jpg" alt="neuron" class="lazyload" data-proofer-ignore></a></p>

<p>对于信息传递而言，涉及到的生物神经元组成包括：</p>

<ul>
  <li><strong>树突</strong>：树突是细胞体外的微小分支，用于接收来自其他神经元的信号，这些信号就是神经元的输入；</li>
  <li><strong>轴突</strong>：轴突是充当传输线的纤维，它将信号从细胞体传递到其他神经元，这些信号就是神经元的输出；</li>
  <li><strong>突触</strong>：突触是一个神经元与另一个神经元相联系的特殊部位，通常是一个神经元轴突的端部靠化学接触或电接触将信号传递给下一个神经元的树突或细胞体。</li>
</ul>

<p>基于生物神经元，建立人工神经元数学模型如下图所示：</p>

<p><a href="/assets/img/postsimg/20250527/artifical-neuron.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/artifical-neuron.jpg" alt="artifical_neuron" class="lazyload" data-proofer-ignore></a></p>

<p>如同生物神经元能够接收多个输入信号一样，人工神经元也有多个信号输入通道，这里我们将输入信号表示为 $x_i$，输入信号的属性（兴奋或抑制神经元）及重要程度（信号强度）则通过输入通道权重参数 $w_i$ 的符号及其绝对值大小来控制。所有输入信号的累计效果则通过输入信号与权重参数的加权求和进行量化。当量化的累计效果达到某个阈值时，人工神经元将被激活，并输出一个非零数值 $y$ 代表输出信号。这里，激活阈值通过偏置项 $b$ 进行控制，具体来说，通过调整该偏置项的数值，我们可以控制神经元对于不同输入的敏感程度。当偏置项的数值增加时，神经元更容易被激活，即更容易输出激活信号；而偏置项减小时，则意味着对输入信号的敏感度下降，相对更不容易激活。输出信号和输入信号的具体映射关系通过激活函数（Activation Function）$f$ 进行控制。经过上述过程，该神经元输出可表示为：</p>

\[y=f\left(\sum_{i=1}^n x_iw_i+b\right)\]

<h3 id="12-激活函数"><span class="me-2">1.2. 激活函数</span><a href="#12-激活函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>神经元的激活函数通常选为非线性函数，这使得由神经元层层连接构成的神经网络具有了逼近任何非线性函数的能力。神经网络强大的非线性建模能力使其被广泛应用于各种复杂非线性建模场景。本节介绍传统神经网络中常用的一些激活函数。</p>

<blockquote>
  <p>可证明，当激活函数采用线性函数时，对于多层神经网络，无论网络中隐含层的层数为多少，其输出是输入的线性拟合。这表明没有引入非线性激活函数的神经网络和直接使用线性模型的最终效果相同，也就是说神经网络中的隐含层不起作用。</p>

\[\begin{aligned}
A^{[2]} &amp;= g(Z^{[2]}) = \lambda(W^{[2]}A^{[1]} + B^{[2]}) \\
&amp;= \lambda(W^{[2]}\lambda(W^{[1]}X + B^{[1]}) + B^{[2]}) \\
&amp;= \lambda^2 W^{[2]}W^{[1]}X + \lambda^2 W^{[2]}B^{[1]} + \lambda B^{[2]}
\end{aligned}\]

  <p>因此，要使神经网络能够发挥其作用，其激活函数必须是非线性函数。</p>
</blockquote>

<h4 id="121-阶跃函数"><span class="me-2">1.2.1. 阶跃函数</span><a href="#121-阶跃函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>阶跃函数又被称为阈值激活函数，是最简单的激活函数，输出取二进制值。当输入高于某一个阈值时，神经元被激活；当输入低于该阈值时，神经元不被激活。阈值激活函数的数学表达式为：</p>

\[f(x) = \text{sgn}(x) = \begin{cases}
  0,\; x\leq 0\\
  1,\; x&gt; 0
\end{cases}\]

<p>其函数图像为</p>

<p><a href="/assets/img/postsimg/20250527/step-function.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/step-function.jpg" alt="step_function" class="lazyload" data-proofer-ignore></a></p>

<p>当使用节约函数时，神经元输出的数学表达式为：</p>

\[y=\text{sgn}\left(\sum_{i=1}^n x_i w_i+b\right)\]

<p>通过阶跃函数，我们可以直观认识到偏置参数 $b$ 对神经元兴奋抑制的调节。当偏置项的数值增加时，神经元更容易被激活，即更容易输出激活信号；而偏置项减小时，则意味着对输入信号的敏感度下降，相对更不容易激活。</p>

<h4 id="122-sigmoid-函数"><span class="me-2">1.2.2. Sigmoid 函数</span><a href="#122-sigmoid-函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>Sigmoid 函数是一个形似 S 的函数，又被称为 Logistic 函数，因为它是线性回归转换为Logistic（逻辑回归）的核心函数。其本身及其反函数都具有单调递增等的性质，因此常被用作人工神经元的激活函数。Sigmoid 函数将变量映射到 $(0,1)$ 范围内，其函数与其导数的数学表达式为：</p>

\[\begin{aligned}
f(x) &amp;= \frac{1}{1+e^{-x}}\\
f^\prime(x) &amp;= \frac{e^{-x}}{(1+e^{-x})^2} = f(x)(1-f(x)) 
\end{aligned}\]

<p>函数及其导数的图像为</p>

<p><a href="/assets/img/postsimg/20250527/sigmoid.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/sigmoid.jpg" alt="sigmoid" class="lazyload" data-proofer-ignore></a></p>

<p>Sigmoid 函数是可微且梯度平滑的，避免了出现跳跃的输出函数值。该函数的值域为 $[0,1]$ 这使得其具备以下优势：</p>

<ul>
  <li>
    <p>概率分布：根据概率公理化定义知道，概率的取值范围在 $[0, 1]$ 之间，Sigmoid 函数的输出和概率分布的取值范围契合。这也是 Logistic（逻辑回归）使用 Sigmoid 函数的原因之一；</p>
  </li>
  <li>
    <p>信号强度：一般可以将 $0\sim 1$ 理解成某种信号的强度。由于RNN循环神经网络只能够解决短期依赖的问题，不能够解决长期依赖的问题，因此提出了 LSTM、GRU，这些网络相比于 RNN 最大的特点就是加入了门控制，通过门来控制是否允许记忆通过，而 Sigmoid 函数还能够代表门控值（Gate）的强度，当 Sigmoid 输出 $1$ 的时候代表当前门控全部开放（允许全部记忆通过），当 Sigmoid 输出 $0$ 的时候代表门控关闭（不允许任何记忆通过）。</p>
  </li>
</ul>

<p>观察 Sigmoid 函数及其导数可以发现，其导数可以用其函数值简单计算求得，规避了复杂的求导计算。这也是激活函数选取的重要因素之一。</p>

<p>Sigmoid 函数的缺点在于：</p>

<ul>
  <li>包含指数项，计算量较大；</li>
  <li>函数输出不是以 $0$ 为中心正负分布的，导致网络权重都往正或都往负的方向更新，使得权重更新效率低；</li>
  <li>当输入远离原点时，函数的梯度趋近于0，导致梯度消失问题。</li>
</ul>

<p>Sigmoid型函数的以上缺陷限制了其在现代神经网络模型中的应用。</p>

<h4 id="123-tanh-函数"><span class="me-2">1.2.3. Tanh 函数</span><a href="#123-tanh-函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>Tanh 函数是双曲正切函数，由基本双曲函数中的双曲正弦函数和双曲余弦函数推导所得，其数学表达式为：</p>

\[\tanh(x)=\frac{\sinh(x)}{\cosh(x)} = \frac{e^x-e^{-x}}{e^x+e^{-x}}\]

<p>该函数可以看作是经过放大平移的Sigmoid型激活函数，两个函数数学表达式之间的关系为</p>

\[\tanh(x) = \frac{2}{1+e^{-2x}}-1 = 2\times\text{Sigmoid}(2x)-1\]

<p>Tanh 函数及其导数图像为</p>

<p><a href="/assets/img/postsimg/20250527/tanh.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/tanh.jpg" alt="tanh" class="lazyload" data-proofer-ignore></a></p>

<p>Tanh 函数的值域是 $(-1,1)$，且以 $0$ 为中心。相较于 Sigmoid 函数，Tanh 函数收敛速度较快。</p>

<p>优点：</p>

<ul>
  <li>输出均值为 $0$。这是 tanh 非常重要的一个优点；</li>
  <li>在原点附近与 $y = x$ 函数形式相近，当激活值比较低的时候，训练相对比容易；</li>
  <li>变化敏感区间较宽，缓解梯度弥散的现象。tanh 导数取值范围在 0 到 1 之间，要优于 sigmoid 激活函数的 0 到 0.25，相比于Sigmoid 函数能够缓解梯度弥散的现象；</li>
</ul>

<p>缺点：</p>

<ul>
  <li>包含指数项，计算量比较大。</li>
</ul>

<h4 id="124-relu-系列函数"><span class="me-2">1.2.4. ReLU 系列函数</span><a href="#124-relu-系列函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<h5 id="1241-relu-函数"><span class="me-2">1.2.4.1. ReLU 函数</span><a href="#1241-relu-函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>ReLU 全称是 Rectified Linear Unit，中文名称是线性整流函数，是在神经网络中常用的激活函数。通常意义下，其指代数学中的斜坡函数，即</p>

\[\text{ReLU}(x) = max(0, x)\]

<p>函数及其导数图像如下</p>

<p><a href="/assets/img/postsimg/20250527/relu.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/relu.jpg" alt="ReLU函数" class="lazyload" data-proofer-ignore></a></p>

<p>通过 ReLU 激活函数的函数图像可以看到，ReLU 对小于 $0$ 的值全部抑制为 $0$；对于正数则直接输出，这是一种单边抑制的特性，而这种单边抑制来源于生物学。ReLU 函数的导数计算也非常简单，$x$ 大于等于 $0$ 的时候，导数值为 $1$，在反向传播的过程中，它既不会放大梯度，造成梯度爆炸；也不会缩小梯度，造成梯度弥散的现象。</p>

<blockquote>
  <p>2012 年 ImageNet 竞赛的冠军模型是由 Hinton 和他的学生 Alex 设计的 AlexNet，其中使用了一个新的激活函数 ReLU（REctified Linear Unit，修正线性单元）。在这之前 Sigmoid 函数通常是神经网络激活函数的首选，上面也提到过，Sigmoid 函数在输入值较大或较小的时候容易出现梯度值接近于0的现象，也就是梯度弥散。出现梯度弥散现象，网络参数会长时间得不到更新，很容易导致训练不收敛或停滞不动的现象发生，网络层数较深的网络模型更容易发生梯度弥散现象，使得对神经网络的研究一直停留在浅层网络。值得一提的是，AlexNet是一个 8 层的网络，而后续提出的上百层的卷积神经网络也多是采用 ReLU 激活函数。</p>
</blockquote>

<p>ReLU函数的优缺点。</p>

<p>优点：</p>

<ul>
  <li>梯度不发散。当输入值为正的时候，梯度恒为 $1$，没有梯度发散的现象，收敛速度快；</li>
  <li>增大了网络的稀疏性。当输入值 $x &lt; 0$ 的时候，该层的输出为 $0$，训练完成后为 $0$ 的神经元越来越多，稀疏性会变大，网络的协同性会被破坏，更够迫使网络学习到更一般性的特征，泛化能力会变强。这也正是 dropout 的原理；</li>
  <li>计算量变小。ReLU 函数和 Sigmoid 函数相比少了复杂的幂运算，且计算量变小；</li>
</ul>

<p>缺点：</p>

<ul>
  <li>输出均值非 $0$；</li>
  <li>ReLU 死亡。若遇到异常输入导致输出和真实标签差距很大，此时误差反向传播更新参数（如偏置 $b$）使得 $b &lt; 0$，这使得后续正常输入经过该神经元后的输出值小于 $0$，此时神经元输出恒为 $0$, 对输出没有任何贡献，梯度反向传播时参数也得不到更新，那么此时该神经元就会 “死亡”。并且随着网络的训练，该层前面的神经元参数的更新幅度会随着模型的收敛而趋缓，输出变动会逐渐降低，这使得从概率上来说当前神经元完全复活的可能性极小, 也就可以称之为不可逆的死亡。</li>
</ul>

<h5 id="1242-leakyrelu-函数"><span class="me-2">1.2.4.2. LeakyReLU 函数</span><a href="#1242-leakyrelu-函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>LeakyReLU 函数用于环节 ReLU 神经元死亡的问题，它在激活函数输入 $x&lt;0$ 区间给出一个很小的负数输出梯度值，而不是直接输出 $0$，计算公式为：</p>

\[f(x) = \max(\alpha x, x)\]

<p>其中 $\alpha$ 是一个很小的常数，通常可赋值为 $0.01$。</p>

<p>函数及其导数图像为</p>

<p><a href="/assets/img/postsimg/20250527/leaky-relu.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/leaky-relu.jpg" alt="Leaky-ReLU函数" class="lazyload" data-proofer-ignore></a></p>

<p>通过这个操作可以保留一些输入为负数时的信息，避免了输入为负数时神经元死亡的情况。</p>

<h5 id="1243-elu-函数"><span class="me-2">1.2.4.3. ELU 函数</span><a href="#1243-elu-函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>ELU 函数（Exponential Linear Unit，指数线性单元）融合了 Sigmoid 函数和 ReLU 函数，该函数左侧具有软饱和性，右侧无饱和性。ELU 的定义为：</p>

\[f(x) = \begin{cases}
  x  &amp; \text{if } x \ge 0 \\
  \alpha(e^x - 1) &amp; \text{if } x &lt; 0
\end{cases}\]

<p>函数图像为</p>

<p><a href="/assets/img/postsimg/20250527/elu.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/elu.jpg" alt="ELU函数" class="lazyload" data-proofer-ignore></a></p>

<h4 id="125-swish-函数"><span class="me-2">1.2.5. Swish 函数</span><a href="#125-swish-函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>Swish 函数（Swish Activation Function）是 2017 年 Google Brain 团队提出的一种激活函数，该函数的数学定义为：</p>

\[f(x)  = x \cdot \text{Sigmoid}(\beta x)\]

<p>Swish 函数的图像如下</p>

<p><a href="/assets/img/postsimg/20250527/swish.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/swish.jpg" alt="Swish函数" class="lazyload" data-proofer-ignore></a></p>

<ul>
  <li>当 $\beta = 0$ 时，Swish 函数变为线性函数 $f(x) = \frac{x}{2}$；</li>
  <li>当 $\beta = \infty$ 时，Swish 函数变为 $0$ 或 $x$，相当于 ReLU 函数。</li>
</ul>

<p>所以，Swish 函数可以看作是介于线性函数与 ReLU 函数之间的平滑函数。</p>

<p>Swish 函数的导数计算如下</p>

\[\frac{d}{dx} \text{Swish}(x) = \beta\cdot \text{Swish}(x) + \text{Sigmoid}(\beta x) (1-\beta\cdot \text{Swish}(x))\]

<p>Swish 函数的导数图像如下</p>

<p><a href="/assets/img/postsimg/20250527/swish-derivation.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/swish-derivation.jpg" alt="Swish函数的导数" class="lazyload" data-proofer-ignore></a></p>

<h4 id="126-恒等映射函数"><span class="me-2">1.2.6. 恒等映射函数</span><a href="#126-恒等映射函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>恒等映射函数作为激活函数，通常被用于<strong>回归任务中的输出层</strong>，就是没有任何非线性变换，即</p>

\[f(x) = x\]

<p>这意味着网络输出层的激活值直接作为预测值，预测值是最后一层的线性组合结果。</p>

<h4 id="127-softmax-函数"><span class="me-2">1.2.7. softmax 函数</span><a href="#127-softmax-函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>softmax 函数是常用的激活函数，一般用于<strong>多分类任务的输出层</strong>。表达式如下：</p>

\[\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}\]

<p>它将输入向量映射到 $[0,1]$ 区间的输出量，且各个输出量之和为 $1$，因此很好吻合多分类任务中的概率输出。如下图所示：</p>

<p><a href="/assets/img/postsimg/20250527/softmax.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/softmax.jpg" alt="softmax" class="lazyload" data-proofer-ignore></a></p>

<p>由于交叉熵的输入是概率，因此经常和 sigmoid 或 softmax 结合在一起。一般情况下，在神经网络中，最后一个输出层的节点个数与分类任务的目标数相等。假设最后的节点数为 $N$，那么对于每一个样例，神经网络可以得到一个 $N$ 维的数组作为输出结果，数组中每一个维度会对应一个类别。在最理想的情况下，如果一个样本属于第 $k$ 类别，那么这个类别所对应的的输出节点的输出值应该为 $1$，而其他节点的输出都为 $0$，即 $[0,0,1,0,\cdots,0,0]$，这个数组也就是样本的「独热编码标签」，是神经网络最期望的输出结果，交叉熵就是用来判定实际的输出与期望的输出的接近程度。</p>

<h2 id="2-神经网络"><span class="me-2">2. 神经网络</span><a href="#2-神经网络" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="21-感知机"><span class="me-2">2.1. 感知机</span><a href="#21-感知机" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>感知机是最简单的前馈式人工神经网络，它是由美国心理学家 Frank Roseblatt 在 1957 年提出，是一种单层的前馈神经网络，包含一个输入层和一个输出节点，且神经元的激活函数为 <a href="#121-阶跃函数">阶跃函数</a>。使用阶跃函数作为激活函数时，神经元就是前面介绍过的 <a href="../Pattern-Recognition-Linear-Classifier/##53-感知机模型">感知机模型</a> 。</p>

<p>感知机模型如图：</p>

<p><a href="/assets/img/postsimg/20250527/perceptron.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/perceptron.jpg" alt="单层感知机" class="lazyload" data-proofer-ignore></a></p>

<p>数学模型如下：</p>

\[y = f(\boldsymbol{W}^\top\boldsymbol{x}+b),\quad f(z)=\begin{cases}
  1  &amp; \text{if } z\geq 0,\\
  0  &amp; \text{if } z&lt; 0.
\end{cases}\]

<blockquote>
  <p>1969年 ， Marvin Minsky 和 Seymour Papert 出版了《感知机：计算几何学导论》（Perceptrons: An Introduction to Computational Geometry），从数学上证明了单层前馈感知机的局限性，<strong>感知机只能解决「线性可分问题」</strong>，甚至不能解决简单的 “异或” 逻辑问题。此后，神经网络的研究几乎处于休眠状态，直到上世纪 80 年代才又被从历史的角落重新拾起。</p>
</blockquote>

<p>假设分离超平面的误分类点集合为 $Y_e$，则所有误分类点到超平面的总距离为：</p>

\[\frac{1}{\Vert\boldsymbol{W}\Vert}\sum_{\boldsymbol{x}_i\in Y_e}\vert \boldsymbol{W}^\top\boldsymbol{x}_i+b\vert\]

<p>如果不考虑系数，感知机学习的算法是通过学习找到一组参数 $(\boldsymbol{W}^\star,b^\star)$ ，使得以下损失函数最小：</p>

\[\min L(\boldsymbol{W},b) = -\sum_{\boldsymbol{x}_i\in Y_e} y_i(\boldsymbol{W}^\top\boldsymbol{x}_i+b)\]

<blockquote>
  <p>（1）当误分类时，满足 $-y_i(\boldsymbol{W}^\top\boldsymbol{x}_i+b) \geq 0$，因此可设计上述损失函数（思考：为什么？）</p>

  <p>（2）感知机的一般推导会使用增广变换（消除偏置）和规范化（负样本取反），但此处没有使用；</p>
</blockquote>

<p>如果样本集非常大，每次都要计算所有误分样本，这需要耗费非常大的计算资源，实际应用中通常使用随机梯度下降算法代替，即每次随机选取一个误分样本更新参数。感知机学习算法本质是随机梯度下降的变种</p>

\[\min L(\boldsymbol{W},b) = -y_i(\boldsymbol{W}^\top\boldsymbol{x}_i+b),\quad \boldsymbol{x}_i\in Y_e\]

<p>对应的参数更新式为</p>

\[\begin{aligned}
\boldsymbol{W} &amp;\leftarrow \boldsymbol{W} + \eta y_i \boldsymbol{x}_i\\
b &amp;\leftarrow b + \eta y_i
\end{aligned}\]

<blockquote>
  <p>Novikoff 定理保证，若规范化增广训练数据集是线性可分的，则随机梯度下降法在规范化增广训练数据集上的误分类次数一定小于某个值，即感知机训练过程可在有限轮迭代内完成，即感知机训练算法是收敛的。</p>
</blockquote>

<hr />

<p>例：某个训练数据集，其正样本为：$\boldsymbol{x}_1=[3,3]^\top, \boldsymbol{x}_2=[4,3]^\top$，负样本为：$\boldsymbol{x}_3=[1,1]^\top$，试用感知准则函数求解感知机模型 $g(x) = f(\boldsymbol{W}^\top\boldsymbol{x}+b),\; \boldsymbol{W}=[w_1, w_2]^\top$。</p>

<p><a href="/assets/img/postsimg/20250527/Perceptron-classifier.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/Perceptron-classifier.jpg" alt="Perception_classifier" class="lazyload" data-proofer-ignore></a></p>

<p>构建最优问题：$\min \; L(\boldsymbol{W},b)=-\sum_{\boldsymbol{x}\in Y_e} y_i(\boldsymbol{W}^\top\boldsymbol{x}+b)$；</p>

<p>设学习率 $\eta=1$，初始权值和偏置 $\boldsymbol{w}=[0,0]^\top, b=0$；</p>

<p>对于 $\boldsymbol{x}_1=[3,3]^\top$，计算得到 $y_1(\boldsymbol{W}^\top\boldsymbol{x}_1+b) = 0$，判定为错误分类，利用该点更新参数</p>

\[\begin{aligned}
\boldsymbol{W} &amp;\leftarrow \boldsymbol{W} + \eta y_1\boldsymbol{x}_1 \Rightarrow \boldsymbol{w} = [3,3]^\top\\
b &amp;\leftarrow b + \eta y_1 \Rightarrow b = 1
\end{aligned}\]

<p>对于 $\boldsymbol{x}_1,\boldsymbol{x}_2$，计算判定分类正确；</p>

<p>对于 $\boldsymbol{x}_3=[1,1]^\top$，计算得到判定分类错误，更新参数如下：</p>

\[\begin{aligned}
\boldsymbol{W} &amp;\leftarrow \boldsymbol{W} + \eta y_3\boldsymbol{x}_3 \Rightarrow \boldsymbol{w} = [2,2]^\top\\
b &amp;\leftarrow b + \eta y_3 \Rightarrow b = 0
\end{aligned}\]

<p>不断遍历误分类点，使用误分类样本更新参数，直到误分类点为空集。</p>

<p><a href="/assets/img/postsimg/20250527/Perceptron-classifier-2.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/Perceptron-classifier-2.jpg" alt="Perception_classification_process" class="lazyload" data-proofer-ignore></a></p>

<p>最终得到参数为</p>

\[\boldsymbol{W} = [1,1]^\top,\; b = -3\]

<p>此时分类超平面为 $x^{(1)}+x^{(2)}-3 = 0$, 感知机模型为 $g(\boldsymbol{x})=f(x^{(1)}+x^{(2)}-3)$，如图</p>

<p><a href="/assets/img/postsimg/20250527/Perceptron-classifier-3.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/Perceptron-classifier-3.jpg" alt="Perception_classifier-result" class="lazyload" data-proofer-ignore></a></p>

<p>注意到，选取误分类点时不同的选择会导致最终得到的分类面不同，如在上述计算中误分类点时：</p>

<ul>
  <li>先后取 $x_1,x_3,x_3,x_3,x_1,x_3,x_3$，分离超平面为红色实线。</li>
  <li>先后取 $x_1,x_3,x_3,x_3,x_2,x_3,x_3,x_3,x_1,x_3,x_3$，分离超平面为绿色实线。</li>
</ul>

<p><a href="/assets/img/postsimg/20250527/Perceptron-classifier-4.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/Perceptron-classifier-4.jpg" alt="Perception_classifier-result" class="lazyload" data-proofer-ignore></a></p>

<hr />

<h3 id="22-多层感知机"><span class="me-2">2.2. 多层感知机</span><a href="#22-多层感知机" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>在前面介绍的单层感知机中，采用阶跃函数只能更新「一层」神经元的参数，因为多层感知机的权重梯度需要依赖链式法则求解，其中涉及到激活函数的导数，而阶跃函数的激活函数几乎处处为零，梯度无法通过隐藏层传播到更早的层（梯度消失），导致前面的层参数无法更新。</p>

\[f(z) = 
\begin{cases} 
1 &amp; \text{if } z \geq 0, \\ 
0 &amp; \text{if } z &lt; 0. 
\end{cases}, \quad f'(z) = 
\begin{cases} 
\text{未定义} &amp; \text{at } z = 0, \\ 
0 &amp; \text{otherwise}. 
\end{cases}\]

<p>1986 年，David Rumelhart、Geoffrey Hinton 和 Ronald Williams 等科学家提出了误差反向传播算法，解决了具有隐藏层的多层感知机的训练问题。多层感知机使用 Sigmoid 函数替代了阶跃函数作为激活函数。从此，神经网络研究掀起第二次浪潮。</p>

<p>多层感知机是具有一个或多个隐藏层的神经网络模型，具有学习复杂非线性关系的能力。多层感知机包含三个层级：输入层、隐藏层、输出层。其中，输入层的节点严格来说并不是神经元，因为它们没有参数，也没有激活函数；隐藏层是多层感知机的核心组成部分，可以有一个或多个隐藏层，且每个隐藏层包含多个节点（神经元），这些节点通过权重与输入层或前一隐藏层的节点相连。隐藏层的引入使得网络深度增加，增强了网络的学习能力和非线性模式表达能力。</p>

<h4 id="221-网络结构"><span class="me-2">2.2.1. 网络结构</span><a href="#221-网络结构" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>多层感知机是具有一个或多个隐藏层的神经网络模型，具有学习复杂非线性关系的能力。下图所示为多层感知机的示例，示例中的多层感知机包含三个层级：输入层、隐藏层、输出层。其中：</p>

<ul>
  <li>输入层的节点严格来说并不是神经元，因为它们没有参数，也没有激活函数；</li>
  <li>隐藏层是多层感知机的核心组成部分，可以有一个或多个隐藏层，且每个隐藏层包含多个节点（神经元），这些节点通过权重与输入层或前一隐藏层的节点相连。隐藏层的引入使得网络深度增加，增强了网络的学习能力和非线性模式表达能力；</li>
  <li>输出层最可以看作是最后一层隐藏层，但其输出需要与样本标签进行比较，计算损失函数并通过反向传播来更新神经网络的参数。</li>
</ul>

<p><a href="/assets/img/postsimg/20250527/dnn.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/dnn.jpg" alt="DNN" class="lazyload" data-proofer-ignore></a></p>

<p>注意到，多层感知机的每一层神经元均与前一层所有神经元都进行连接，因此多层感知机又被称为「<strong>全连接神经网络</strong>」。又因为多层感知机得以实现的前提是采用反向传播（BP）算法，因此又被称为「<strong>BP 神经网络</strong>」。</p>

<h4 id="222-信号前向推理"><span class="me-2">2.2.2. 信号前向推理</span><a href="#222-信号前向推理" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>神经网络的前向推理过程就是从输入样本特征开始，逐一套用每个神经元的数学模型，一步步往前递推得到输出层的神经网络输出结果的过程。以第 $L_2$ 层隐层的第 $q$ 个神经元为例：</p>

\[x_2^{q} = f(\sum_{i=1}^p w_2^{q,i} x_i + b_2^i)\]

<p>写成矩阵形式有</p>

\[\boldsymbol{x}_2 = f(\boldsymbol{W}_2 \boldsymbol{x}_1 + \boldsymbol{b}_2)\]

<p>以此类推，整个神经网络的前向推理过程可以写成</p>

\[\boldsymbol{x}_l = f(\boldsymbol{W}_l \boldsymbol{x}_{l-1} + \boldsymbol{b}_l)\]

<p>其中，权重矩阵的维度为</p>

\[\boldsymbol{W}\in \mathbb{R}^{n^l\times n^{l-1}}\]

<p>其中 $n^{l}$ 表示第 $l$ 层神经元的个数，$n^l$ 表示第 $l$ 层神经元的个数。</p>

<h4 id="223-误差反向传播"><span class="me-2">2.2.3. 误差反向传播</span><a href="#223-误差反向传播" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>对于神经网络的每一层神经元，需要采用链式求导法则得到损失函数对各层神经元权重、阈值参数的梯度。这个过程就是「<strong>反向传播</strong>」（Back Propagation，BP）。为了推导简便，约定各个符号如下图所示：</p>

<p><a href="/assets/img/postsimg/20250527/dnn-1.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/dnn-1.jpg" alt="neuron-model" class="lazyload" data-proofer-ignore></a></p>

<p>则前向传播过程改写为：</p>

\[\begin{aligned}
\boldsymbol{z}^l &amp;= \boldsymbol{W}^l \boldsymbol{a}^{l-1} + \boldsymbol{b}^l\\
\boldsymbol{a}^l &amp;= f(\boldsymbol{z}^l)
\end{aligned}\]

<h5 id="2231-损失函数"><span class="me-2">2.2.3.1. 损失函数</span><a href="#2231-损失函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>一个多层感知机或者一个前馈神经网络可以看作是一个复杂的非线性多元函数。在进行网络参数优化前，我们需要设置一个<strong>衡量误差的量化函数</strong>，该函数除了用于评判当前网络对于目标任务的性能外，其最重要的功能就是指导网络参数的优化方向及优化量，这个量化函数就是我们常说的「损失函数」。</p>

<p>神经网络的训练目标就是通过调整权重和偏置来最小化损失函数。</p>

<ul>
  <li>
    <p>对于 <u>回归任务</u>，希望在 $n$ 个样本点上的预测输出 $a^L_i$ 与真实标签 $y_i$ 尽可能接近（$i=1,2,\cdots,n$），采用「均方误差」（Mean Square Error，MSE）来衡量相似性，即需要最小化</p>

\[L = \frac{1}{n}\sum_{i=1}^n\Vert a^L_i - y_i \Vert_2^2\\\]
  </li>
  <li>
    <p>对于 $k$ <u>分类任务</u>，设第 $i$ 个样本的标签向量 $\boldsymbol{y}_i = [y_i^{1},y_i^{2},\cdots, y_i^k]^\top$（一般为独热编码），那么我们希望神经网络的输出 $\boldsymbol{a}^L_i$ 与标签 $\boldsymbol{y}_i$ 尽可能的相似，采用 <strong>CE</strong> （交叉熵）来衡量相似性，即需要最小化</p>

\[L = -\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^k y_i^j \ln a^{L,j}_i\]
  </li>
</ul>

<p>上述损失函数是批量梯度下降法（BGD）或小批量梯度下降法（MBGD）的损失函数。后文为了简化起见，均采用随机梯度下降法（SGD），因此不考虑多个样本的损失函数求和。关于梯度下降法的介绍，详见 <a href="../reinforcement-learning-Temporal-Differences/#235-bgdmbgd-和-sgd">此处</a> 。</p>

<h5 id="2232-反向传播"><span class="me-2">2.2.3.2. 反向传播</span><a href="#2232-反向传播" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>由于神经网络及其损失函数构成了一个非常复杂的多元函数，求解其局部最小值点的解析解在绝大多数情况下是不可行的，需采用数值优化方法来寻找其局部极小值点，这里要介绍的 <strong>误差</strong>「<strong>反向传播算法</strong>」（BP算法）就是其中之一，其核心是采用「梯度下降法」来更新网络参数，从而达到网络参数的优化。而运行梯度下降算法，就需要计算出损失函数关于网络中各个参数（权重和偏置）的偏导数，这就需要借助「链式法则」来实现：</p>

\[\frac{\partial L}{\partial W} =  \frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial W}\]

<p>这里的「<strong><font color="blue">误差</font></strong>」是一个新的概念 $\delta$，其定义为损失函数 $L$ 关于某层神经元预激活值 ${z}^L$ 的梯度，可简要表述如下：</p>

\[\frac{\partial L}{\partial W} =  \underbrace{\frac{\partial L}{\partial a} \frac{\partial a}{\partial z}}_{\delta} \frac{\partial z}{\partial W} \quad\Rightarrow \quad \delta = \frac{\partial L}{\partial z}\]

<p>引入误差不仅能简化反向传播过程的表达式，还可以将其写为递推形式（参见后文用 $\delta^{l+1}$ 来求解 $\delta^l$）。</p>

<p>根据前面关于激活函数的讨论可知，多层感知机的隐层和输出层的激活函数是不同的，因此反向传播过程也需要针对输出层和隐层分别讨论。</p>

<p>==================== <strong>输出层误差</strong> ====================</p>

<p>在计算输出层误差时，需要进一步针对回归问题和分类问题分别讨论。</p>

<ul>
  <li>
    <p><u>回归问题</u></p>

    <p><a href="/assets/img/postsimg/20250527/output-layer-regression.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/output-layer-regression.jpg" alt="回归问题输出层结构" class="lazyload" data-proofer-ignore></a></p>

    <p>采用「MSE + 恒等函数$f$」组合的输出层 $L$，有</p>

\[{a}^{L} = {z}^L\\\]

    <p>损失函数为</p>

\[L = \Vert a^L - y \Vert_2^2\]

    <p>输出层 $L$ 的误差为（忽略常数系数）</p>

\[\delta^{L} \triangleq \frac{\partial L}{\partial {z}^L} = \frac{\partial L}{\partial {a}^L} = \textcolor{red}{a^L-y}\]
  </li>
  <li>
    <p><u>分类问题</u></p>

    <p><a href="/assets/img/postsimg/20250527/output-layer-classification.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/output-layer-classification.jpg" alt="分类问题输出层结构" class="lazyload" data-proofer-ignore></a></p>

    <p>采用「CE + softmax函数」组合的输出层，损失函数为</p>

\[L = -\sum_{j=1}^k y_j \ln a^L_j\]

    <p>其中第 $j$ 个神经元的输出为</p>

\[a^L_j = \text{softmax}(z^L) = \frac{e^{z_j^L}}{\sum_{i=1}^k e^{z^L_i}}\]

    <p>则第 $j$ 个神经元的误差为</p>

\[\delta^{L}_j \triangleq \frac{\partial L}{\partial z_j^L} = \sum_{i=1}^k\frac{\partial L}{\partial a_i^L} \frac{\partial a_i^L}{\partial z_j^L}\]

    <p>注意此处需要对 $i$ 进行<strong>遍历</strong>，因为根据 softmax 函数定义，每个输出 $a_i$ 都和第 $j$ 个神经元预激活值 $z_j$ 有关（分母求和部分）。</p>

    <p>第一项</p>

\[\frac{\partial L}{\partial a^L_i} = -\frac{y_i}{a^L_i}\]

    <p>第二项（也即 softmax 函数的导数），又因为 softmax 分子和分母都含有预激活值 $z_j^L$，需要根据其下标取值分两种情况讨论：</p>

    <p><strong>（a）</strong> 当 $i=j$ 时</p>

\[\frac{\partial a^L_i}{\partial z^L_j} = a^L_i(1-a^L_i)\]

    <details>
      <summary>
<font color="blue">展开详细推导</font>
</summary>

      <blockquote>
        <p>为了表述方便，省去上标 $L$</p>

\[\begin{aligned}
  \frac{\partial a_i}{\partial z_j} &amp;= \frac{\partial a_i}{\partial z_i} \\
  &amp;= \frac{\partial}{\partial z_i} \left( \frac{e^{z_i}}{\sum_k e^{z_k}} \right) \\
  &amp;= \frac{(e^{z_i})' (\sum_k e^{z_k}) - e^{z_i} (\sum_k e^{z_k})'}{(\sum_k e^{z_k})^2} \\
  &amp;= \frac{e^{z_i} \cdot (\sum_k e^{z_k}) - e^{z_i} \cdot e^{z_i}}{(\sum_k e^{z_k})^2} \\
  &amp;= \frac{e^{z_i}}{\sum_k e^{z_k}} - \frac{e^{z_i}}{\sum_k e^{z_k}} \cdot \frac{e^{z_i}}{\sum_k e^{z_k}} \\
  &amp;= a_i - a_i \cdot a_i \\
  &amp;= a_i (1 - a_i)
  \end{aligned}\]
      </blockquote>

    </details>

    <p><strong>（b）</strong> 当 $i\neq j$ 时</p>

\[\frac{\partial a^L_i}{\partial z^L_j} = -a^L_i a^L_j\]

    <details>
      <summary>
<font color="blue">展开详细推导</font>
</summary>

      <blockquote>
        <p>为了表述方便，省去上标 $L$</p>

\[\begin{aligned}
  \frac{\partial a_i}{\partial z_j} &amp;= \frac{\partial}{\partial z_j} \left( \frac{e^{z_i}}{\sum_k e^{z_k}} \right) \\
  &amp;= \frac{(e^{z_i})' (\sum_k e^{z_k}) - e^{z_i} (\sum_k e^{z_k})'}{(\sum_k e^{z_k})^2} \\
  &amp;= \frac{0 \cdot (\sum_k e^{z_k}) - e^{z_i} \cdot e^{z_j}}{(\sum_k e^{z_k})^2} \\
  &amp;= \frac{-e^{z_i} \cdot e^{z_j}}{(\sum_k e^{z_k})^2} \\
  &amp;= -\frac{e^{z_i}}{\sum_k e^{z_k}} \cdot \frac{e^{z_j}}{\sum_k e^{z_k}} \\
  &amp;= -a_i \cdot a_j
  \end{aligned}\]
      </blockquote>

    </details>

    <p>合并上述结果，误差为 $\textcolor{red}{\delta_j^L = a_j^L-y_j}$。</p>

    <details>
      <summary>
<font color="blue">展开详细推导</font>
</summary>

      <blockquote>
\[\begin{aligned}
  \delta^{L}_j &amp;= -\frac{y_j}{a^L_i} \cdot a^L_i(1-a^L_i) - \sum_{i\neq j} \frac{y_i}{a^L_i} (-a^L_i a^L_j)\\
  &amp;= - y_j+y_ja_i^L+\sum_{i\neq j} y_ia_j^L\\
  &amp;= -y_j+\sum_{i} y_ia_j^L\\
  &amp;=-y_j + a_j^L\sum_i y_i
  \end{aligned}\]

        <p>注意到 $y_i$ 是独热编码的概率值，其和为 $1$，因此有</p>

\[\delta^{L}_j = -y_j + a_j^L\]

        <p>又因为 $y_i$ 是独热编码的概率值，只有一个位置（假设是第 $i$ 个位置）是 $1$, 其他位置都是 $0$，因此对于 $k$ 分类任务，输出层误差可写为</p>

\[\boldsymbol{\delta}^L = \begin{bmatrix}
    a_1^L\\
    a_2^L\\
    \vdots\\
    a_i^L - 1\\
    \vdots\\
    a_k^L
  \end{bmatrix}\]

        <p>结果十分优美。这也是 PyTorch 和 TensorFlow 等深度学习框架中，将交叉熵损失函数和 softmax 激活函数合并为一个函数（即 <code class="language-plaintext highlighter-rouge">tf.nn.softmax_cross_entropy_with_logits</code> 和 <code class="language-plaintext highlighter-rouge">torch.nn.CrossEntropyLoss</code>）一起求导的原因。</p>
      </blockquote>

    </details>

    <p>则权重的梯度为</p>

\[\frac{\partial L}{\partial \boldsymbol{W}^L} = \frac{\partial L}{\partial \boldsymbol{z}^L} \cdot \frac{\partial \boldsymbol{z}^L}{\partial \boldsymbol{W}^L} = \boldsymbol{\delta}^{L} \cdot (\boldsymbol{a}^{L-1})^\top\quad\in\mathbb{R}^{n^L\times n^{L-1}}\]

    <p>偏置的梯度为</p>

\[\frac{\partial L}{\partial \boldsymbol{b}^L} = \frac{\partial L}{\partial \boldsymbol{z}^L} \cdot \frac{\partial \boldsymbol{z}^L}{\partial \boldsymbol{b}^L} = \boldsymbol{\delta}^{L}\quad\in\mathbb{R}^{n^L\times 1}\]
  </li>
</ul>

<p>==================== <strong>隐层误差</strong> ====================</p>

<p>对于隐层 $l$，有</p>

\[\begin{aligned}
\boldsymbol{a}^{l} &amp;= f(\boldsymbol{z}^l)\\
\boldsymbol{z}^{l} &amp;= \boldsymbol{W}^l \boldsymbol{a}^{l-1} + \boldsymbol{b}^l
\end{aligned}\]

<p>定义隐层 $l$ 的误差为损失函数关于隐层预激活值 $\boldsymbol{z}^l$ 的梯度，根据链式法则有</p>

\[\boldsymbol{\delta}^l \triangleq \frac{\partial L}{\partial \boldsymbol{z}^l} = \frac{\partial L}{\partial \boldsymbol{z}^{l+1}} \cdot \frac{\partial \boldsymbol{z}^{l+1}}{\partial \boldsymbol{a}^{l}}\cdot \frac{\partial \boldsymbol{a}^{l}}{\partial \boldsymbol{z}^l}\quad\in \mathbb{R}^{n^{l}\times 1}\]

<blockquote>
  <p>注意，这里是矩阵形式的链式法则，如果写为标量形式，对于第 $j$ 个神经元有</p>

\[\delta_j^l = \frac{\partial L}{\partial z_j^l} = \sum_{j=1}^{n_{l+1}}\left(\frac{\partial L}{\partial z_j^{l+1}} \cdot \frac{\partial z_j^{l+1}}{\partial a_j^{l}}\cdot \frac{\partial a_j^{l}}{\partial z_j^l}\right)\]

  <p>因为隐层 $l$ 每一个神经元都与后一层是全连接的，需要接收后一层 $l+1$ 层所有神经元反向传播来的误差，因此需要对所有误差（梯度）进行求和。</p>
</blockquote>

<p>第一项</p>

\[\frac{\partial L}{\partial \boldsymbol{z}^{l+1}} = \boldsymbol{\delta}^{l+1}\quad\in\mathbb{R}^{n^{l+1}\times 1}\]

<p>第二项</p>

\[\boldsymbol{z}^{l+1} = \boldsymbol{W}^{l+1} \boldsymbol{a}^l + \boldsymbol{b}^{l+1}\quad\Rightarrow \quad\frac{\partial \boldsymbol{z}^{l+1}}{\partial \boldsymbol{a}^l} = \boldsymbol{W}^{l+1}\in\mathbb{R}^{n^{l+1}\times n^l}\]

<p>最后项</p>

\[\boldsymbol{a}^{l} = f(\boldsymbol{z}^{l}) \quad\Rightarrow \quad\frac{\partial \boldsymbol{a}^{l}}{\partial \boldsymbol{z}^{l}} = f'(\boldsymbol{z}^{l})\in\mathbb{R}^{n^{l}\times 1}\]

<p>最终得到（权重矩阵转置是因为需要维度匹配）</p>

\[\textcolor{red}{\boldsymbol{\delta}^{l} = (\boldsymbol{W}^{l+1})^\top\boldsymbol{\delta}^{l+1}  \odot f'(\boldsymbol{z}^{l})}\]

<p>其中 $\odot$ 表示 Hadamard 积，即逐元素相乘。</p>

<blockquote>
  <p>如果展开为标量形式，对于第 $j$ 个神经元的误差为</p>

\[\delta_j^{l} = \left(\sum_{k}W_{k,j}^{l+1}\delta_k^{l+1}\right)f'(z_j^{l})\]
</blockquote>

<p>则权重的梯度为</p>

\[\begin{aligned}
\frac{\partial L}{\partial \boldsymbol{W}^l} &amp;= \frac{\partial L}{\partial \boldsymbol{z}^l} \cdot \frac{\partial \boldsymbol{z}^l}{\partial \boldsymbol{W}^l}=  \boldsymbol{\delta}^{l} \cdot (\boldsymbol{a}^{l-1})^\top
\end{aligned}\]

<p>偏置的梯度为</p>

\[\frac{\partial L}{\partial \boldsymbol{b}^l} = \frac{\partial L}{\partial \boldsymbol{z}^l} \cdot \frac{\partial \boldsymbol{z}^l}{\partial \boldsymbol{b}^l}=  \boldsymbol{\delta}^{l} \cdot 1 = \boldsymbol{\delta}^{l}\]

<p>========================================</p>

<p>综上，输出误差（由损失函数刻画）通过隐含层向输入层逐层反向传播，并将误差分摊给各层所有节点（获得所有层的误差估计），采用梯度下降的方式，按误差函数的负梯度方向修改各节点连接权重和偏置。</p>

\[\begin{aligned}
  \boldsymbol{W} &amp;\leftarrow  \boldsymbol{W}-\alpha\frac{\partial L}{\partial \boldsymbol{W}}\\
  \boldsymbol{b} &amp;\leftarrow  \boldsymbol{b}-\alpha\frac{\partial L}{\partial \boldsymbol{b}}
\end{aligned}\]

<blockquote>
  <p>关于反向传播的标量推导可参考：https://zhuanlan.zhihu.com/p/96046514</p>
</blockquote>

<h5 id="2233-梯度消失"><span class="me-2">2.2.3.3. 梯度消失</span><a href="#2233-梯度消失" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>分析权重的梯度</p>

\[\begin{aligned}
\frac{\partial L}{\partial \boldsymbol{W}^l} &amp;= \textcolor{red}{\delta^{l}} \cdot (\boldsymbol{a}^{l-1})^\top\\
&amp;=\textcolor{red}{(\boldsymbol{W}^{l+1})^\top\delta^{l+1} \odot f'(\boldsymbol{z}^{l})}\cdot (\boldsymbol{a}^{l-1})^\top\\
&amp;= \textcolor{red}{(\boldsymbol{W}^{l+1})^\top}
\textcolor{blue}{(\boldsymbol{W}^{l+2})^\top\delta^{l+2}  \odot f'(\boldsymbol{z}^{l+1})}
\textcolor{red}{\odot f'(\boldsymbol{z}^{l})}\cdot (\boldsymbol{a}^{l-1})^\top\\
&amp;= \underbrace{\textcolor{red}{(\boldsymbol{W}^{l+1})^\top}
\textcolor{blue}{(\boldsymbol{W}^{l+2})^\top \cdots}}_{\text{weights}}
\cdot
\underbrace{\cdots \textcolor{blue}{\odot f'(\boldsymbol{z}^{l+1})}
\textcolor{red}{\odot f'(\boldsymbol{z}^{l})}}_{\text{activation derivative}}
\cdot
\underbrace{(\boldsymbol{a}^{l-1})^\top}_{\text{neuron output}}
\end{aligned}\]

<p>可以看出，导致梯度趋近于零（梯度消失）可能存在以下原因：</p>

<ul>
  <li>（1）激活函数导数连乘
    <ul>
      <li>在使用 Sigmoid 或 Tanh 激活函数时，其导数是小于零的数。</li>
      <li>当大量小于或趋近于 $0$ 的激活函数导数相乘时，会导致梯度在反向传播过程中迅速减小，从而使得网络中较早层的权重更新非常缓慢或几乎不更新。</li>
      <li>这种情况随着网络层数的增加而更加严重。</li>
    </ul>
  </li>
  <li>（2） 权重设置不合理
    <ul>
      <li>权重初始化过小时，权重连乘会导致梯度很小；</li>
      <li>权重初始化过小（如 $W\sim N(0,0.01)$）时，会导致网络深层输出的均值不变但方差急剧缩小，$a^l$ 趋于零附近；</li>
      <li>权重初始化过大（如 $W\sim N(0,1)$）时，会导致神经网络每层输出均值和方差虽然正常，但几乎所有的值集中在 $\pm 1$ 附近，表明神经元饱和，此时激活函数导数几乎为零，导致权重几乎不更新。</li>
    </ul>
  </li>
  <li>（3） 权重初始化异常大时还会导致「梯度爆炸」。</li>
</ul>

<p>权重初始化的两种情况，对应的网络每层输出直方图如下图所示：</p>

<p><a href="/assets/img/postsimg/20250527/gradient-vanish.png" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/gradient-vanish.png" alt="权重初始化不合理导致梯度消失" class="lazyload" data-proofer-ignore></a></p>

<p>可以通过如下方式缓解梯度消失：</p>

<ul>
  <li>使用合适的激活函数：
    <ul>
      <li>ReLU（Rectified Linear Unit）：导数为0或1，避免连乘导致的梯度消失。</li>
      <li>Leaky ReLU、Parametric ReLU：解决ReLU的“死亡”问题。</li>
    </ul>
  </li>
  <li>权重初始化：合理的初始化（如Xavier、He初始化）可以避免初始权重过小。</li>
  <li>Batch Normalization：通过规范化每一层的输入，使得激活函数的输入分布在较稳定的范围内，避免梯度消失。</li>
  <li>残差连接（ResNet）：引入跳跃连接（skip connections），使得梯度可以直接绕过某些层传播，缓解梯度消失。</li>
</ul>

<h5 id="2234-梯度爆炸"><span class="me-2">2.2.3.4. 梯度爆炸</span><a href="#2234-梯度爆炸" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5>

<p>与梯度消失相反，梯度爆炸指的是梯度在反向传播过程中指数级增长，导致参数更新过大，模型无法收敛。这主要是由于：</p>

<ol>
  <li>权重矩阵的范数较大：如果权重矩阵的元素较大，其转置的乘积会导致梯度迅速增大；</li>
  <li>输入数据范围过大（如遇到野值或某一维特征远大于其它特征），会逐层累计导致爆炸；</li>
  <li>激活函数：一般出现在使用 ReLU 函数时，因为其在 $x&gt;0$ 部分导数为 $1$，无法缓解前两个梯度爆炸的原因。</li>
</ol>

<p>可以通过如下方式缓解梯度爆炸：</p>

<ul>
  <li>梯度裁剪（Gradient Clipping）：设定梯度的阈值，超过时进行缩放，防止梯度爆炸。</li>
  <li>权重正则化：L1或L2正则化，限制权重的范数，防止其过大。</li>
  <li>使用更小的学习率：降低学习率可以减少参数更新的幅度。</li>
  <li>权重初始化：合理的初始化（如Xavier、He初始化）可以避免初始权重过大。</li>
</ul>

<h3 id="23-自组织神经网络"><span class="me-2">2.3. 自组织神经网络</span><a href="#23-自组织神经网络" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="231-自组织竞争神经网络"><span class="me-2">2.3.1. 自组织竞争神经网络</span><a href="#231-自组织竞争神经网络" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>自组织竞争神经网络（Self-Organizing Competitive Neural Networks）是一类「无监督」学习神经网络模型，能够通过竞争机制自动发现输入数据中的模式或特征。自组织神经网络是早期无监督学习网络模型的代表，模拟了生物神经系统侧抑制现象。</p>

<blockquote>
  <p>在生物神经系统中，存在着一种「侧抑制现象」，即一个神经细胞兴奋以后，会对周围其他神经细胞产生抑制作用。这种抑制作用会使神经细胞之间出现竞争，竞争获胜的神经细胞兴奋，失败的神经细胞被抑制。</p>
</blockquote>

<p>与感知机相比，自组织竞争神经网络的单层网络又被称为竞争层，如图所示：</p>

<p><a href="/assets/img/postsimg/20250527/socnn.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/socnn.jpg" alt="SOCNN" class="lazyload" data-proofer-ignore></a></p>

<p>输入层：接收外界信息，将输入模式向竞争层传递。竞争层对输入模型进行分析比较，寻找规律，并进行归类处理。竞争层神经元之间相互竞争激活，在任意时刻只有一个神经元被激活。被激活的神经元被称为胜利者神经元（winner-takes-all neuron），而其它神经元的状态被抑制，故称为 Winner Take All（赢者通吃，简称 WTA）。</p>

<p>WTA 的具体步骤如下：</p>

<ul>
  <li>向量归一化：对网络当前输入向量 $X$ 和竞争层中各神经元对应的权重向量 $W_j$（对应 $j$ 神经元）全部进行归一化，使得二者的模为 $1$；</li>
  <li>寻找获胜神经元：
    <ul>
      <li>计算输入向量与每个输出神经元权重向量的相似度，如采用欧式距离；</li>
      <li>选择相似度最大的神经元为获胜神经元；
\(c = \arg\min_j(||X - W_j||^2)\)</li>
    </ul>
  </li>
  <li>更新获胜神经元权重，其他神经元权重保持不变
    <ul>
      <li>$W_c = W_c + \eta(X-W_c) = \eta X + (1 - \eta) W_c$;</li>
      <li>学习率 $0&lt;\eta&lt;1$ 随着迭代的进行逐渐减小；</li>
    </ul>
  </li>
  <li>将更新后的权重向量重新归一化（保持单位长度）；</li>
  <li>重复上述步骤，直到权重变化小于阈值或达到最大迭代次数。</li>
</ul>

<p>WTA竞争学习是许多自组织神经网络的基础，通过这种简单的竞争机制，网络能够自动发现数据中的主要模式。</p>

<details>
  <summary>
<font color="blue">思考：最终竞争层权重收敛到什么位置？</font>
</summary>

  <p>在反复的竞争学习中，竞争层的各神经元所对应的权向量被逐渐调整为输入样本控件的<strong>聚类中心</strong>。</p>

</details>

<h4 id="232-自组织映射神经网络"><span class="me-2">2.3.2. 自组织映射神经网络</span><a href="#232-自组织映射神经网络" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>自组织特征映射网络（Self-Organizing Feature Maps，SOFM）又称自组织映射网络（SOM），由 Kohonen 于 1981 年提出。与竞争性网络非常相似，SOM 也是单层网络结构，神经元都具有竞争性，都采用无监督学习方式，但在输出层引入网络的「<strong>拓扑结构</strong>」，可以更好地模拟生物学中的侧抑制现象，如图所示：</p>

<p><a href="/assets/img/postsimg/20250527/som.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/som.jpg" alt="SOM" class="lazyload" data-proofer-ignore></a></p>

<ul>
  <li>假设一个输入样本为 $x_i$ 是一个 $n$ 维向量，则输入层神经元个数为 $n$ 个；</li>
  <li>输入神经元和输出神经元也通过权值相连；</li>
  <li>输出神经元一般以二维拓扑结构排列，神经元数量和训练集样本的类别数相关，比如要分成 $4$ 类，那可以将输出层（竞争层）设计为 $2\times2$。若不清楚类别数，尽可能地设定较多的节点数，以便较好地映射样本的拓扑结构，如果分类过细再适当减少输出节点；</li>
  <li>一般而言输出层神经元个数小于样本特征维度，从而达到降维的目的。</li>
</ul>

<p>SOM 的训练过程与 自组织竞争神经网络类似，但也有不同：</p>

<ol>
  <li>初始化权值矩阵 $\boldsymbol{w}$，即每个神经元的权值向量；</li>
  <li>输入样本 $\boldsymbol{x}$，计算 $\boldsymbol{x}$ 与每个输出层神经元权重 $\boldsymbol{w}_i$ 的距离，并找到距离最小的神经元 $c_i$，称之为最佳匹配单元（Best Matching Unit，BMU）；</li>
  <li>最佳匹配单元<strong>及其近邻神经元</strong>的权向量将被调整。</li>
</ol>

<p>上述步骤中，如何确定邻近神经元以及如何计算邻域神经元更新权重的方式是 SOM 的核心。</p>

<ul>
  <li><strong>邻域距离度量</strong></li>
</ul>

<p>对于最佳匹配单元 BMU，假设其网格坐标为 $(c_i,c_j)$，其他神经元$(c_i^\prime,c_j^\prime)$与其的网格距离可以采用：</p>

<ol>
  <li>欧式距离：$d = \sqrt{(c_i - c_i^\prime)^2 + (c_j - c_j^\prime)^2}$</li>
  <li>曼哈顿距离：$d = \vert c_i - c_i^\prime\vert + \vert c_j - c_j^\prime \vert$</li>
</ol>

<ul>
  <li><strong>邻域影响程度</strong></li>
</ul>

<p>确定距离后，就需要根据距离来计算邻域的影响程度，用以确定邻域神经元权向量更新的幅度（实现距离近的神经元被同步激活的程度高，距离远的激活程度低），可采用：</p>

<ol>
  <li>高斯邻域函数：$h(d) = e^{-d^2/2\sigma^2}$，其中 $\sigma$ 是邻域半径参数，随时间衰减；</li>
  <li>墨西哥草帽函数：$h(d) = (1 - (d/\sigma)^2) \cdot e^{-d^2/(2\sigma^2)}$，其中 $\sigma$ 是邻域半径参数，随时间衰减；</li>
  <li>大礼帽函数：如图；</li>
  <li>矩形邻域函数（厨师帽函数）：$h(d) = 1 \; \text{if}\; d &lt; R\; \text{else}\; 0$，其中 $R$ 是邻域半径参数，随时间衰减；</li>
</ol>

<p><a href="/assets/img/postsimg/20250527/neighbor-functions.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/neighbor-functions.jpg" alt="邻域函数" class="lazyload" data-proofer-ignore></a></p>

<ul>
  <li><strong>权重更新规则</strong></li>
</ul>

<p>和自组织竞争神经网络类似，唯一区别在于邻域神经元需要额外考虑与距离有关的影响：</p>

\[\begin{aligned}
  W_{ij} &amp;= W_{ij} + \eta \cdot h(d_{ij}) \cdot (x - W_{ij}) \\
\end{aligned}\]

<p>在训练开始时，学习率 $\eta$ 可以选取较大的值，之后以较快的速度下降，这样有利于很快地捕捉到输入向量的大致结构，然后学习率在较小的值上缓降至0值，可以精细地调整权值，使得符合输入空间的样本分布结构。常见的学习率函数为</p>

\[\eta(t) = \eta(t)e^{-t}\]

<p>其中 $t$ 为迭代次数。</p>

<p><strong>思考：SOM 与 K-均值算法对比？</strong></p>

<blockquote>
  <p>(1) K-均值算法为每个输入数据找到一个最相似的类后，只更新这个类的参数（重新计算该类的聚类中心）；自组织映射神经网络则会更新邻近节点。</p>

  <p>(2) K-均值算法受噪声影响较大，而自组织映射神经网络的准确性可能会比 K-均值算法低（因为也更新了邻近节点，噪声点所对应的神经元节点同样被更新，加大了噪声干扰）</p>
</blockquote>

<p>SOM 尤为适合高维数据的低维（二维）可视化。</p>

<p>下图使用两个热图说明平均教育水平和失业率之间的关系</p>

<p><a href="/assets/img/postsimg/20250527/som-0.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/som-0.jpg" alt="SOM可视化0" class="lazyload" data-proofer-ignore></a></p>

<p>输出层神经元的权重向量 代表/相似于 映射到该节点的样本。通过可视化整个图上的权重向量，可以看到样本和变量分布中的模型。权重向量的默认可视化是一个“扇形图”，其中为每个节点显示了权重向量中每个变量的大小的各个扇形表示。</p>

<p><a href="/assets/img/postsimg/20250527/som-1.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/som-1.jpg" alt="SOM可视化1" class="lazyload" data-proofer-ignore></a></p>

<p>某个五分类任务中的 SOM 可视化结果如下：</p>

<p><a href="/assets/img/postsimg/20250527/som-2.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/som-2.jpg" alt="SOM可视化2" class="lazyload" data-proofer-ignore></a></p>

<p>观察如上结果，发现其采用六边形邻域，好处在于邻域神经元的个数更多（$6-12-\cdots$），性能更好。</p>

<p><a href="/assets/img/postsimg/20250527/som-neighbor.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20250527/som-neighbor.jpg" alt="SOM六边形邻域" class="lazyload" data-proofer-ignore></a></p>

<p><strong>思考：用曼哈顿距离时，邻域神经元个数是多少？</strong></p>

<blockquote>
  <p>答案：$4-8-\cdots$</p>
</blockquote>

<h2 id="3-参考文献"><span class="me-2">3. 参考文献</span><a href="#3-参考文献" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>[1] <a href="https://zhuanlan.zhihu.com/p/96046514">解读反向传播算法（图与公式结合）</a></p>

<p>[2] <a href="https://tecdat.cn/r%e8%af%ad%e8%a8%80%e4%bd%bf%e7%94%a8%e8%87%aa%e7%bb%84%e7%bb%87%e6%98%a0%e5%b0%84%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%ef%bc%88som%ef%bc%89%e8%bf%9b%e8%a1%8c%e5%ae%a2%e6%88%b7%e7%bb%86%e5%88%86/">R语言使用自组织映射神经网络（SOM）进行客户细分</a></p>

</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw me-1"></i>
    
      <a href='/categories/academic/'>Academic</a>,
      <a href='/categories/knowledge/'>Knowledge</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw me-1"></i>
      
      <a href="/tags/pattern-recognition/"
          class="post-tag no-text-decoration" >pattern recognition</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        本文由作者按照 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         进行授权

      
    </div>

    <!-- Post sharing snippet -->

<div class="share-wrapper">
  <span class="share-label text-muted me-1">分享</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%EF%BC%88%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89%20-%20SIRLIS&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FPattern-Recognition-DNN%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%EF%BC%88%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89%20-%20SIRLIS&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FPattern-Recognition-DNN%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FPattern-Recognition-DNN%2F&text=%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%EF%BC%88%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89%20-%20SIRLIS"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <i
      id="copy-link"
      class="fa-fw fas fa-link small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="分享链接"
      data-title-succeed="链接已复制！"
    >
    </i>
  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
      
    </div>
  </div>
  <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
    <div class="access">
      <!-- Get the last 5 posts from lastmod list. -->














  <div id="access-lastmod" class="post">
    <div class="panel-heading">最近更新</div>
    <ul class="post-content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-DNN/">模式识别（经典神经网络）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Nonlinear-Classifier/">模式识别（非线性分类器）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/artificial-intelligence-search-strategy/">人工智能（搜索策略）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/artificial-intelligence-knowledge-etc/">人工智能（知识表示与推理）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/artificial-intelligence-introduction/">人工智能（绪论）</a>
        </li>
      
    </ul>
  </div>
  <!-- #access-lastmod -->


      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pattern-recognition/">pattern recognition</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial intelligence</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
    </div>
  </div>


    </div>

    
      
      



  <div id="toc-wrapper" class="ps-0 pe-4 mb-5">
    <div class="panel-heading ps-3 pt-2 mb-2">文章内容</div>
    <nav id="toc"></nav>
  </div>


    
  </div>
</div>

<!-- tail -->

  <div class="row">
    <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-3 pe-xl-4 mt-5">
      
        
        <!--
  Recommend the other 3 posts according to the tags and categories of the current post,
  if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->








  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  






<!-- Fill with the other newlest posts -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ms-1" data-toc-skip>
      相关文章
    </h3>
    <div class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        
        
        <div class="col">
          <a href="/posts/Pattern-Recognition-Nonlinear-Classifier/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1743341659"
  data-df="YYYY/MM/DD"
  
>
  2025/03/30
</em>

              <h4 class="pt-0 my-2" data-toc-skip>模式识别（非线性分类器）</h4>
              <div class="text-muted small">
                <p>
                  





                  很多情况下，类别之间的分类边界并不是线性的，一种更好的选择是使用更复杂的非线性函数来描述分类。本文介绍了模式识别常用的非线性分类器，主要包括近邻法分类器（NN、KNN）、支持向量机（SVM）、决策树（DT），最后介绍分类器的集成。本章内容预计花费 4 个课时学习。






  1. 引言
  2. 近邻法分类器（NN / KNN）
    
      2.1. 最近邻法
      2...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/Pattern-Recognition-Feature-Selection-and-Extraction/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1744708549"
  data-df="YYYY/MM/DD"
  
>
  2025/04/15
</em>

              <h4 class="pt-0 my-2" data-toc-skip>模式识别（特征选择与特征提取）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了模式识别中特征的概念，包括颜色特征、形状特征、纹理特征、空间关系特征。然后介绍了常用的特征选择与特征提取方法，包括类别可分性准则、最优搜索、次优搜索、启发式搜索、线性特征提取、非线性特征提取等。






  1. 特征
    
      1.1. 颜色特征
        
          1.1.1. 颜色直方图
          1.1.2. 颜色矩
      ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/Pattern-Recognition-Unsupervised-Classifier/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1747033849"
  data-df="YYYY/MM/DD"
  
>
  2025/05/12
</em>

              <h4 class="pt-0 my-2" data-toc-skip>模式识别（无监督分类器）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了无监督分类器的设计原理和方法。不同于前面章节介绍的分类器，无监督分类器旨在没有类别标签的情况下完成样本的分类。由于样本的类别标签位置，无监督分类器具有一定的学习能力，因此相关分类方法又被称为无监督学习。聚类（Clustering）分析是最典型的无监督学习方法。






  1. 聚类
    
      1.1. 相似性测度
        
          1.1.1....
                </p>
              </div>
            </div>
          </a>
        </div>
      
    </div>
    <!-- .card-deck -->
  </div>
  <!-- #related-posts -->


      
        
        <!-- Navigation buttons at the bottom of the post. -->

<div class="post-navigation d-flex justify-content-between">
  
    <a
      href="/posts/Pattern-Recognition-Unsupervised-Classifier/"
      class="btn btn-outline-primary"
      prompt="上一篇"
    >
      <p>模式识别（无监督分类器）</p>
    </a>
  

  
    <a
      href="/posts/Pattern-Recognition-CNN/"
      class="btn btn-outline-primary"
      prompt="下一篇"
    >
      <p>模式识别（卷积神经网络）</p>
    </a>
  
</div>

      
        
        <!--  The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script src="https://utteranc.es/client.js"
        repo="sirlis/sirlis.github.io"
        issue-term="pathname"
        crossorigin="anonymous"
        async>
</script>

<script type="text/javascript">
  $(function() {
    const origin = "https://utteranc.es";
    const iframe = "iframe.utterances-frame";
    const lightTheme = "github-light";
    const darkTheme = "github-dark";
    let initTheme = lightTheme;

    if ($("html[data-mode=dark]").length > 0
        || ($("html[data-mode]").length == 0
            && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
      initTheme = darkTheme;
    }

    addEventListener("message", (event) => {
      let theme;

      /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */
      if (event.origin === origin) {
        /* page initial */
        theme = initTheme;

      } else if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {
        /* global theme mode changed */
        const mode = event.data.message;
        theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme);

      } else {
        return;
      }

      const message = {
        type: "set-theme",
        theme: theme
      };

      const utterances = document.querySelector(iframe).contentWindow;
      utterances.postMessage(message, origin);
    });

  });
</script>



      
    </div>
  </div>


        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 post-content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pattern-recognition/">pattern recognition</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial intelligence</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>
    </div>

    <!-- The Footer -->

<footer>
  <div class="container px-lg-4">
    <div class="d-flex justify-content-center align-items-center text-muted mx-md-3">
      <p>本站采用 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 主题 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
      </p>

      <p>©
        2025
        <a href="https://github.com/sirlis">sirlis</a>.
        
          <span
            data-bs-toggle="tooltip"
            data-bs-placement="top"
            title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。"
          >保留部分权利。</span>
        
      </p>
    </div>
  </div>
</footer>


    <div id="mask"></div>

    <button id="back-to-top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow">
      <i class="fas fa-angle-up"></i>
    </button>

    
      <div
        id="notification"
        class="toast"
        role="alert"
        aria-live="assertive"
        aria-atomic="true"
        data-bs-animation="true"
        data-bs-autohide="false"
      >
        <div class="toast-header">
          <button
            type="button"
            class="btn-close ms-auto"
            data-bs-dismiss="toast"
            aria-label="Close"
          ></button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="px-2 mb-3">发现新版本的内容。</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            更新
          </button>
        </div>
      </div>
    

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.0/dist/jquery.min.js,npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.7/dayjs.min.js,npm/dayjs@1.11.7/locale/zh.min.js,npm/dayjs@1.11.7/plugin/relativeTime.min.js,npm/dayjs@1.11.7/plugin/localizedFormat.min.js,npm/tocbot@4.21.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '<div class="px-1 px-sm-2 px-lg-4 px-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

