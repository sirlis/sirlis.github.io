<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="zh-CN" 
  
>
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="强化学习（马尔可夫决策过程）" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="本文介绍了强化学习的基本概念和模型，主要包括马尔可夫过程、马尔可夫奖励过程和马尔可夫决策过程。" />
<meta property="og:description" content="本文介绍了强化学习的基本概念和模型，主要包括马尔可夫过程、马尔可夫奖励过程和马尔可夫决策过程。" />
<link rel="canonical" href="http://localhost:4000/posts/reinforcement-learning-markov-process/" />
<meta property="og:url" content="http://localhost:4000/posts/reinforcement-learning-markov-process/" />
<meta property="og:site_name" content="SIRLIS" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-11-09T12:36:19+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="强化学习（马尔可夫决策过程）" />
<meta name="twitter:site" content="@none" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-29T17:49:13+08:00","datePublished":"2022-11-09T12:36:19+08:00","description":"本文介绍了强化学习的基本概念和模型，主要包括马尔可夫过程、马尔可夫奖励过程和马尔可夫决策过程。","headline":"强化学习（马尔可夫决策过程）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/reinforcement-learning-markov-process/"},"url":"http://localhost:4000/posts/reinforcement-learning-markov-process/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>强化学习（马尔可夫决策过程） | SIRLIS
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SIRLIS">
<meta name="application-name" content="SIRLIS">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.0/dist/tocbot.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/head.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <div class="site-title">
      <a href="/">SIRLIS</a>
    </div>
    <div class="site-subtitle fst-italic">分享科研和生活的日常</div>
  </div>
  <!-- .profile-wrapper -->

  <ul class="nav flex-column flex-grow-1 w-100 ps-0">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home"></i>
        <span>首页</span>
      </a>
    </li>
    <!-- the real tabs -->
    
      <li class="nav-item">
        <a href="/categories/" class="nav-link">
          <i class="fa-fw fas fa-stream"></i>
          

          <span>分类</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/tags/" class="nav-link">
          <i class="fa-fw fas fa-tags"></i>
          

          <span>标签</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/archives/" class="nav-link">
          <i class="fa-fw fas fa-archive"></i>
          

          <span>归档</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/about/" class="nav-link">
          <i class="fa-fw fas fa-info-circle"></i>
          

          <span>关于</span>
        </a>
      </li>
      <!-- .nav-item -->
    
  </ul>
  <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/sirlis"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/none"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['lihongjue','buaa.edu.cn'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</div>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container px-xxl-5">
        <!-- The Top Bar -->

<div id="topbar-wrapper">
  <div
    id="topbar"
    class="container d-flex align-items-center justify-content-between h-100"
  >
    <span id="breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                首页
              </a>
            </span>

          
        
          
        
          
            
              <span>强化学习（马尔可夫决策过程）</span>
            

          
        
      
    </span>
    <!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      文章
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="搜索..."
      >
    </span>
    <span id="search-cancel">取消</span>
  </div>
</div>

        











<div class="row">
  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pe-xl-4">
    

    <div class="post px-1 px-md-2">
      

      
        
      
        <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->



  
  

  <!-- CDN URL -->
  

  <!-- Add image path -->
  

  
    
      
      
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  




<!-- return -->




<h1 data-toc-skip>强化学习（马尔可夫决策过程）</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      发表于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1667968579"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2022/11/09
</em>

    </span>

    <!-- lastmod date -->
    
    <span>
      更新于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1743241753"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2025/03/29
</em>

    </span>
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      作者

      <em>
      
        <a href="https://github.com/sirlis">sirlis</a>
      
      </em>
    </span>

    <div>
      <!-- read time -->
      <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="9930 字"
>
  <em>55 分钟</em>阅读</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <p>本文介绍了强化学习的基本概念和模型，主要包括马尔可夫过程、马尔可夫奖励过程和马尔可夫决策过程。</p>

<!--more-->

<hr />

<ul>
  <li><a href="#1-强化学习">1. 强化学习</a>
    <ul>
      <li><a href="#11-状态空间">1.1. 状态空间</a>
        <ul>
          <li><a href="#111-状态">1.1.1. 状态</a></li>
          <li><a href="#112-观测">1.1.2. 观测</a></li>
        </ul>
      </li>
      <li><a href="#12-动作空间">1.2. 动作空间</a></li>
      <li><a href="#13-策略">1.3. 策略</a></li>
    </ul>
  </li>
  <li><a href="#2-马尔可夫过程mp">2. 马尔可夫过程（MP）</a>
    <ul>
      <li><a href="#21-基本概念">2.1. 基本概念</a></li>
      <li><a href="#22-环境">2.2. 环境</a></li>
    </ul>
  </li>
  <li><a href="#3-马尔可夫奖励过程mrp">3. 马尔可夫奖励过程（MRP）</a>
    <ul>
      <li><a href="#31-奖励reward">3.1. 奖励（Reward）</a>
        <ul>
          <li><a href="#311-奖励的定义">3.1.1. 奖励的定义</a></li>
          <li><a href="#312-与动作的关系">3.1.2. 与动作的关系</a></li>
          <li><a href="#313-与下一时刻状态的关系">3.1.3. 与下一时刻状态的关系</a></li>
        </ul>
      </li>
      <li><a href="#32-回报return">3.2. 回报（Return）</a></li>
      <li><a href="#33-价值函数">3.3. 价值函数</a></li>
      <li><a href="#34-贝尔曼方程">3.4. 贝尔曼方程</a></li>
    </ul>
  </li>
  <li><a href="#4-马尔可夫决策过程mdp">4. 马尔可夫决策过程（MDP）</a>
    <ul>
      <li><a href="#41-动作action">4.1. 动作（Action）</a></li>
      <li><a href="#42-策略policy">4.2. 策略（Policy）</a></li>
      <li><a href="#43-动态特性">4.3. 动态特性</a>
        <ul>
          <li><a href="#431-状态转移概率">4.3.1. 状态转移概率</a></li>
          <li><a href="#432-奖励概率">4.3.2. 奖励概率</a></li>
        </ul>
      </li>
      <li><a href="#44-价值函数">4.4. 价值函数</a>
        <ul>
          <li><a href="#441-状态价值函数">4.4.1. 状态价值函数</a></li>
          <li><a href="#442-动作价值函数">4.4.2. 动作价值函数</a></li>
          <li><a href="#443-二者的意义">4.4.3. 二者的意义</a></li>
          <li><a href="#444-回溯图与回溯操作">4.4.4. 回溯图与回溯操作</a></li>
        </ul>
      </li>
      <li><a href="#45-贝尔曼方程">4.5. 贝尔曼方程</a>
        <ul>
          <li><a href="#451-状态价值函数的贝尔曼方程">4.5.1. 状态价值函数的贝尔曼方程</a></li>
          <li><a href="#452-矩阵化表述与迭代求解">4.5.2. 矩阵化表述与迭代求解</a></li>
          <li><a href="#453-状态-动作价值函数的贝尔曼方程">4.5.3. 状态-动作价值函数的贝尔曼方程</a></li>
        </ul>
      </li>
      <li><a href="#46-贝尔曼最优方程">4.6. 贝尔曼最优方程</a>
        <ul>
          <li><a href="#461-最优策略">4.6.1. 最优策略</a></li>
          <li><a href="#462-最优价值函数">4.6.2. 最优价值函数</a></li>
          <li><a href="#463-贝尔曼最优方程">4.6.3. 贝尔曼最优方程</a></li>
          <li><a href="#464-最优策略的求解">4.6.4. 最优策略的求解</a></li>
        </ul>
      </li>
      <li><a href="#47-贝尔曼期望方程">4.7. 贝尔曼期望方程</a></li>
    </ul>
  </li>
  <li><a href="#5-参考文献">5. 参考文献</a></li>
</ul>

<h2 id="1-强化学习"><span class="me-2">1. 强化学习</span><a href="#1-强化学习" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>强化学习是机器学习领域之一，受到行为心理学的启发，主要关注智能体如何在环境中采取不同的行动，以最大限度地提高累积奖励。强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法。与监督学习不同的是，强化学习不需要带标签的输入输出对，同时也无需对非最优解的精确地纠正。</p>

<p>强化学习主要由智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）组成。智能体执行了某个动作后，环境将会转换到一个新的状态，对于该新的状态环境会给出奖励信号（正奖励或者负奖励）。随后，智能体根据新的状态和环境反馈的奖励，按照一定的策略执行新的动作。上述过程为智能体和环境通过状态、动作、奖励进行交互的方式。</p>

<p>在每个时刻 $t$，智能体观察到所在的环境状态记为 $s_t$，，并再次基础上选择一个动作 $a_t$。作为动作的结果，智能体接收到一个数值化的奖励 $r_{t+1}$，并转移到一个新的状态 $s_{t+1}$ 。</p>

<p><a href="/assets/img/postsimg/20221109/0-reinforcement-learning-basic-diagram.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20221109/0-reinforcement-learning-basic-diagram.jpg" alt="强化学习示意图" class="lazyload" data-proofer-ignore></a></p>

<p>如此反复迭代交互，可以得到一个序列或轨迹（trajectory）。</p>

<p>强化学习的朴素目标：<strong>训练智能体尽可能多的收集奖励</strong>。</p>

<h3 id="11-状态空间"><span class="me-2">1.1. 状态空间</span><a href="#11-状态空间" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="111-状态"><span class="me-2">1.1.1. 状态</span><a href="#111-状态" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>「状态」$s$ 是对环境当前所处环境的完整描述，对于状态来说环境的所有信息都是可知的。在数学上，状态空间是一系列状态组成的集合 $\mathcal{S}$，其中 $s \in \mathcal{S}$。</p>

<p>在深度强化学习中，我们通常会使用实值向量<code class="language-plaintext highlighter-rouge">vector</code>、矩阵 <code class="language-plaintext highlighter-rouge">matrix</code> 或者高维张量 <code class="language-plaintext highlighter-rouge">tensor</code> 来表示状态。比如，视觉的状态可以表示为像素值构成的 RGB 矩阵，机器人的状态则可以表示为其关节角度和速度。</p>

<h4 id="112-观测"><span class="me-2">1.1.2. 观测</span><a href="#112-观测" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>「观测」$o$ 则是一个状态的部分描述，可能会忽略一些信息，其同样可用实值向量<code class="language-plaintext highlighter-rouge">vector</code>、矩阵 <code class="language-plaintext highlighter-rouge">matrix</code> 或者高维张量 <code class="language-plaintext highlighter-rouge">tensor</code> 来表示。</p>

<p>当智能体能够观测到环境的全部状态时，这样的环境是可完全观测的 <code class="language-plaintext highlighter-rouge">fully observed</code>；当智能体只能观测到部分状态时，这样的环境称为可部分观测 <code class="language-plaintext highlighter-rouge">partially observed</code>。</p>

<blockquote>
  <p>「注」：在强化学习的公式中我们经常会看到表示状态的符号 $s$，但是实际上更准确的用法应该是使用表示观测的符号 $o$。比如，当我们探讨智能体如果进行动作决策时，公式中通常会说动作是基于状态的，但是「实际上动作是基于观测的」，因为智能体是无法直接感知到状态的。为了遵循惯例，之后的公式仍然会使用符号 $s$。</p>
</blockquote>

<h3 id="12-动作空间"><span class="me-2">1.2. 动作空间</span><a href="#12-动作空间" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>「<strong>给定的环境中有效的动作集合称为动作空间</strong>」。动作是对状态而言的，不同的状态可能有不同的动作空间，因此动作空间是一个与状态有关的集合。动作可以表示为 $\mathcal{A}(s),\; \forall s\in \mathcal{S}$。</p>

<p>「<strong>动作空间是离散的还是连续的</strong>」在强化学习问题中非常重要，有些方法只适合用于其中一个场景，所以这点需要特别关注。有些环境中（比如 Atari 和 Go），「动作空间是离散的」<code class="language-plaintext highlighter-rouge">discrete</code>，也就是说智能体的动作数量是有限的；而有些环境中（比如机器人控制），「动作空间是连续的」<code class="language-plaintext highlighter-rouge">continuous</code>，这些空间中动作通常用实值向量表示。</p>

<h3 id="13-策略"><span class="me-2">1.3. 策略</span><a href="#13-策略" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>强化学习是从环境状态到动作的映射学习，称该映射关系为策略。通俗的理解，即智能体如何选择动作的思考过程称为策略。</p>

<p>最简单的策略可以是固定策略，如不管什么情况都执行特定动作。也可以采用规则策略，即满足什么条件下执行什么动作。也可以采用随机策略，即按照某个概率采样来选取动作。</p>

<h2 id="2-马尔可夫过程mp"><span class="me-2">2. 马尔可夫过程（MP）</span><a href="#2-马尔可夫过程mp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="21-基本概念"><span class="me-2">2.1. 基本概念</span><a href="#21-基本概念" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>几个基本概念定义如下：</p>

<ul>
  <li>
    <p><strong>马尔可夫性</strong>（Markov property）：在一个时序过程中，如果 $t＋1$ 时刻的状态仅取决于 $t$ 时刻的状态 $s_t$ 而与 $t$ 时刻之前的任何状态都无关时，则认为 $t$ 时刻的状态 $s_t$ 具有马尔可夫性，数学表述为
\(p(s_{t+1}\vert s_{t},\cdots, s_1 ) = p( s_{t+1} \vert s_t)\)</p>
  </li>
  <li>
    <p><strong>马尔科夫过程</strong>（Markov Process）：若过程中的每一个状态都具有马尔科夫性，则这个过程具备马尔科夫性。具备了马尔科夫性的随机过程称为马尔科夫过程，</p>
  </li>
  <li>
    <p><strong>马尔可夫链</strong>（Markov chain）：离散状态空间的马尔可夫过程称为马尔科夫链（Markov chain）。</p>
  </li>
</ul>

<p>马尔可夫过程中的三个概念：</p>

<ul>
  <li><strong>采样</strong>（sample）或抽样：从符合马尔科夫过程给定的状态转移概率矩阵生成一个轨迹或回合的过程</li>
  <li><strong>轨迹</strong>（trajectory）或序列：采样得到的一条无限长的状态序列,如：$s_1-s_2-s_3-…$</li>
  <li><strong>回合</strong>（episode）或情节：采样直至一个终止状态，形成一条完整的状态序列。如：$s_1-s_2-s_3-\cdots-s_T$</li>
</ul>

<h3 id="22-环境"><span class="me-2">2.2. 环境</span><a href="#22-环境" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>环境是智能体交互的对象，是智能体在某状态下采取某动作后转移到新状态的最终参与者。以自动驾驶为例，在某个雨天的行驶状态下，智驾决定在 $120$ km/h 车速下（状态 $s_1$）向左打方向盘（动作）变向超车，期望状态是变到左侧相邻车道，但由于雨天湿滑（环境）导致车子最终滑到路边（新状态 $s_2$）。但是如果换一个干燥的晴天，大概率车子会正常变到左侧相邻车道（新状态 $s_3$）。</p>

<p>上述例子比较直观体现了环境在状态转移中的作用，即在状态 $s_1$ 下动作 $a_1$ 的结果并不一定是某个确定的新状态，而是与实际环境有关。一般情况下，环境可以建模为一个状态转移概率矩阵 $P$。</p>

<p>在马尔可夫过程中，我们暂时还不用考虑动作的作用，因此可以假设雨天环境大概率滑到路边（$s_2$），状态转移矩阵的元素为：</p>

\[\begin{aligned}
P(s_2\vert s_1) &amp;= 0.85\\
P(s_3\vert s_1) &amp;= 0.15
\end{aligned}\]

<p>用二元组 $&lt;S,P&gt;$ 表述马尔可夫过程。其中，$S$ 为状态，$P$ 为不同状态间的转移概率，如状态 $s$ 到 $s^\prime$ 的状态转移概率</p>

\[P_{s s^\prime}=P_{s s^\prime}=\mathbb{P}[S_{t+1}=s^\prime \vert S_t = s]\]

<p>从上式也可以看到，下一时刻的状态$S_{t+1}$ 仅与当前时刻的状态 $S_t$ 有关，而与 $S_1,\cdots,S_{t-1}$ 无关。注意：这里的记号非常严谨， $S_{t}, S_{t+1}$ 代表某一时刻的状态，而 $s,s^\prime$ 代表某一种具体的状态类型。而在实际情况下，人们会将其简写多种不同的形式，需要特别注意</p>

\[P_{s s^\prime}=P=p(S_{t+1}=s^\prime \vert S_{t}=s)=p(s^\prime \vert s) = p(s_{t+1} \vert s_{t})\]

<p>对于离散的状态空间，为了描述整个状态空间中不同类型状态之间的关系，自然用矩阵表示，即：</p>

\[P=
\begin{bmatrix}
p(s_1\vert s_1) &amp; p(s_2\vert s_1) &amp;\dots &amp; p(s_n\vert s_1)\\
p(s_1\vert s_2) &amp; p(s_2\vert s_2) &amp;\dots &amp; p(s_n\vert s_2)\\
\vdots &amp; \vdots &amp; \ddots &amp;\vdots\\
p(s_1\vert s_n) &amp; p(s_2\vert s_n)&amp;\dots &amp; p(s_n\vert s_n)\\
\end{bmatrix}\]

<p>显然状态转移概率矩阵 $P$ 的规模是所有状态类型数 $n$ 的平方。且从一个状态转移到所有可能状态的概率之和为 $1$。</p>

<p>以下图为例：</p>

<p><a href="/assets/img/postsimg/20221109/1-mp.png" class="popup img-link "><img data-src="/assets/img/postsimg/20221109/1-mp.png" alt="马尔可夫过程" class="lazyload" data-proofer-ignore></a></p>

<p>不难写出状态转移概率矩阵：</p>

\[\begin{aligned}
&amp;\quad C1\;\;\;C2\;\;\;C3\;\;\;Pass\;Pub\;FB\;\;Sleep\\
P=
\begin{array}{r}
    C1\\
    C2\\
    C3\\
    Pass\\
    Pub \\
    FB\\
    Sleep
\end{array}
&amp;\begin{bmatrix}
    0&amp; 0.5 &amp;0&amp;0&amp;0&amp;0.5&amp;0\\
    0&amp;0&amp;0.8&amp;0&amp;0&amp;0&amp;0.2\\
    0&amp;0&amp;0&amp;0.6&amp;0.4&amp;0&amp;0\\
    0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1\\
    0.2&amp;0.4&amp;0.4&amp;0&amp;0&amp;0&amp;0\\
    0.1&amp;0&amp;0&amp;0&amp;0&amp;0.9&amp;0\\
    0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1\\
\end{bmatrix}
\end{aligned}\]

<h2 id="3-马尔可夫奖励过程mrp"><span class="me-2">3. 马尔可夫奖励过程（MRP）</span><a href="#3-马尔可夫奖励过程mrp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>马尔可夫奖励过程（Markov Reward Process，MRP）就是在马尔可夫过程中增加了奖励（收益） $R$ 项和奖励因子 $\gamma$。</p>

<p>表述一个马尔可夫奖励过程需要<strong>四元组</strong> $&lt;S,P,R,\gamma&gt;$，其中：</p>

<ul>
  <li>$S$ 是一个有限状态集；</li>
  <li>$P$ 是集合中状态转移概率矩阵：$P_{s s^\prime}=\mathbb{P}[S_{t+1}=s^\prime \vert S_t = s]$</li>
  <li>$R$ 是奖励（收益）函数：$R_t = \mathbb{E}[R_{t+1}\vert S_t=s]$</li>
  <li>$\gamma$ 是折扣因子：$\gamma \in [0,1)$</li>
</ul>

<p>如下图所示</p>

<p><a href="/assets/img/postsimg/20221109/2-mrp.png" class="popup img-link "><img data-src="/assets/img/postsimg/20221109/2-mrp.png" alt="马尔可夫奖励过程" class="lazyload" data-proofer-ignore></a></p>

<h3 id="31-奖励reward"><span class="me-2">3.1. 奖励（Reward）</span><a href="#31-奖励reward" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="311-奖励的定义"><span class="me-2">3.1.1. 奖励的定义</span><a href="#311-奖励的定义" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>某时刻 $t$ 的即时奖励（immediate reward） $R_t$ 定义为<strong>从该时刻所处状态转移到下一时刻状态后从环境获得的标量数值</strong>。它是环境直接返回的反馈值，这个值可以是确定的，也可以是随机的（从某个概率分布中采样得到），具体取决于环境的定义。如果环境是随机的，即时奖励可能是一个随机变量，但从单次交互的角度来看，它是一个具体的标量值。</p>

\[r_{t+1} = r(s_t)\]

<p>关于下标为何为 $t+1$ 而不是 $t$，是因为通常约定奖励是离开当前状态后在下一时刻才获得的。</p>

<blockquote>
  <p>在后续马尔可夫决策过程中，其是与下一时刻状态 $s_{t+1}$ 一同被环境所返回的。</p>
</blockquote>

<h4 id="312-与动作的关系"><span class="me-2">3.1.2. 与动作的关系</span><a href="#312-与动作的关系" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>即时奖励与动作有关，因此在后续马尔可夫决策过程中，奖励可以表述为</p>

\[r_{t+1} = r(s_t,a_t)\]

<h4 id="313-与下一时刻状态的关系"><span class="me-2">3.1.3. 与下一时刻状态的关系</span><a href="#313-与下一时刻状态的关系" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>奖励是环境赋予的，其与下一时刻是否有关取决于环境的定义。从正常的思维来讲，奖励当然与下一时刻的状态有关系的（比如状态转移后进入终点导致获得正奖励，或者进入陷阱导致获得负奖励）。这种情况下，奖励应表述为</p>

\[r_{t+1} = r(s_t,a_t,s_{t+1})\]

<p>但我们通常约定，奖励函数只依赖于当前状态和动作，与下一时刻无关。</p>

<blockquote>
  <p>在后续马尔可夫决策过程中，强调下一时刻的奖励和下一时刻的状态是被环境一起决定的。这导致离开当前状态获得的奖励是概率性的，我们需要求解它的期望</p>

\[r_{t+1} = \mathbb{E}[r\vert s_t,a_t] = r\cdot p(r\vert s_t,a_t) = r\cdot \sum_{s_{t+1}}p(r\vert s_t,a_t,s_{t+1})p(s_{t+1}\vert s_t,a_t)\]

  <p>可以发现，即使奖励和下一时刻状态有关，这种相关性也可以通过状态转移概率来体现。</p>
</blockquote>

<p>此时，强化学习的目标：<strong>使得智能体收到的累计奖励最大化。</strong></p>

<p>对于无限长度的采样轨迹，奖励的期望是无限大的，所以需要做出一个折扣，以便在远期奖励的期望值有较好的收敛性。这就引入了回报和折扣因子的必要性。</p>

<h3 id="32-回报return"><span class="me-2">3.2. 回报（Return）</span><a href="#32-回报return" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>某时刻 $t$ 的回报（收益）定义为<strong>从时刻 $t$ 开始的 <font color="red">采样一条</font> 状态序列得到的所有奖励的折扣和</strong>：</p>

\[G_t\doteq r_{t+1}+\gamma r_{t+2}+...=\sum_{k=0}^\infty \gamma^k r_{t+k+1}\]

<p>其中 $\gamma \in [0,1)$ 被称为折扣因子，通常取值为 $0.9$。设置折扣因子的原因如下：</p>

<ul>
  <li>数学表达的方便，这也是最重要的</li>
  <li>保证奖励的收敛性，避免陷入无限循环</li>
  <li>远期利益具有一定的不确定性</li>
  <li>符合人类更看重眼前利益的性格</li>
</ul>

<p>折扣因子可以作为强化学习的一个超参数来进行调整，折扣因子不同就会得到不同行为的智能体：</p>

<ul>
  <li>折扣因子接近 $0$ 则表明趋向于“近视”性评估；</li>
  <li>折扣因子接近 $1$ 则表明偏重考虑远期的利益。</li>
</ul>

<p>前述例子中，假设从 $Class 1$ 状态开始到 $Sleep$ 状态终止，折扣因子 $\gamma = 0.5$，采样两条序列计算回报如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>episode 1: C1 - C2 - C3 - Pass - Sleep
G_1 = -2 + 1/2 × (-2) + 1/4 × (-2) + 1/8 × (+10) = -2.25
episode 2: C1 - FB - FB - C1 - C2 - Sleep
G_2 = -2 + 1/2 × (-1) + 1/4 × (-1) + 1/8 × (-2) + 1/16 ×(-2) = -3.125
</pre></td></tr></tbody></table></code></div></div>

<p>此时，强化学习的目标：<strong>使得智能体收到的回报最大化。</strong></p>

<p>回报值是针对一次完整的采样序列的结果，存在很大的样本偏差。即 $G(s)$ 是从 $t$ 时刻的状态到终止状态的一条状态转移序列的回报值，但从 $t$ 时刻的状态到终止状态的马尔可夫链不止一条，每一条都有对应的采样概率和回报。对于复杂问题而言，要想精确的计算出 $G(s)$ 是几乎不可能的，因为无法穷举所有序列。</p>

<p>为了能够评估状态的好坏，引入新概念：价值函数。</p>

<h3 id="33-价值函数"><span class="me-2">3.3. 价值函数</span><a href="#33-价值函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p><strong>价值函数</strong>（Value Function）：<strong>从某个状态 $s_t$ 开始的回报的期望，也即从某个状态 $s_t$ 开始采样无数条完整状态序列的回报的平均值</strong>，即</p>

\[V(s_t) = \mathbb{E}[G_t \vert S_t=s_t]\]

<p>对于马尔可夫奖励过程，价值函数即为状态价值函数。</p>

<p>以前面的例子，如果仅观测到两个序列，那么在状态 Class 1 处的学生的值函数就是 2 个回报值除以 2 即可。</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>v(Class1) = (G_1 + G_2) / 2 = ( (-2.25) + (-3.125)) / 2 = -2.6875
</pre></td></tr></tbody></table></code></div></div>

<p>状态值函数的引入，从数学上解决了回报 $G(s)$ 计算时依赖大量采样，难以实际应用的问题。</p>

<p>但状态价值函数也不好算，因为在计算某个状态时候需要使用到将来所有状态的 $G(s)$。为了便于计算，对价值函数进行展开</p>

\[\begin{aligned}
V(s_t) &amp;= \mathbb{E}[G_t \vert S_t=s_t]\\
&amp;=\mathbb{E}[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+...\vert S_t=s_t]\\
&amp;=\mathbb{E}[r_{t+1}+\gamma (r_{t+2}+\gamma r_{t+3}+...)\vert S_t=s_t]\\
&amp;=\mathbb{E}[r_{t+1}+\gamma G_{t+1}\vert S_t=s_t]\\
&amp;=\mathbb{E}[r_{t+1}\vert S_t=s]+\gamma \mathbb{E}[G_{t+1}\vert S_t=s_t]\\
&amp;=R_{s}+\gamma \mathbb{E}[G_{t+1}\vert S_t=s_t]
\end{aligned}\]

<p>上式中，第一项 $R_s$ 对应即时奖励的期望</p>

\[R_s = \sum_{a}\pi(a\vert s)\sum_r(p(r\vert s,a)\cdot r)\]

<p>第二项则代表了长期的潜在奖励。可以看出，长期潜在奖励的计算需要获取下一时刻状态对应回报的期望。然而，未来时刻的状态及其回报是不确定的，即</p>

\[\mathbb{E}[G_{t+1}\vert S_t=s_t]\]

<p>依然是一个很难求解的期望形式。因此直接计算价值函数是不现实的。下面介绍贝尔曼方程来计算价值函数。</p>

<h3 id="34-贝尔曼方程"><span class="me-2">3.4. 贝尔曼方程</span><a href="#34-贝尔曼方程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p><strong>[ 推导 1 ]：</strong></p>

<blockquote>
  <ul>
    <li><strong>定义</strong>：如果 $X$ 和 $Y$ 都是离散型随机变量，则条件期望（Conditional Expectation）定义为
$\mathbb{E}[Y\vert X=x]=\sum_y yP(Y=y\vert X=x)$</li>
    <li><strong>定义</strong>：如果 $X$ 是随机变量，其期望为 $\mathbb{E}[X]$，$Y$ 为相同概率空间上的任意随机变量，则有全期望（Total Expectation）公式
$\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X\vert Y]]$</li>
  </ul>
</blockquote>

<p>现证明（主要证明第一个等式）</p>

\[\mathbb{E}[G_{t+1}\vert S_t=s_t] = \mathbb{E}[\mathbb{E}[G_{t+1}\vert S_{t+1}]\vert S_t=s_t] = \mathbb{E}[V(s_{t+1})\vert S_t=s_t]\]

<p>为了推导简便，另 $s_{t+1} = s^\prime$，$s_t=s$，有</p>

\[\begin{aligned}
\mathbb{E}[\mathbb{E}[G_{t+1}\vert S_{t+1}]\vert S_t=s] &amp;= \mathbb{E}\left[\sum_{g^\prime}g^{\prime}P(G(s^\prime)=g^{\prime}\vert S_{t+1})\vert s\right]\quad (条件期望)\\
&amp;=\sum_{s^\prime} \sum_{g^\prime}g^{\prime}P(G(s^\prime)=g^{\prime}\vert S_{t+1}=s^\prime, s)P(S_{t+1}=s^\prime\vert s)\\
&amp;=\sum_{s^\prime} \sum_{g^\prime}g^{\prime} \frac{P(G(s^\prime)=g^{\prime}\vert S_{t+1}=s^\prime, s)P(S_{t+1}=s^\prime\vert s)\cdot P(s)}{P(s)} \\
&amp;=\sum_{s^\prime} \sum_{g^\prime}g^{\prime} \frac{P(G(s^\prime)=g^{\prime}\vert S_{t+1}=s^\prime, s)P(S_{t+1}=s^\prime, s)}{P(s)} \\
&amp;=\sum_{s^\prime} \sum_{g^\prime}g^{\prime} \frac{P(G(s^\prime)=g^{\prime}, S_{t+1}=s^\prime, s)}{P(s)} \\
&amp;=\sum_{s^\prime} \sum_{g^\prime}g^{\prime} P(G(s^\prime)=g^{\prime}, S_{t+1}=s^\prime \vert s) \\
&amp;=\sum_{g^\prime} \sum_{s^\prime}g^{\prime} P(G(s^\prime)=g^{\prime}, S_{t+1}=s^\prime \vert s) \\
&amp;=\sum_{g^\prime}g^{\prime} P(G(s^\prime)=g^{\prime} \vert s) \\
&amp;=\mathbb{E}[G(s^\prime)\vert s]=\mathbb{E}[G_{t+1}\vert s_t]
\end{aligned}\]

<p>得证。则当前时刻的状态价值函数</p>

\[\begin{aligned}
V(s_t)&amp;=R_{s}+\gamma \mathbb{E}[G_{t+1}\vert S_t=s_t]\\
&amp;=R_{s}+\gamma \mathbb{E}[V(s_{t+1})\vert S_t=s_t]\\
&amp;=R_{s}+\gamma \sum_{s_{t+1}\in S} V(s_{t+1})P(s_{t+1}\vert s_t)
\end{aligned}\]

<p>上式即为马尔可夫奖励过程的贝尔曼方程。</p>

<p><strong>[ 推导 2 ]：</strong></p>

<p>对后项进行全概率展开</p>

\[\begin{aligned}
\gamma \mathbb{E}[G_{t+1}\vert S_t=s_t] &amp;= \gamma \sum_{s_{t+1}\in S}\mathbb{E}[G_{t+1}\vert S_{t+1}=s_{t+1}]P(s_{t+1}\vert s_t)\\
&amp;= \gamma \sum_{s_{t+1}\in S} V(s_{t+1})P(s_{t+1}\vert s_t)
\end{aligned}\]

<p>上面第二步是因为（根据价值的定义）</p>

\[V(s_{t+1}) = \mathbb{E}[G_{t+1}  \vert S_{t+1} = s_{t+1}]\]

<p>最终得到</p>

\[V(s_t) = r_{s}+\gamma \sum_{s_{t+1}\in S} V(s_{t+1})P(s_{t+1}\vert s_t)\]

<p>即为马尔可夫奖励过程的贝尔曼方程。</p>

<hr />

<p><strong>贝尔曼方程刻画了当前状态 $s_t$ 和下一个状态 $s_{t+1}$ 之间的关系</strong>。可以看出，当前状态的价值函数可以通过下一个状态的价值函数来迭代计算。</p>

<p>若将马尔可夫奖励过程的状态构成 $n$ 维状态空间，贝尔曼方程可以写成矩阵形式</p>

\[\begin{aligned}
\begin{bmatrix}
    V(s_1)\\
    V(s_2)\\
    \vdots\\
    V(s_n)
\end{bmatrix} &amp;=
\begin{bmatrix}
    R(s_1)\\
    R(s_2)\\
    \vdots\\
    R(s_n)
\end{bmatrix}
+\gamma
\begin{bmatrix}
P(s_1\vert s_1) &amp; P(s_2\vert s_1)&amp; \cdots &amp; P(s_n\vert s_1)\\    
P(s_1\vert s_2) &amp; P(s_2\vert s_2)&amp; \cdots &amp; P(s_n\vert s_2)\\    
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\    
P(s_1\vert s_n) &amp; P(s_2\vert s_n)&amp; \cdots &amp; P(s_n\vert s_n)\\    
\end{bmatrix}
\begin{bmatrix}
    V(s_1)\\
    V(s_2)\\
    \vdots\\
    V(s_n)
\end{bmatrix}\\
\boldsymbol{V} &amp;= \boldsymbol{R}+\gamma \boldsymbol{P} \boldsymbol{V}\\
\end{aligned}\]

<p>上述是个线性方程组，可直接得到闭式解</p>

\[\boldsymbol{V} = (\boldsymbol{I}-\gamma\boldsymbol{P})^{-1}\boldsymbol{R}\]

<p>需要注意的是，矩阵求逆的复杂度为 $O(n^3)$，因此直接求解仅适用于状态空间规模小的问题。状态空间规模大的问题的求解通常使用迭代法，在后续介绍马尔可夫决策过程时进行详细介绍。</p>

<blockquote>
  <p>注意到，只有当 $P$ 已知的情况下，也就是模型已知时，才可以得到解析形式的闭式解。</p>
</blockquote>

<h2 id="4-马尔可夫决策过程mdp"><span class="me-2">4. 马尔可夫决策过程（MDP）</span><a href="#4-马尔可夫决策过程mdp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>马尔可夫决策过程是在马尔可夫奖励过程的基础上加入了决策，即增加了动作。其定义为：</p>

<p>马尔科夫决策过程是一个<strong>五元组</strong> $&lt;S,A,P,R,\gamma&gt;$，其中</p>

<ul>
  <li>$S$ 是有限数量的状态集</li>
  <li>$A$ 是有限数量的动作集</li>
  <li>$P$ 是状态转移概率，$P_{ss^\prime}^a=\mathbb{P}[S_{t+1} = s^\prime \vert S_t = s, A_t=a]$</li>
  <li>$R$ 是一个奖励函数，\(R_{s}^a=\mathbb{E}[R_{t+1} \vert S_t = s, A_t=a]\)</li>
  <li>$\gamma$ 是一个折扣因子，$\gamma \in [0,1)$</li>
</ul>

<p>从上面定义可以看出，马尔可夫决策过程的状态转移概率和奖励函数不仅取决于智能体当前状体，<strong>还取决于智能体选取的动作</strong>。而马尔可夫奖励过程仅取决于当前状态。</p>

<h3 id="41-动作action"><span class="me-2">4.1. 动作（Action）</span><a href="#41-动作action" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>以下图为例：</p>

<p><a href="/assets/img/postsimg/20221109/3-mdp.png" class="popup img-link "><img data-src="/assets/img/postsimg/20221109/3-mdp.png" alt="马尔可夫决策过程举例" class="lazyload" data-proofer-ignore></a></p>

<p>图中红色的文字表示学生采取的动作，而不是 MRP 时的状态名。对比之前的学生 MRP 示例可以发现，即时奖励与动作有关了，同一个状态下采取不同的动作得到的即时奖励是不一样的。</p>

<p>由于引入了动作，容易与状态名称混淆，因此此图没有给出各状态的名称；此图还把 Pass 和 Sleep 状态合并成一个终止状态；另外当选择”去查阅文献（Pub）”这个动作时，主动进入了一个临时状态（图中用黑色小实点表示），随后被动的被环境按照其动力学分配到另外三个状态，也就是说此时智能体没有选择权决定去哪一个状态。</p>

<p>可以看出，状态转移概率 $P_{ss^\prime}^a$ 和奖励函数 $R_{s}^a$ 均与当前状态 $s$ 下采取的动作 $a$ 有关。由于动作的选取不是固定的，因此引入新概念：策略。</p>

<h3 id="42-策略policy"><span class="me-2">4.2. 策略（Policy）</span><a href="#42-策略policy" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>策略 $\pi(a\vert s)$ 是从状态 $s$ 到每个动作 $a$ 的选择概率之间的映射，即</p>

\[\pi(a\vert s) = \mathbb{P}[A_t=a \vert S_t=s]\]

<p>一个策略完整地定义了智能体的行为方式，即策略定义了智能体在各种状态下可能采取的动作，以及在各种状态下采取各种动作的概率。MDP的策略仅与当前的状态有关，与历史信息无关；同时某一确定的策略是静态的，与时间无关；但是个体可以随着时间更新策略。</p>

<p>给定一个马尔可夫决策过程 $&lt;S,A,P,R,\gamma&gt;$ 和一个策略 $\pi$ 后，相应的状态转移概率 $P_{ss^\prime}^\pi$ 和奖励函数 $R_{s}^\pi$ 可更新描述如下</p>

\[\begin{aligned}
    P_{ss^\prime}^\pi &amp;= \sum_{a\in A}\pi(a\vert s)P_{ss^\prime}^a\\
    R_{s}^\pi &amp;= \sum_{a\in A}\pi(a\vert s)R_{s}^a
\end{aligned}\]

<p>对应的 $&lt;S,P^\pi,R^\pi,\gamma&gt;$ 是一个马尔可夫奖励过程， $&lt;S,P^\pi&gt;$ 是一个马尔可夫过程。</p>

<h3 id="43-动态特性"><span class="me-2">4.3. 动态特性</span><a href="#43-动态特性" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>在有限 MDP 中，状态、动作和奖励的集合（$S, A, R$）都只有有限个元素。在这种情况下，随机变量 $R_t$ 和 $S_t$ 具有定义明确的离散概率分布，并且之依赖于前一时刻的状态和动作。也就是说，给定前一时刻的状态和动作的值时，这些随机变量的特定值 $s^\prime \in S, r^\prime \in R$ 在 $t$ 时刻出现的概率为</p>

\[p(s^\prime,r \vert s,a) \doteq \mathbb{P}\{S_t=s^\prime, R_t=r \vert S_{t-1}=s, A_{t-1}=a\}\]

<p>函数 $p$ 定义了 MDP 的<strong>动态特性</strong>。动态特性函数是一个描述 $t-1$ 和 $t$ 前后两个相邻时刻的随机变量间动态关系的条件概率。</p>

<p>在 MDP 中，由 $p$ 给出的概率完全刻画了环境的动态特性，即$S_t,R_t$ 的每个可能的值出现的概率只取决于前一个状态 $S_{t-1}$ 和动作 $A_{t-1}$，且与更早时刻的状态和动作无关（马尔可夫性）。</p>

<h4 id="431-状态转移概率"><span class="me-2">4.3.1. 状态转移概率</span><a href="#431-状态转移概率" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>从四参数动态函数 $p$ 中，可以计算出关于环境的任何其它信息。比如，想表达MDP的状态转移过程，可以将随机变量 $R$ 求积分得到 <strong>状态转移概率</strong></p>

\[P_{ss^\prime}^a = p(s^\prime \vert s,a) \doteq \mathbb{P}\{ S_t=s^\prime \vert S_{t-1}=s, A_{t-1}=a \} = \sum_{r\in R}p(s^\prime,r \vert s,a)\]

<p>所以，整个马尔可夫决策过程的全部信息包含在状态变量集合 $A$，$S$，$R$ 和函数空间 $P$ 中，每个时刻都有一个 $A_t$，$S_t$，$R_t$，每两个相邻时刻之间都有一个 $p_t$。</p>

<h4 id="432-奖励概率"><span class="me-2">4.3.2. 奖励概率</span><a href="#432-奖励概率" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>类似地，状态动作 $s,a$ 对的期望奖励可以写作两个参数的函数</p>

\[r(s,a) = \mathbb{E}_\pi [R_{t+1} \vert S_t = s] = \sum_{r\in R} r \sum_{s^\prime \in S} p(s^\prime, r \vert s,a)\]

<p>想表达 MDP 的即时奖励获取过程，可以将对 $s^\prime$ 求积分得到 <strong>奖励概率</strong></p>

\[P_{ss^\prime}^r = p(r \vert s,a) = \sum_{s^\prime \in S}p(s^\prime, r \vert s,a)\]

<h3 id="44-价值函数"><span class="me-2">4.4. 价值函数</span><a href="#44-价值函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>马尔可夫决策过程中，价值函数分为状态价值函数和动作价值函数。</p>

<h4 id="441-状态价值函数"><span class="me-2">4.4.1. 状态价值函数</span><a href="#441-状态价值函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p><strong>状态价值函数（state-value Function）是从某个状态 $s$ 开始，<font color="red">执行策略 $\pi$</font> 所获得的回报的期望</strong>；也即在执行当前策略时，衡量智能体处在状态 $s$ 时的价值大小。即</p>

\[v_\pi(s) \doteq \mathbb{E}_\pi[G_t \vert S_t=s]\]

<p>注意，终止状态的价值始终为零。我们把函数 $v_{\pi}(s)$ 称为策略 $\pi$ 的状态价值函数。</p>

<p>状态价值函数衡量了一个状态的好坏，好的状态拥有较大的状态价值函数，表明从这个状态出发，预期能获得更高的回报。</p>

<h4 id="442-动作价值函数"><span class="me-2">4.4.2. 动作价值函数</span><a href="#442-动作价值函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>类似地，我们把策略 $\pi$ 下在状态 $s$ 时采取动作 $a$ 的价值即为 $q_\pi(s,a)$。即根据策略 $\pi$，从状态 $s$ 开始，执行动作 $a$ 之后，所有可能的决策序列的期望回报</p>

\[q_\pi(s,a) \doteq \mathbb{E}_\pi[G_t \vert S_t=s, A_t=a]\]

<p>状态-动作价值函数衡量了某个状态下不同动作的好坏，好的状态-动作拥有较大的状态-动作价值函数，表明从这个状态出发选择这个动作，预期能获得更高的回报。</p>

<h4 id="443-二者的意义"><span class="me-2">4.4.3. 二者的意义</span><a href="#443-二者的意义" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<blockquote>
  <p>如何理解强化学习中的Q值和V值？ https://zhuanlan.zhihu.com/p/109498587</p>
</blockquote>

<blockquote>
  <p>强化学习-1-Q_Table based Method  https://zhuanlan.zhihu.com/p/138291295</p>
</blockquote>

<p><strong>状态价值函数</strong>是对状态的价值的衡量。从某个状态出发，依据特定策略 $\pi$ 采用不同动作，在环境的影响下进行状态转移。</p>

<p><strong>动作价值函数</strong>是对某状态下特定动作的价值的衡量。从某个状态出发，采取特定的动作 $a$，在环境的影响下进行状态转移；后续依据特定策略 $\pi$ 采用不同动作，在环境的影响下进行状态转移。</p>

<p>强化学习模型的好坏，主要取决于在当前状态下，是否能选择出收益最大的动作，因此 $q(s,a)$ 的精准预估显得十分重要。</p>

<h4 id="444-回溯图与回溯操作"><span class="me-2">4.4.4. 回溯图与回溯操作</span><a href="#444-回溯图与回溯操作" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>回溯操作就是将后继状态（或“状态-动作”二元组）的价值信息 <em>回传</em> 给当前时刻的状态（或”状态-动作“二元组），可以用回溯图来表示，这是强化学习的核心内容。</p>

<p>典型的回溯图如下</p>

<p><a href="/assets/img/postsimg/20221109/4-backup-diagram.png" class="popup img-link "><img data-src="/assets/img/postsimg/20221109/4-backup-diagram.png" alt="回溯图" class="lazyload" data-proofer-ignore></a></p>

<p>严谨地说，$q_\pi$ 和 $v_\pi$ 的作用是评估给定策略 $\pi$ 的价值，也就是一直使用这个策略来选取动作能得到的期望回报。不同之处是，$v_\pi$ 评估的对象是状态，考虑从状态 $s$ 出发，遵循策略 $\pi$ 得到的期望回报；$q_\pi$ 评估的对象是一个状态-动作对，考虑从状态 $s$ 出发，执行动作 $a$ 之后，遵循策略 $\pi$ 得到的期望回报。</p>

<p>因此，$v_\pi$ 可以写成 $q_\pi$ 关于策略 $\pi$（执行不同动作）的期望，$q_\pi$ 可以写成 $v_\pi$ 关于状态转移 $P_{ss^\prime}^a=p(s^\prime \vert s,a)$（执行动作 $a$ 后转移到不同状态）的期望。然后它们相互套娃，就得到了下面的两条等式，这两个等式也可以通过回溯图来直观理解。</p>

<p>[ <strong>等式1</strong> ]：</p>

\[v_\pi(s) = \mathbb{E}_aq_\pi(s,a) = \sum_{a\in A}\pi(a\vert s)q_\pi(s,a)\]

<p>上面回溯图的上半部分对应上述等式，描述了处于特定状态 $s$ 的价值。即在状态 $s$ 时，遵循策略 $\pi$ 后，状态 $s$ 的价值体表示为在该状态下采取所有可能动作的动作价值（$q$ 值）按该状态下动作发生概率（策略 $\pi$）的乘积求和。</p>

<p>从状态 $s$ 来看，我们有可能采取两种行动（图中黑点），每个动作都有一个 $q$ 值（状态-动作值函数）。对 $q$ 值进行平均，这个均值告诉我们在特定状态下有多好，也即 $v_\pi(s)$。</p>

<p>上述等式可以通过 $v_\pi(s)$ 的贝尔曼方程推导得到，即</p>

\[\begin{aligned}
v_\pi(s) &amp;= \sum_{a, s^\prime, r}\pi(a\vert s)p(s^\prime,r \vert s,a)\cdot [  r+\gamma v_\pi(s^\prime)  ]\\
&amp;= \sum_{a}\pi(a\vert s)\cdot \sum_{s^\prime, r}p(s^\prime,r \vert s,a)\cdot [  r+\gamma v_\pi(s^\prime)  ]\\
&amp;=\sum_{a}\pi(a\vert s)\cdot \sum_{s^\prime, r}p(s^\prime,r \vert s,a)\cdot [r+\gamma r+\gamma^2 r+...\vert s,a]\\
&amp;=\sum_{a}\pi(a\vert s)\cdot \mathbb{E}_\pi[G_t\vert s,a]\\
&amp;=\sum_{a}\pi(a\vert s)\cdot q_\pi(s,a)
\end{aligned}\]

<p>[ <strong>等式2</strong> ]：</p>

\[q_\pi(s,a) = \sum_{s^\prime,r}p(s^\prime,r \vert s,a) \left[r+\gamma v_\pi(s^\prime) \right]\]

<p>证明如下</p>

\[\begin{aligned}
q_\pi(s,a) &amp;= \mathbb{E}_\pi\left[ G_t \vert S_t=s, A_t=a \right]\\
&amp;=\mathbb{E}_\pi\left[R_{t+1}+\gamma G_{t+1}\vert S_t=s, A_t=a \right]\\
&amp;=\sum_{s^\prime,r}p(s^\prime,r \vert s,a) \left[r+\gamma \sum_{a^\prime}   \pi(a^\prime \vert s^\prime) \mathbb{E}_\pi [G_{t+1} \vert S_{t+1}=s^\prime, A_{t+1}=a^\prime]  \right]\\
&amp;=\sum_{s^\prime,r}p(s^\prime,r \vert s,a) \left[r+\gamma \sum_{a^\prime}   \pi(a^\prime \vert s^\prime) q_\pi(s^\prime, a^\prime)  \right]\\
&amp;=\sum_{s^\prime,r}p(s^\prime,r \vert s,a) \left[r+\gamma v_\pi(s^\prime) \right]
\end{aligned}\]

<p>上式也可以从回溯图中推导得出。</p>

<h3 id="45-贝尔曼方程"><span class="me-2">4.5. 贝尔曼方程</span><a href="#45-贝尔曼方程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="451-状态价值函数的贝尔曼方程"><span class="me-2">4.5.1. 状态价值函数的贝尔曼方程</span><a href="#451-状态价值函数的贝尔曼方程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>与马尔可夫奖励过程中的价值函数类似，状态价值函数也有如下贝尔曼方程成立</p>

\[\begin{aligned}
v_\pi(s) &amp;= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} \vert S_t = s]\\
&amp;= \sum_a \pi(a\vert s) \sum_{s^\prime}\sum_r p(s^\prime,r \vert s,a) [  r+\gamma v_\pi(s^\prime)  ],\; \forall s\in \mathcal{S}
\end{aligned}\]

<p>方程推导过程如下，首先易知第一项为即时奖励的期望</p>

\[\begin{aligned}
\mathbb{E}_\pi[R_{t+1}\vert S_t=s] &amp;= \sum_a \pi(a\vert s)\mathbb{E} [R_{t+1}\vert S_t=s, A_t=a]\\
&amp;= \sum_a \pi(a\vert s)\sum_{s^\prime} p(r \vert s,a) r\\
&amp;=\sum_a\pi(a\vert s)\sum_{s^\prime}\sum_r p(s^\prime, r \vert s,a) r\\
\end{aligned}\]

<p>第二项为未来奖励的折扣期望（推导中暂不关注折扣因子）</p>

\[\begin{aligned}
\mathbb{E}_\pi[G_{t+1}\vert S_{t}=s] &amp;= \sum_{s^\prime}\mathbb{E}_\pi[G_{t+1}\vert S_{t}=s, S_{t+1}=s^\prime]p(s^\prime\vert s)\\
&amp;= \sum_{s^\prime}\mathbb{E}_\pi[G_{t+1}\vert S_{t+1}=s^\prime]p(s^\prime\vert s)\quad (\text{Markov Property})\\
&amp;=\sum_{s^\prime}v_\pi(s^\prime)p(s^\prime\vert s)\\
&amp;= \sum_{s^\prime}v_\pi(s^\prime) \sum_a  p(s^\prime \vert s,a) \pi(a\vert s)\\
&amp;= \sum_{s^\prime} v_\pi(s^\prime)\sum_a\sum_r p(s^\prime,r \vert s,a)  \pi(a\vert s) \\
&amp;= \sum_a \pi(a\vert s) \sum_{s^\prime}\sum_r  p(s^\prime,r \vert s,a) v_\pi(s^\prime)
\end{aligned}\]

<p>将上述两项推导加和，则有贝尔曼（期望）方程推导如下</p>

\[\begin{aligned}
v_\pi(s) &amp;= \mathbb{E}_\pi[G_t \vert S_t=s]\\
&amp;=\mathbb{E}_\pi[R_{t+1}+ \gamma G_{t+1}\vert S_t=s]\\
&amp;=\sum_a \pi(a\vert s) \left[ \sum_{r} p(r \vert s,a) r+\gamma \sum_{s^\prime} p(s^\prime \vert s,a) v_\pi(s^\prime) \right]\\
&amp;=\sum_a \pi(a\vert s) \sum_{s^\prime}\sum_r p(s^\prime,r \vert s,a) [  r+\gamma v_\pi(s^\prime)  ] \\
&amp;=\sum_{a, s^\prime, r}\pi(a\vert s)p(s^\prime,r \vert s,a)\cdot [  r+\gamma v_\pi(s^\prime)  ]
\end{aligned}\]

<p>最后一行，通过将求和符号合并后，我们可以看出，上述状等式描述了一个关于三参数 $a\in A, s^\prime \in S, r\in R$ 在所有可能性上的求和。对于每一个三元组，我们计算出其概率 $\pi(a\vert s)p(s^\prime,r \vert s,a)$ 然后乘以方括号内的值作为权值，最后加权求和得到状态价值函数的期望。参考回溯图可以更好理解。</p>

<h4 id="452-矩阵化表述与迭代求解"><span class="me-2">4.5.2. 矩阵化表述与迭代求解</span><a href="#452-矩阵化表述与迭代求解" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>与马尔可夫奖励过程类似，将状态价值函数的贝尔曼方程矩阵化，首先列写贝尔曼方程如下</p>

\[v_\pi(s) =\sum_a \pi(a\vert s) \left[ \sum_{r} p(r \vert s,a) r+\gamma \sum_{s^\prime} p(s^\prime \vert s,a) v_\pi(s^\prime) \right]\]

<p>令</p>

\[\begin{aligned}
r_\pi(s) &amp;= \sum_a \pi(a\vert s)\sum_{r} p(r \vert s,a) r\\
p_\pi(s^\prime\vert s) &amp;= \sum_a \pi(a\vert s)\sum_{r} p(s^\prime \vert s, a)
\end{aligned}\]

<p>则对于任意状态 $s_i \in \mathcal{S}\in \mathbb{R}^n$ 和其后续状态 $s_j \in \mathcal{S}\in \mathbb{R}^n$，有</p>

\[v_\pi(s_i) = r_\pi(s) + \gamma \sum_{s_j}p_\pi(s_j\vert s_i)v_\pi(s_j)\]

<p>将所有状态写成矩阵形式，有</p>

\[\boldsymbol{V}_\pi = \boldsymbol{R}_\pi + \gamma \boldsymbol{P}_\pi \boldsymbol{V}_\pi\]

<p>其中</p>

\[\begin{aligned}
\boldsymbol{V}_\pi &amp;= [v_\pi(s_1),\cdots, v_\pi(s_n)]^\top \in \mathbb{R}^n\\
\boldsymbol{R}_\pi &amp;= [r_\pi(s_1),\cdots, r_\pi(s_n)]^\top \in \mathbb{R}^n\\
\boldsymbol{P}_\pi &amp;= [p_\pi(s_1\vert s_1),\cdots, p_\pi(s_1\vert s_n); \cdots; p_\pi(s_n\vert s_1),\cdots, p_\pi(s_n\vert s_n)] \in \mathbb{R}^{n\times n}
\end{aligned}\]

<p>则闭式解为</p>

\[\boldsymbol{V}_\pi^* = (\boldsymbol{I}-\gamma \boldsymbol{P}_\pi)^{-1}\boldsymbol{R}_\pi\]

<p>可通过迭代求解的方法规避矩阵求逆操作，即</p>

\[\boldsymbol{V}_\pi^{(k+1)} = \boldsymbol{R}_\pi + \gamma \boldsymbol{P}_\pi \boldsymbol{V}_\pi^{(k)}\]

<p>可证明，当 $k\rightarrow \infty$ 时，$V_\pi^{(k)}\rightarrow V_\pi^*$，证明如下：</p>

<p>定义误差</p>

\[\delta^k = \boldsymbol{V}_\pi^{(k)} - \boldsymbol{V}_\pi^*\]

<p>我们需要证明</p>

\[\delta^k \rightarrow 0\]

<p>有</p>

\[\begin{aligned}
  \boldsymbol{V}_\pi^{(k)} &amp;= \delta^k + \boldsymbol{V}_\pi^*\\
  \boldsymbol{V}_\pi^{(k+1)} &amp;= \delta^{k+1} + \boldsymbol{V}_\pi^*\\
\end{aligned}\]

<p>代入迭代形式的贝尔曼方程，有</p>

\[\begin{aligned}
\delta^{k+1} + \boldsymbol{V}_\pi^* &amp;= \boldsymbol{R}_\pi + \gamma \boldsymbol{P}_\pi (\delta^k + \boldsymbol{V}_\pi^*)\\
\delta^{k+1} &amp;= -\boldsymbol{V}_\pi^* + \boldsymbol{R}_\pi + \gamma \boldsymbol{P}_\pi \delta^k + \gamma \boldsymbol{P}_\pi \boldsymbol{V}_\pi^*\\
\end{aligned}\]

<p>注意到 $\boldsymbol{V}_\pi^*$ 同样也满足贝尔曼方程，因此</p>

\[\begin{aligned}
\delta^{k+1} &amp;= -\boldsymbol{V}_\pi^* + (\boldsymbol{R}_\pi + \gamma \boldsymbol{P}_\pi \boldsymbol{V}_\pi^*) + \gamma \boldsymbol{P}_\pi \delta^k\\
&amp;= \gamma \boldsymbol{P}_\pi \delta^k
\end{aligned}\]

<p>因此</p>

\[\delta^{k+1} = \gamma \boldsymbol{P}_\pi \delta^k = \gamma^2 \boldsymbol{P}_\pi^2 \delta^{k-1} = \cdots = \gamma^{k+1} \boldsymbol{P}_\pi^{k+1} \delta^0\]

<p>由于 $\gamma &lt; 1,\; \gamma^{k+1}\rightarrow 0$，且 $0\leq \boldsymbol{P}_\pi^{k} &lt; 1$（其每一行之和等于 $1$），所以当 $k\rightarrow \infty$ 时，$\delta^k \rightarrow 0$，证毕。</p>

<h4 id="453-状态-动作价值函数的贝尔曼方程"><span class="me-2">4.5.3. 状态-动作价值函数的贝尔曼方程</span><a href="#453-状态-动作价值函数的贝尔曼方程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>类似地，动作价值函数也有如下贝尔曼方程成立</p>

\[\begin{aligned}
q_\pi(s,a) &amp;= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} \vert S_t = s, A_t = a]\\
&amp;=\sum_{s^\prime,r}p(s^\prime,r \vert s,a) \left[r+\gamma \sum_{a^\prime}   \pi(a^\prime \vert s^\prime) q_\pi(s^\prime, a^\prime)  \right]
\end{aligned}\]

<h3 id="46-贝尔曼最优方程"><span class="me-2">4.6. 贝尔曼最优方程</span><a href="#46-贝尔曼最优方程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="461-最优策略"><span class="me-2">4.6.1. 最优策略</span><a href="#461-最优策略" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>强化学习的最终目标是寻找最优策略，<strong>最优策略是使得价值函数最大的策略</strong></p>

\[\pi^* \; \text{is optimal if}\; v_{\pi^*}(s)\geq v_\pi(s),\; \forall s\in\mathcal{S},\forall\pi\in\mathcal{\Pi}\]

<blockquote>
  <p>这个定义引出了许多问题：</p>
  <ol>
    <li>最优策略是否存在？因为定义里的最优策略非常理想，它比其他所有策略都要好，并且在所有状态上都能打败其它策略，那么是否存在这样的情况，最优策略在某些状态上能打败其它的策略，但是在某些状态上没法打败。</li>
    <li>最优策略是否唯一？（分别使状态价值函数最大，和使状态-动作价值函数最大，这两个策略等价么？如果等价那么唯一么？）</li>
    <li>最优策略是随机的还是确定的？</li>
    <li>如何获得最优策略？</li>
  </ol>
</blockquote>

<p>由于价值函数有两种，因此可分别定义如下两种最优策略：</p>

\[\begin{aligned}
\pi^* &amp;= \mathop{\text{argmax}}\limits_\pi \;v_\pi(s)\\
\pi^o &amp;= \mathop{\text{argmax}}\limits_\pi \;q_\pi(s,a)\\
\end{aligned}\]

<p>其中最大状态价值函数可定义为</p>

\[\begin{aligned}
v_*(s) &amp;\doteq \mathop{\text{max}}\limits_\pi \;v_\pi(s) = v_{\pi^*}(s)\geq v_\pi(s)\quad\forall s\in\mathcal{S},\forall\pi\in\mathcal{\Pi}\\
q_*(s,a) &amp;\doteq \mathop{\text{max}}\limits_\pi \;q_\pi(s,a) = q_{\pi^o}(s,a)\geq q_\pi(s,a)\quad\forall s\in\mathcal{S},\forall\pi\in\mathcal{\Pi}\\
\end{aligned}\]

<p>首先证明，对于不同的价值函数（即 $v_\pi(s)$ 和 $q_\pi(s,a)$），上述两种最优策略是等价的。</p>

<p>根据两个价值函数的关系有</p>

\[v_\pi(s) = \sum_{a}\pi(a\vert s)\cdot q_\pi(s,a) = \mathbb{E}[q_\pi(s,a)]\]

<p>对上式的策略统一取到令状态价值函数最大的最优策略，有</p>

\[\begin{aligned}
  v_{\pi^*}(s) &amp;= \max_a\sum_{a}\pi^*(a\vert s)\cdot q_\pi(s,a)\\
  &amp;=\max_a[\pi(a_1\vert s)q_\pi(s,a_1)+\cdots+\pi(a_n\vert s)q_\pi(s,a_n)]\\
\end{aligned}\]

<p>等式右边可以看作对所有状态-动作价值函数的加权和求最大值，若其中某个状态-动作价值函数最大，其权重必然为 $1$，才能保证上式取到最大值。此时对应的策略可以写为</p>

\[\pi^*(a\vert s) = \left\{
\begin{aligned}
  1&amp; \quad \text{if}\quad a=\text{argmax}_a q_{\pi^*}(s,a)\\
  0&amp; \quad \text{otherwise}
\end{aligned}
\right.\]

<p>我们发现，这个策略就是一个 $0-1$ 策略，对于不同的价值函数而言，最优策略是等价的</p>

\[\pi^* = \mathop{\text{argmax}}\limits_\pi \;q_\pi(s,a) = \pi^o\]

<h4 id="462-最优价值函数"><span class="me-2">4.6.2. 最优价值函数</span><a href="#462-最优价值函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>已知状态价值函数是动作价值函数的期望（加权平均）</p>

\[v_\pi(s) = \mathbb{E}_aq_\pi(s,a)\]

<p>那么当取最优策略时，其必然取到最大值，即</p>

\[v_*(s) = \mathop{\text{max}}\limits_a \; q_*(s,a)\]

<p>将最优状态价值函数代入前述【<strong>等式2</strong>】，得到最优状态-动作价值函数的表达式</p>

\[q_*(s,a) = \sum_{s^\prime,r}p(s^\prime,r \vert s,a) \left[r+\gamma v_*(s^\prime) \right]\]

<h4 id="463-贝尔曼最优方程"><span class="me-2">4.6.3. 贝尔曼最优方程</span><a href="#463-贝尔曼最优方程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>将最优状态价值函数的代入改写贝尔曼方程，得到贝尔曼最优方程为</p>

\[v_*(s) = \mathop{\text{max}}\limits_a \sum_{s^\prime}\sum_r p(s^\prime,r \vert s,a) [  r+\gamma v_*(s^\prime)  ]\]

<p>矩阵形式为</p>

\[\boldsymbol{V} = \max_\pi(\boldsymbol{R}_\pi + \gamma\boldsymbol{P}_\pi\boldsymbol{V})\]

<p>类似地 ，最优状态-动作价值函数对应的贝尔曼最优方程为</p>

\[q_*(s,a) = \sum_{s^\prime,r}p(s^\prime,r \vert s,a) \left[r+\gamma \mathop{\text{max}}\limits_a \; q_*(s^\prime,a^\prime) \right]\]

<p>贝尔曼最优方程给出了最优价值函数和最优策略的关系，即二者通过贝尔曼最优方程约束彼此，相互共同达到最优。但关于方程解的存在性和唯一性还有待进一步证明。</p>

<p>以状态价值函数的贝尔曼最优方程为例，定义贝尔曼算子：</p>

\[\mathcal{B}(\boldsymbol{V}) = \max_\pi(\boldsymbol{R}_\pi + \gamma\boldsymbol{P}_\pi\boldsymbol{V})\]

<p>其解（也即最优状态价值函数）是存在且唯一的，可通过不动点定理来证明。</p>

<blockquote>
  <ul>
    <li><strong>不动点</strong>（fix point）：若 $x\in X$ 满足 $f(x)=x$，其中 $f:X\rightarrow X$，则 $x$ 为一个不动点</li>
    <li><strong>压缩映射</strong>（contraction mapping）：若 $f:X\rightarrow X$ 满足 $\Vert f(x)-f(y)\Vert\leq \lambda\Vert y-x\Vert$，其中 $\lambda&lt;1$，则 $f$ 为一个压缩映射，其中 $d=\Vert \cdot \Vert$ 可以是任意向量范数</li>
    <li><strong>不动点定理</strong>：对于完备度量空间 $(X,d)$ 中任何形式的 $x=f(x)$ 方程，如果 $f$ 是压缩映射，那么满足：
      <ul>
        <li>存在性：存在一个不动点 $x^<em>$ 使得 $f(x^</em>)=x^*$</li>
        <li>唯一性：不动点是唯一存在的</li>
        <li>求解方法：考虑一个序列 $x_k$ 满足 $x_{k+1}=f(x_k)$，则 $x_k\rightarrow x^*, k\rightarrow \infty$，且收敛过程是呈指数级的</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>对于度量空间，我们使用 $L_\infty$ 范数</p>

\[\Vert \boldsymbol{V}\Vert_{\infty} = \max_i \vert V_i \vert\]

<p>根据此度量空间范数的定义，两个值函数之间的距离等于两个值函数向量各方向绝对值之差的最大值。同样，对于有限奖励的有限MDP，值函数将始终在实数空间中。因此，此有限空间是完备的。</p>

<p>定理：贝尔曼算子 $\mathcal{B}$ 是有限空间 $(X, L_\infty)$ 上的压缩映射。证明过程略，可参考《Mathematical Foundation of Reinforcement Learning》第 3.3.4 节。</p>

<p>上述定理保证了贝尔曼最优方程解的存在性和唯一性，并且保证可以通过迭代的形式求解得到最优价值函数（也即后文中的价值迭代（value iteration））。</p>

<h4 id="464-最优策略的求解"><span class="me-2">4.6.4. 最优策略的求解</span><a href="#464-最优策略的求解" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>通过迭代求解得到最优价值函数后，如何确定最优策略？我们根据迭代求解的价值函数的类别分别讨论：</p>

<ul>
  <li><strong>已知</strong> $v_{*}$</li>
</ul>

<p>为了求解最优策略，只需要做一步搜索就行。也就是对于不同的 $a\in A$，计算</p>

\[\pi_*(a\vert s) \leftarrow \mathop{\text{argmax}}\limits_a \sum_{s^\prime,r}p(s^\prime,r\vert s,a)[r+\gamma v_*(s^\prime)]\]

<p>就是我们的最优策略。为什么只需要一步搜索就行呢？因为 $v_*$ 已经考虑未来可能行为的回报。</p>

<ul>
  <li><strong>已知</strong> $q_{*}$</li>
</ul>

<p>已知 $q_*$ 就更直接了，只要取其中最大值对应的动作就是最优动作（策略）</p>

\[\pi_*(a\vert s) \leftarrow \mathop{\text{argmax}}\limits_a\; q_*(s,a)\]

<p>可以看到，如果已经得到了所有状态（或状态-动作）的最优价值函数，那么最优策略是很容易得到的。经过前面的分析我们指导，最优的状态（或状态-动作）价值函数可以通过对贝尔曼最优方程迭代来求解。具体来说，我们需要将强化学习分为两步，第一步解决一个<strong>预测问题</strong>，即给定状态、动作、奖励、状态转移概率，策略，预测出所有状态（或状态-动作）价值函数；第二步解决一个<strong>控制问题</strong>，即在预测问题的基础上，如何更新策略使得策略逐渐变得更优。</p>

<p>当状态转移概率（也即环境 $p$）已知时，预测问题可以通过 <strong>策略迭代（policy iteration）</strong> 或者 <strong>值迭代（value iteration）</strong> 的方式来先进行价值预测，再求解最优策略。这就是 <strong>动态规划方法（Dynamic Programming，DP）</strong>。</p>

<p>当状态转移概率（也即环境 $p$）未知时，可以通过 <strong>蒙特卡洛方法（Monte Carlo，MC）</strong> 进行价值预测。</p>

<h3 id="47-贝尔曼期望方程"><span class="me-2">4.7. 贝尔曼期望方程</span><a href="#47-贝尔曼期望方程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>从状态价值函数的定义出发</p>

\[v_\pi(s) = \mathbb{E}[R+\gamma G\vert S=s] = \mathbb{E}[R\vert S=s] + \gamma \mathbb{E}[G\vert S=s], \;s\in S\]

<p>其中</p>

\[\mathbb{E}[G\vert S=s] = \sum_a\pi(a\vert s)\sum_{s^\prime}p(s^\prime\vert s,a)v_\pi(s^\prime) = \mathbb{E}[v_\pi(s^\prime)\vert S=s]\]

<p>带回状态价值函数定义式，有</p>

\[v_\pi(s) = \mathbb{E}[R+\gamma v_\pi(s^\prime)\vert S=s], \;\forall s\]

<p>类似地，针对状态-动作价值函数，也有</p>

\[q_\pi(s,a) = \mathbb{E}[R+\gamma q_\pi(s^\prime,a^\prime)\vert S=s,A=a],\; \forall s,a\]

<p>上述两个公式合称为<strong>贝尔曼期望方程</strong>。</p>

<h2 id="5-参考文献"><span class="me-2">5. 参考文献</span><a href="#5-参考文献" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>[1] 知乎. <a href="https://www.zhihu.com/topic/20039099/intro">强化学习（Reinforcement Learning）</a>.</p>

<p>[2] ReEchooo. <a href="https://blog.csdn.net/qq_41773233/article/details/114698902">强化学习知识要点与编程实践（1）——马尔可夫决策过程</a></p>

<p>[3] ReEchooo. <a href="https://blog.csdn.net/qq_41773233/article/details/114435113">强化学习笔记（2）——马尔可夫决策过程</a></p>

<p>[4] Ping2021. <a href="https://zhuanlan.zhihu.com/p/494755866">第二讲 马尔可夫决策过程</a></p>

<p>[5] 木头人puppet. <a href="https://www.jianshu.com/p/9878238a1c9e">强化学习：贝尔曼方程和最优性</a></p>

<p>[6] koch. <a href="https://zhuanlan.zhihu.com/p/505723322">强化学习-贝尔曼方程和贝尔曼最优方程的推导</a></p>

<p>[7] Alvin. <a href="https://zhuanlan.zhihu.com/p/54728513">知乎：3.6 最优策略和最优值函数</a></p>

</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw me-1"></i>
    
      <a href='/categories/academic/'>Academic</a>,
      <a href='/categories/knowledge/'>Knowledge</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw me-1"></i>
      
      <a href="/tags/python/"
          class="post-tag no-text-decoration" >python</a>
      
      <a href="/tags/reinforcement-learning/"
          class="post-tag no-text-decoration" >reinforcement learning</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        本文由作者按照 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         进行授权

      
    </div>

    <!-- Post sharing snippet -->

<div class="share-wrapper">
  <span class="share-label text-muted me-1">分享</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%89%20-%20SIRLIS&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-markov-process%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%89%20-%20SIRLIS&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-markov-process%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-markov-process%2F&text=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%89%20-%20SIRLIS"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <i
      id="copy-link"
      class="fa-fw fas fa-link small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="分享链接"
      data-title-succeed="链接已复制！"
    >
    </i>
  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
      
    </div>
  </div>
  <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
    <div class="access">
      <!-- Get the last 5 posts from lastmod list. -->














  <div id="access-lastmod" class="post">
    <div class="panel-heading">最近更新</div>
    <ul class="post-content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Nonlinear-Classifier/">模式识别（非线性分类器）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Temporal-Differences/">强化学习（时序差分法）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Value-Approximation/">强化学习（值函数近似）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Bayes/">模式识别（贝叶斯决策）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-markov-process/">强化学习（马尔可夫决策过程）</a>
        </li>
      
    </ul>
  </div>
  <!-- #access-lastmod -->


      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>

    
      
      



  <div id="toc-wrapper" class="ps-0 pe-4 mb-5">
    <div class="panel-heading ps-3 pt-2 mb-2">文章内容</div>
    <nav id="toc"></nav>
  </div>


    
  </div>
</div>

<!-- tail -->

  <div class="row">
    <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-3 pe-xl-4 mt-5">
      
        
        <!--
  Recommend the other 3 posts according to the tags and categories of the current post,
  if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->








  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  






<!-- Fill with the other newlest posts -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ms-1" data-toc-skip>
      相关文章
    </h3>
    <div class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Dynamic-Programming/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1669433059"
  data-df="YYYY/MM/DD"
  
>
  2022/11/26
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（动态规划）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的动态规划法（Dynamic Programming，DP），采用动态规划的思想，分别介绍策略迭代和价值迭代方法。






  1. 强化学习问题的求解
  2. 动态规划
    
      2.1. 策略迭代
        
          2.1.1. 策略评估
          2.1.2. 策略改进
          2.1.3. 算法流程
   ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Monte-Carlo/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1669712839"
  data-df="YYYY/MM/DD"
  
>
  2022/11/29
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（蒙特卡洛法）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的 model-free 方法——蒙特卡洛法。






  1. 引言
  2. 蒙特卡洛法
    
      2.1. 大数定律与蒙特卡洛思想
      2.2. 蒙特卡洛基础算法
        
          2.2.1. 蒙特卡洛采样
          2.2.2. 蒙特卡洛价值估计
          2.2.3. 算法流程
        
 ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Temporal-Differences/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1671346759"
  data-df="YYYY/MM/DD"
  
>
  2022/12/18
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（时序差分法）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文首先引入了随机近似理论，然后通过比较动态规划和蒙特卡洛，引出结合二者优势的时序差分法。通过分析可知，时序差分法是随机近似理论的一个特例。随后详细介绍了同轨策略下的时序差分控制（SARSA）、离轨策略下的时序差分控制（Q-Learning）和期望SARSA。最后介绍了基于价值的深度强化学习方法：Deep Q-Network（DQN）。






  1. 引言
  2. 随机近似理论
 ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
    </div>
    <!-- .card-deck -->
  </div>
  <!-- #related-posts -->


      
        
        <!-- Navigation buttons at the bottom of the post. -->

<div class="post-navigation d-flex justify-content-between">
  
    <a
      href="/posts/windows-mingw64-assimp/"
      class="btn btn-outline-primary"
      prompt="上一篇"
    >
      <p>Windows环境下使用CMake+MinGW-w64编译模型加载库assimp</p>
    </a>
  

  
    <a
      href="/posts/reinforcement-learning-Dynamic-Programming/"
      class="btn btn-outline-primary"
      prompt="下一篇"
    >
      <p>强化学习（动态规划）</p>
    </a>
  
</div>

      
        
        <!--  The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script src="https://utteranc.es/client.js"
        repo="sirlis/sirlis.github.io"
        issue-term="pathname"
        crossorigin="anonymous"
        async>
</script>

<script type="text/javascript">
  $(function() {
    const origin = "https://utteranc.es";
    const iframe = "iframe.utterances-frame";
    const lightTheme = "github-light";
    const darkTheme = "github-dark";
    let initTheme = lightTheme;

    if ($("html[data-mode=dark]").length > 0
        || ($("html[data-mode]").length == 0
            && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
      initTheme = darkTheme;
    }

    addEventListener("message", (event) => {
      let theme;

      /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */
      if (event.origin === origin) {
        /* page initial */
        theme = initTheme;

      } else if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {
        /* global theme mode changed */
        const mode = event.data.message;
        theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme);

      } else {
        return;
      }

      const message = {
        type: "set-theme",
        theme: theme
      };

      const utterances = document.querySelector(iframe).contentWindow;
      utterances.postMessage(message, origin);
    });

  });
</script>



      
    </div>
  </div>


        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 post-content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>
    </div>

    <!-- The Footer -->

<footer>
  <div class="container px-lg-4">
    <div class="d-flex justify-content-center align-items-center text-muted mx-md-3">
      <p>本站采用 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 主题 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
      </p>

      <p>©
        2025
        <a href="https://github.com/sirlis">sirlis</a>.
        
          <span
            data-bs-toggle="tooltip"
            data-bs-placement="top"
            title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。"
          >保留部分权利。</span>
        
      </p>
    </div>
  </div>
</footer>


    <div id="mask"></div>

    <button id="back-to-top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow">
      <i class="fas fa-angle-up"></i>
    </button>

    
      <div
        id="notification"
        class="toast"
        role="alert"
        aria-live="assertive"
        aria-atomic="true"
        data-bs-animation="true"
        data-bs-autohide="false"
      >
        <div class="toast-header">
          <button
            type="button"
            class="btn-close ms-auto"
            data-bs-dismiss="toast"
            aria-label="Close"
          ></button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="px-2 mb-3">发现新版本的内容。</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            更新
          </button>
        </div>
      </div>
    

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.0/dist/jquery.min.js,npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.7/dayjs.min.js,npm/dayjs@1.11.7/locale/zh.min.js,npm/dayjs@1.11.7/plugin/relativeTime.min.js,npm/dayjs@1.11.7/plugin/localizedFormat.min.js,npm/tocbot@4.21.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '<div class="px-1 px-sm-2 px-lg-4 px-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

