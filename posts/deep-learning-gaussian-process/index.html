<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="zh-CN" 
  
>
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="深度学习基础（高斯过程）" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="本文介绍了高斯过程，包括高斯函数、多元高斯分布、高斯过程。" />
<meta property="og:description" content="本文介绍了高斯过程，包括高斯函数、多元高斯分布、高斯过程。" />
<link rel="canonical" href="http://localhost:4000/posts/deep-learning-gaussian-process/" />
<meta property="og:url" content="http://localhost:4000/posts/deep-learning-gaussian-process/" />
<meta property="og:site_name" content="SIRLIS" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-18T16:21:49+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="深度学习基础（高斯过程）" />
<meta name="twitter:site" content="@none" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-10-10T15:02:18+08:00","datePublished":"2021-01-18T16:21:49+08:00","description":"本文介绍了高斯过程，包括高斯函数、多元高斯分布、高斯过程。","headline":"深度学习基础（高斯过程）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/deep-learning-gaussian-process/"},"url":"http://localhost:4000/posts/deep-learning-gaussian-process/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>深度学习基础（高斯过程） | SIRLIS
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SIRLIS">
<meta name="application-name" content="SIRLIS">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.0/dist/tocbot.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/head.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <div class="site-title">
      <a href="/">SIRLIS</a>
    </div>
    <div class="site-subtitle fst-italic">分享科研和生活的日常</div>
  </div>
  <!-- .profile-wrapper -->

  <ul class="nav flex-column flex-grow-1 w-100 ps-0">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home"></i>
        <span>首页</span>
      </a>
    </li>
    <!-- the real tabs -->
    
      <li class="nav-item">
        <a href="/categories/" class="nav-link">
          <i class="fa-fw fas fa-stream"></i>
          

          <span>分类</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/tags/" class="nav-link">
          <i class="fa-fw fas fa-tags"></i>
          

          <span>标签</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/archives/" class="nav-link">
          <i class="fa-fw fas fa-archive"></i>
          

          <span>归档</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/about/" class="nav-link">
          <i class="fa-fw fas fa-info-circle"></i>
          

          <span>关于</span>
        </a>
      </li>
      <!-- .nav-item -->
    
  </ul>
  <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/sirlis"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/none"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['lihongjue','buaa.edu.cn'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</div>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container px-xxl-5">
        <!-- The Top Bar -->

<div id="topbar-wrapper">
  <div
    id="topbar"
    class="container d-flex align-items-center justify-content-between h-100"
  >
    <span id="breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                首页
              </a>
            </span>

          
        
          
        
          
            
              <span>深度学习基础（高斯过程）</span>
            

          
        
      
    </span>
    <!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      文章
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="搜索..."
      >
    </span>
    <span id="search-cancel">取消</span>
  </div>
</div>

        











<div class="row">
  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pe-xl-4">
    

    <div class="post px-1 px-md-2">
      

      
        
      
        <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->



  
  

  <!-- CDN URL -->
  

  <!-- Add image path -->
  

  
    
      
      
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  
    

    
    

    

    
    

    
    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    

    

    <!-- lazy-load images <https://github.com/aFarkas/lazysizes#readme> -->
    
    

    <!-- add image placeholder -->
    
      
    

    <!-- Bypass the HTML-proofer test -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        
      
    

    <!-- combine -->
    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  

  
  

  




<!-- return -->




<h1 data-toc-skip>深度学习基础（高斯过程）</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      发表于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1610958109"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2021/01/18
</em>

    </span>

    <!-- lastmod date -->
    
    <span>
      更新于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1696921338"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2023/10/10
</em>

    </span>
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      作者

      <em>
      
        <a href="https://github.com/sirlis">sirlis</a>
      
      </em>
    </span>

    <div>
      <!-- read time -->
      <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="7199 字"
>
  <em>39 分钟</em>阅读</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <p>本文介绍了高斯过程，包括高斯函数、多元高斯分布、高斯过程。</p>

<!--more-->

<hr />
<ul>
  <li><a href="#1-一元高斯分布">1. 一元高斯分布</a></li>
  <li><a href="#2-多元高斯分布">2. 多元高斯分布</a></li>
  <li><a href="#3-高斯过程">3. 高斯过程</a>
    <ul>
      <li><a href="#31-概念">3.1. 概念</a></li>
      <li><a href="#32-举例">3.2. 举例</a></li>
      <li><a href="#33-高斯过程回归">3.3. 高斯过程回归</a>
        <ul>
          <li><a href="#331-构建高斯过程先验">3.3.1. 构建高斯过程先验</a></li>
          <li><a href="#332-求解超参数">3.3.2. 求解超参数</a></li>
          <li><a href="#333-测试样本预测">3.3.3. 测试样本预测</a></li>
        </ul>
      </li>
      <li><a href="#34-深度核回归">3.4. 深度核回归</a>
        <ul>
          <li><a href="#341-初始化">3.4.1. 初始化</a></li>
          <li><a href="#342-前向传播">3.4.2. 前向传播</a></li>
          <li><a href="#343-反向传播">3.4.3. 反向传播</a></li>
          <li><a href="#344-预测">3.4.4. 预测</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#4-参考文献">4. 参考文献</a>
    <h1 id="1-一元高斯分布">1. 一元高斯分布</h1>
  </li>
</ul>

<p><strong>高斯分布又称正态分布。</strong></p>

<p>标准高斯函数为</p>

\[f(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\]

<p>函数图像为</p>

<p><a href="/assets/img/postsimg/20210118/1.png" class="popup img-link "><img data-src="/assets/img/postsimg/20210118/1.png" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<p>这个函数描述了变量 $x$ 的一种分布特性，变量 $x$ 的分布有如下特点：</p>

<ul>
  <li>均值 = 0</li>
  <li>方差 = 1</li>
  <li>概率密度和 = 1</li>
</ul>

<p>一元高斯函数的一般形式为</p>

\[f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\]

<p>这里指数函数的参数 $-\frac{1}{2\sigma^2}(x-\mu)^2$ 是一个关于 $x$ 的二项式函数。由于系数为负，所以是抛物线开口向下的函数。此外，由于最前面的系数与 $x$ 无关，因此可以把它当作是一个正规化因子（normalization factor），以保证</p>

\[\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}{exp(-\frac{1}{2\sigma^2}(x-\mu)^2)}dx=1\]

<!-- 函数图像为

![alt text](/assets/img/postsimg/20210118/2.png) -->

<p>若令</p>

\[z = \frac{x-\mu}{\sigma}\]

<p>称这个过程为标准化</p>

\[\begin{aligned}
  x &amp;= z\cdot \sigma + \mu\\
  \Rightarrow p(x) &amp;= \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(z)^2}\\
  \Rightarrow 1 &amp;=\int_{-\infty}^{\infty}{p(x)dx}\\
  &amp;=\int_{-\infty}^{\infty}{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(z)^2}dx}\\
  &amp;=\int_{-\infty}^{\infty}{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(z)^2}\sigma\cdot dz}\\
  &amp;=\int_{-\infty}^{\infty}{\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(z)^2} dz}\\
\end{aligned}\]

<p>即 $z\sim N(0,1)$。</p>

<p>随机变量 $x$ 标准化的过程, 实际上的消除量纲影响和分布差异的过程. 通过将随机变量的值减去其均值再除以标准差, 使得随机变量与其均值的差距可以用若干个标准差来衡量, 从而实现了不同随机变量与其对应均值的差距, 可以以一种相对的距离来进行比较。</p>

<!-- 唯一不太好理解的是前面的系数， 为什么多了一个 $\sigma$， 不是 $2\sigma$  或其他。直观理解如下图

![alt text](/assets/img/postsimg/20210118/3.png)

实线代表的函数是标准高斯函数，虚线代表的是标准高斯函数在 $x$ 轴方向 2 倍延展，效果如下

$$
\begin{aligned}
A(x = 1) &= D(x = 2)\\
E(x = 1.5) &= F(x = 3)\\
G(x = 2) &= H(x = 4)\\
\end{aligned}
$$

横向拓宽了，纵向还是保持不变，可以想象，最后的函数积分肯定不等于 1。区域的面积可以近似采用公式：$面积 = 底 × 高$ 求得：从 $AQRS -> DTUV$， 底乘以 2 倍，高维持不变，所以，要保持变化前后面积不变，函数的高度应该变为原来的 1/2。所以高斯函数在 $x$ 轴方向做 2 倍延展的同时，纵向应该压缩为原来的一半，才能重新形成新的高斯分布函数

扩展到一般情形，$x$ 轴方向做 $\sigma$ 倍延拓的同时， $y$ 轴应该压缩 $\sigma$ 倍（乘以 $1/\sigma$ ） -->

<h1 id="2-多元高斯分布">2. 多元高斯分布</h1>

<blockquote>
  <p>钱默吟. <a href="https://zhuanlan.zhihu.com/p/58987388">多元高斯分布完全解析</a></p>
</blockquote>

<p>多元高斯分布是一元高斯分布在向量形式的推广。</p>

<p>假设随机向量 $\boldsymbol Z = [z_1,\cdots,z_n]$，其中 $z_i\sim \mathcal N(0,1)(i=1,\cdots,n)$ 且彼此独立，则随机向量的联合概率密度函数为</p>

\[\begin{aligned}
  p(z_1, \cdots, z_n) &amp;= \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(z_i)^2}\\
  &amp;=\frac{1}{2\pi^{n/2}}e^{-1/2\cdot Z^TZ}\\
1&amp;=\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty} p(z_1,\cdots,z_n)dz_1\cdots dz_n
\end{aligned}\]

<p>称随机向量 $\boldsymbol Z\sim \mathcal N(\boldsymbol 0,\boldsymbol I)$，即服从均值为零向量，协方差矩阵为单位矩阵的高斯分布。</p>

<p>对于向量 $X=[x_1,\cdots,x_n]$，其概率密度函数的形式为</p>

\[\begin{aligned}
p(x_1,x_2,\cdots,x_n) &amp;=\prod_{i=1}^n p(x_i)\\
&amp;=\frac{1}{(2\pi)^{n/2}\sigma_1\cdots\sigma_n}exp\left( -\frac{1}{2} \left[ \frac{(x_1-\mu_1)^2}{\sigma^2_1}+\cdots+\frac{(x_n-\mu_n)^2}{\sigma^2_n} \right] \right)\\
\end{aligned}\]

<p>其中 $\mu_i, \sigma_i$ 为第 $i$ 维的均值和方差。按照矩阵表示</p>

\[\begin{aligned}
  \boldsymbol x - \boldsymbol \mu &amp;= [x_1-\mu_1,\cdots,x_n-\mu_n]^T\\
  \Sigma &amp;= \left[
    \begin{matrix}
      \sigma_1^2&amp;0&amp;\cdots&amp;0\\
      0&amp;\sigma_2^2&amp;\cdots&amp;0\\
      \vdots&amp;\vdots&amp;\ddots&amp;0\\
      0&amp;0&amp;\cdots&amp;\sigma_n^2\\
    \end{matrix}
    \right]
\end{aligned}\]

<p>那么有</p>

\[\begin{aligned}
\sigma_1\cdots\sigma_n &amp;= \vert\Sigma\vert^\frac{1}{2}\\
\frac{(x_1-\mu_1)^2}{\sigma^2_1}+\cdots+\frac{(x_n-\mu_n)^2}{\sigma^2_n} &amp;= (\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)
\end{aligned}\]

<p>代入得</p>

\[\begin{aligned}
p(x_1,x_2,\cdots,x_n) &amp;=p(\boldsymbol x\vert \boldsymbol \mu, \Sigma)\\
&amp;= \frac{1}{(2\pi)^{n/2}\vert\Sigma\vert^{1/2}}exp(-\frac{1}{2}(\boldsymbol x-\boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x-\boldsymbol \mu))\\
\end{aligned}\]

<p>则称 $X$ 为具有均值 $\boldsymbol \mu \in \mathbb R^n$，协方差矩阵为 $\Sigma \in S^n$ 的多元高斯分布。</p>

<h1 id="3-高斯过程">3. 高斯过程</h1>

<h2 id="31-概念"><span class="me-2">3.1. 概念</span><a href="#31-概念" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>首先简单理解高斯过程，比如你有 $(t_1,t_2,\cdots,t_N)=\boldsymbol T$ 个时间点，每个时间点的观测值都是高斯分布的，并且任意 $k$ 个时间点的观测值的组合都是联合高斯分布。这样的一个过程称为高斯过程。高斯过程通常可以用来表示一个<strong>函数的分布</strong>。</p>

<p>高斯过程，从字面上分解，我们就可以看出他包含两部分：</p>
<ul>
  <li>高斯，指的是高斯分布</li>
  <li>过程，指的是随机过程</li>
</ul>

<blockquote>
  <p>当随机变量是 1 维时，我们称之为一维高斯分布，概率密度函数 $p(x)=N(\mu,\sigma^2)$
当随机变量是有限的 $p$ 维时，我们称之为高维高斯分布， $p(x) = N(\mu, \Sigma_{p \times p})$
当随机变量是连续域上的无限多个高斯随机变量组成的随机过程，称之为无限维的高斯分布，即高斯过程</p>
</blockquote>

<p>通常如果我们要学习一个函数（或者说学习一个映射），首先定义函数的参数，然后根据训练数据来学习这个函数的参数。例如我们做线性回归，学习这样一个函数就相当于训练回归参数（权重、偏置）。这种方法叫做<strong>参数化的方法</strong>。但是这种做法就把可学习的函数的范围限制死了，无法学习任意类型的函数。非参数化的方法就没有这个缺点。用高斯过程来建模函数，就是一种<strong>非参数方法</strong>。</p>

<h2 id="32-举例"><span class="me-2">3.2. 举例</span><a href="#32-举例" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p><strong>举一个简单的例子</strong>，下面的图中，横轴 $T$ 是一个关于时间的连续域，表示人的一生，而纵轴表示的是体能值 $\xi$。对于一个人而言，在任意不同的时间点体能值都服从正态分布，但是不同时间点分布的均值和方差不同。一个人的一生的体能曲线就是一个<strong>函数</strong>（体能关于时间的函数），该函数的分布就是高斯过程。</p>

<p><a href="/assets/img/postsimg/20210118/3.1.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20210118/3.1.jpg" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<p>对于任意 $t\in T, \xi_t \sim N(\mu_t,\sigma_t^2)$ ，也就是对于一个确定的高斯过程而言，对于任意时刻 $t$ ，他的 $\mu_t$ 和 $\sigma_t$ 都已经确定了。而像上图中，我们对同一人体能值在关键节点进行采样，然后平滑连接，也就是图中的两条虚线，就形成了这个高斯过程中的两个样本。</p>

<p>回顾 $p$ 维度高斯分布，决定他的分布是两个参数，一个是 $p$ 维的均值向量 $\mu_p$ ，他反映了 $p$ 维高斯分布中每一维随机变量的期望，另一个就是 $p\times p$ 的协方差矩阵 $\Sigma_{p\times p}$ ，他反映了高维分布中，每一维自身的方差，以及不同维度之间的协方差。</p>

<p>定义在连续域 $T$ 上的高斯过程其实也是一样，他是无限维的高斯分布，他同样需要描述每一个时间点 $t$ 上的均值，但是这个时候就不能用向量了，因为是在连续域上的，维数是无限的，因此就应该定义成一个关于时刻 $t$ 的<strong>函数</strong> $m(t)$。</p>

<p>协方差矩阵也是同理，无限维的情况下就定义为一个<strong>核函数</strong> $k(t_i,t_j)$ ，其中 $t_i$ 和 $t_j$ 表示任意两个时刻。核函数也称协方差函数，是一个高斯过程的核心，他决定了高斯过程的性质。在研究和实践中，核函数有很多种不同的类型，他们对高斯过程的衡量方法也不尽相同，最为常见的一个核函数是径向基函数，其定义如下：</p>

\[k_\lambda(t_i,t_j)=\sigma^2 exp(-\frac{\vert\vert t_i-t_j\vert\vert^2}{2l^2})\]

<p>$\sigma$ 和 $l$ 是径向基函数的超参数，是我们提前可以设置好的。径向基函数输出的是一个标量，他代表的就是两个时间点各自所代表的高斯分布之间的协方差值，很明显径向基函数是一个关于距离 $\vert\vert x_i-x_j\vert\vert$ 负相关的函数，两个点距离越大，两个分布之间的协方差值越小，即相关性越小，反之越靠近的两个时间点对应的分布其协方差值就越大。</p>

<p>由此，高斯过程的两个核心要素：均值函数和核函数的定义我们就描述清楚了，按照高斯过程存在性定理，一旦这两个要素确定了，那么整个高斯过程就确定了：</p>

\[\xi_t \sim GP(m(t),k(t_i,t_j))\]

<hr />

<p><strong>另一个简单的例子</strong>，假设我们有<strong>两个点</strong> $x_0=0$ 和 $x_1=1$ ，对应这两个点的函数值服从二维高斯分布（高斯过程中“高斯”二字的由来）</p>

\[\begin{aligned}
\left(
  \begin{matrix}
  y_0\\
  y_1
  \end{matrix}
\right)
\sim \mathcal N
\left(
  \begin{matrix}
  \left(
  \begin{matrix}
  0\\
  1
  \end{matrix}
  \right),
  \left(
  \begin{matrix}
  1&amp;0\\
  0&amp;1
  \end{matrix}
  \right)
  \end{matrix}
\right)
\end{aligned}\]

<p>从这个二维高斯分布中采样 10 组数据，其中两个点在 $x$ 轴上的两端，采样得到的两个 $y$ 对应在 $y$ 轴取值，可以得到下图所示的结果</p>

<p><a href="/assets/img/postsimg/20210118/4.png" class="popup img-link "><img data-src="/assets/img/postsimg/20210118/4.png" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<p>每条直线可以被认为是从<strong>一个线性函数分布</strong>中采样出来的线性函数。</p>

<p>如果我们有<strong>20个 $x$ 点</strong>，对应这 20 个 $x$ 的函数值符合均值为 0，协方差矩阵为单位矩阵的联合高斯分布。和上面一样采样 10 组数据，得到下图</p>

<p><a href="/assets/img/postsimg/20210118/5.png" class="popup img-link "><img data-src="/assets/img/postsimg/20210118/5.png" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<p>每一条线都是一个函数，是从某个<strong>函数分布</strong>中采样得到的。但是这样的函数看上去一点也不平滑，并且显得杂乱无章，距离很近的两个 $x$ 对应的函数值 $y$ 可以相差很大。</p>

<p>直观来说，两个 $x$ 离得越近，对应的函数值应该相差越小，也就是说这个函数应该是平滑的，而不是像上图那样是突变的。所以我们应该通过两个 $x$ 之间的某种距离来定义这两个 $x$ 对应的函数值之间的协方差。两个 $x$ 离得越近，对应函数值之间的协方差应该越大，意味着这两个函数值的取值可能越接近。</p>

<p>我们引入核函数（以高斯核为例，也可以用其他核，并不是说我们在讲高斯过程所以这里就一定用高斯核）：</p>

\[k_\lambda(x_i,x_j)=exp(-\frac{\vert\vert \boldsymbol x_i-\boldsymbol x_j\vert\vert^2}{\lambda})\]

<p>和函数可以表示两个点 $x_i,x_j$ 之间的距离。此时，若我们有N个数据点( $x_1,\cdots,x_N$ )，则这个 $N$ 个数据点对应的 $N$ 个函数值服从 $N$ 维高斯分布，这个高斯分布的均值是 0，协方差矩阵是 $K$，$K$ 里的每一个元素对应</p>

\[K_{nm} = k(\boldsymbol x_n,\boldsymbol x_m)\]

<p>此时，再以刚才 20 个数据点的情况为例，我们采样 10 组，得到下图，现在看起来函数就平滑多了</p>

<p><a href="/assets/img/postsimg/20210118/6.png" class="popup img-link "><img data-src="/assets/img/postsimg/20210118/6.png" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<p>如果数据点再多点，例如 100 个数据点，则采样 10 组，得到下图：</p>

<p><a href="/assets/img/postsimg/20210118/7.png" class="popup img-link "><img data-src="/assets/img/postsimg/20210118/7.png" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<p>上图每条曲线就是一个高斯过程的采样，每个数据点上的函数值都是高斯分布。且任意k个数据点对应的函数值的组合都是联合高斯分布。</p>

<h2 id="33-高斯过程回归"><span class="me-2">3.3. 高斯过程回归</span><a href="#33-高斯过程回归" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>高斯过程回归可以看作是一个根据先验与观测值推出后验的过程。一版遵循以下三步</p>

<ul>
  <li>构建高斯过程先验</li>
  <li>求解超参数</li>
  <li>对测试样本进行预测</li>
</ul>

<h3 id="331-构建高斯过程先验"><span class="me-2">3.3.1. 构建高斯过程先验</span><a href="#331-构建高斯过程先验" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>假设一组 $n$ 个观测值，每个观测值为 $D$ 维向量 $\boldsymbol X={\boldsymbol x_1, \cdots, \boldsymbol x_n}$，对应的值为 $n$ 个 1 维目标向量 $\boldsymbol Y={y_1,\cdots, y_n}$。假设回归残差 $\boldsymbol \varepsilon=[\varepsilon_1,\cdots,\varepsilon_n]$ 服从 $iid$ 正态分布 $p(\varepsilon)=\mathcal N(0,\sigma^2_{noise})$，则回归问题就是希望我们通过 $\boldsymbol X,\boldsymbol Y$ 学习一个由 $\boldsymbol X$ 到 $\boldsymbol Y$ 的映射函数 $f$</p>

\[\boldsymbol Y=f(\boldsymbol X)+\boldsymbol \varepsilon,\quad where\quad \varepsilon_i\sim \mathcal N(0,\sigma^2_{noise}), i=1,\cdots,n\]

<p>未知映射 $f$ 遵循高斯过程，通过 $\boldsymbol \mu = [\mu(x_1),\cdots,\mu(x_n)]$ 与 $k(\boldsymbol x_i,\boldsymbol x_j)$ 定义一个高斯过程，但是因为此时没有任何观测值，所以这是一个先验。</p>

\[f(\boldsymbol X) \sim \mathcal{GP}[\boldsymbol \mu,\boldsymbol K(\boldsymbol X, \boldsymbol X)]\]

<blockquote>
  <p>注意：经典高斯过程输入可以是多维，但输出只有 1 维（即单输出）。如果需要多维输出，当输出分量之间不相关时，可以分别设计多个高斯过程模型进行回归。当输出分量之间相关时，可以参考一些paper和工具包来实现，比如：
https://github.com/SheffieldML/multigp
https://github.com/SheffieldML/GPy/blob/devel/GPy/models/gp_multiout_regression.py</p>
</blockquote>

<p>高斯过程由其数学期望 $\boldsymbol \mu$ 和协方差函数 $\boldsymbol K$ 完全决定。常见的选择是平稳高斯过程，即数学期望为一<strong>常数</strong>，协方差函数取平稳高斯过程可用的核函数。</p>

<p>高斯过程的均值函数决定着曲线的走势，常数均值相当于起了一个平移作用，均值函数不再是常数时，曲线将围绕着均值函数这条曲线而波动。</p>

<p>$\mu = 1$ 时</p>

<p><a href="/assets/img/postsimg/20210118/8.png" class="popup img-link "><img data-src="/assets/img/postsimg/20210118/8.png" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<p>$\mu = 2x$ 时</p>

<p><a href="/assets/img/postsimg/20210118/9.png" class="popup img-link "><img data-src="/assets/img/postsimg/20210118/9.png" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<p>但是一般情况下，我们都会对数据集的输出进行标准化处理来达到去均值的目的，这样做的好处就是我们只需要设置 $\boldsymbol \mu=\boldsymbol 0$ 即可，而无需猜测输出大致的模样，并且在后面的超参数寻优的过程中也可以减少我们需要优化的超参数的个数。</p>

<p>使用最多的核函数是 <strong>RBF 核</strong>。</p>

<h3 id="332-求解超参数"><span class="me-2">3.3.2. 求解超参数</span><a href="#332-求解超参数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>高斯过程回归的求解也被称为超参学习（hyper-parameter learning），是按照贝叶斯方法通过学习样本确定核函数的超参数 $\boldsymbol \theta$ 的过程。根据贝叶斯定理，高斯过程回归的超参数的后验表示如下</p>

\[p(\boldsymbol \theta \vert \boldsymbol X,\boldsymbol Y) = \frac{p(\boldsymbol Y\vert \boldsymbol X,\boldsymbol \theta)p(\boldsymbol \theta)}{p(\boldsymbol Y\vert \boldsymbol X)}\]

<p>其中，$\boldsymbol \theta$ 包括核函数的超参数和残差的方差 $\sigma^2_{noise}$。</p>

<p>$p(\boldsymbol Y\vert\boldsymbol X,\boldsymbol \theta)$ 是似然，是对高斯过程回归的输出边缘化得到的边缘似然：</p>

\[p(\boldsymbol Y\vert\boldsymbol X,\boldsymbol \theta) = \int p(\boldsymbol Y\vert f, \boldsymbol X,\boldsymbol \theta)p(f\vert\boldsymbol X,\boldsymbol \theta)df\]

<p>高斯分布的边缘分布也是高斯分布，因此边缘似然也服从高斯分布，则概率可写为</p>

\[p(\boldsymbol Y\vert\boldsymbol X,\boldsymbol \theta) = \frac{1}{(2\pi)^{n/2}\vert\Sigma\vert^{1/2}}exp(-\frac{1}{2}(\boldsymbol Y-\mu(\boldsymbol X))^T\Sigma^{-1}(\boldsymbol Y-\mu(\boldsymbol X)))\]

<p>采用最大似然估计来对高斯过程的超参数进行估计，则负对数似然函数为</p>

\[\begin{aligned}
 \Rightarrow L = -{\rm ln}\ p(\boldsymbol Y\vert\boldsymbol X,\boldsymbol \theta) &amp;= -[-\frac{1}{2}{\rm ln}{\vert\Sigma\vert}-\frac{n}{2}{\rm ln}(2\pi)-\frac{1}{2}(\boldsymbol Y-\mu(\boldsymbol X))^T\Sigma^{-1}(\boldsymbol Y-\mu(\boldsymbol X))]\\
&amp;= \frac{1}{2}{\rm ln}{\vert\Sigma\vert}+\frac{n}{2}{\rm ln}(2\pi)+\frac{1}{2}\boldsymbol Y^T\Sigma^{-1}\boldsymbol Y\quad \boldsymbol (\boldsymbol \mu = 0)
\end{aligned}\]

<p>上式第一项仅与回归模型有关，回归模型的核矩阵越复杂其取值越高，反映了模型的结构风险（structural risk）。第三项包含学习成本，是数据拟合项，表示模型的经验风险（empirical risk）。</p>

<p>其中，$\Sigma=k(\boldsymbol X,\boldsymbol X)+\sigma^2_{noise}\boldsymbol I$，与超参数 $\boldsymbol \theta$ 有关。利用梯度下降的方法更新超参数，上述式子对超参数 $\boldsymbol \theta$ 求导</p>

\[\begin{aligned}
\frac{\partial L}{\partial \boldsymbol \theta} = \frac{\partial {\rm ln}\ p(\boldsymbol Y\vert\boldsymbol X,\boldsymbol \theta)}{\partial \boldsymbol \theta} &amp;= 
-\frac{1}{2}tr(\Sigma^{-1}\frac{\partial \Sigma}{\partial \boldsymbol \theta}) + \frac{1}{2}(\boldsymbol Y-\mu(\boldsymbol X))^T\Sigma^{-1}\frac{\partial \Sigma}{\partial \boldsymbol \theta}(\boldsymbol Y-\mu(\boldsymbol X))\\
&amp;=-\frac{1}{2}tr(\Sigma^{-1}\frac{\partial \Sigma}{\partial \boldsymbol \theta}) + \frac{1}{2}\boldsymbol Y^T\Sigma^{-1}\frac{\partial \Sigma}{\partial \boldsymbol \theta}\boldsymbol Y\quad \boldsymbol (\boldsymbol \mu = 0)
\end{aligned}\]

<p>注意，高斯过程回归中的目标函数不是凸的，因而带来的问题就是：通过求解这个最优化问题，我们可以得到的只能是一个局部最小值，而非真正的全局最小值。局部最小不能保证是全局最优时，初始值的选择变得非常的重要。因为一个初始值可能会走向一个具体的极小值，而不同的初始值或许可以得到不同最优值。一种解决方案就是，多次产生不同的初始超参数，然后操作一次最优化问题的求解，然后比较这些最优化，挑选得出其中最小的值。</p>

<h3 id="333-测试样本预测"><span class="me-2">3.3.3. 测试样本预测</span><a href="#333-测试样本预测" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>接着，给定其它 $m$ 个测试观测值 $\boldsymbol X^<em>$，预测 $\boldsymbol Y^</em>=f(\boldsymbol X^*)$ 。</p>

<p>我们希望对 $\boldsymbol Y^<em>\vert \boldsymbol Y$ 的条件概率 $p(\boldsymbol Y^</em>\vert \boldsymbol X, \boldsymbol Y,\boldsymbol X^<em>)$ 进行建模，即给定观察到的所有数据 $\boldsymbol X, \boldsymbol Y,\boldsymbol X^</em>$，确定预测样本的预测值 $\boldsymbol Y^*$ 的分布。</p>

<p>高斯过程并没有直接对这个条件概率分布进行建模，而是从联合概率分布出发，对 $\boldsymbol Y^<em>$ 的联合概率 $p(\boldsymbol Y,\boldsymbol Y^</em>\vert\boldsymbol X,\boldsymbol X^*)$ 进行建模。当这个概率已知时，可以通过下面的式子得到新样本预测值的条件概率</p>

\[p(\boldsymbol Y^*\vert\boldsymbol X,\boldsymbol Y,\boldsymbol X^*)
= \frac{p(\boldsymbol Y,\boldsymbol Y^*\vert \boldsymbol X,\boldsymbol X^*)}{p(\boldsymbol Y\vert \boldsymbol X,\boldsymbol X^*)}
= \frac{p(\boldsymbol Y,\boldsymbol Y^*\vert \boldsymbol X,\boldsymbol X^*)}{\int _{Y^*}p(\boldsymbol Y,\boldsymbol Y^*\vert \boldsymbol X,\boldsymbol X^*)dY^*}\]

<p>显然，核心是建立 $p(\boldsymbol Y,\boldsymbol Y^<em>\vert \boldsymbol X,\boldsymbol X^</em>)$ 的具体形式，然后通过上式求出关于 $\boldsymbol Y^*$ 的条件概率。</p>

<p>根据回归模型核高斯过程的定义，$\boldsymbol Y$ 和 $\boldsymbol Y^*$ 的概率分布为</p>

\[\begin{aligned}
\boldsymbol Y&amp;\sim \mathcal N(\mu(\boldsymbol X),k(\boldsymbol X, \boldsymbol X)+\sigma^2_{noise}\boldsymbol I)\\
\boldsymbol Y^* &amp;\sim \mathcal N(\mu(\boldsymbol X^*),k(\boldsymbol X^*, \boldsymbol X^*))
\end{aligned}\]

<p>二者的联合分布满足无限维高斯分布</p>

\[\begin{aligned}
  \left[\begin{matrix}
    \boldsymbol Y\\\boldsymbol Y^*
  \end{matrix}\right]
  \sim
  N(
  \left[\begin{matrix}
    \mu(\boldsymbol X)\\\mu(\boldsymbol X^*)
  \end{matrix}\right],
  \left[\begin{matrix}
    k(\boldsymbol X,\boldsymbol X)+\sigma^2_{noise} \boldsymbol I &amp; k(\boldsymbol X,\boldsymbol X^*)\\ k(\boldsymbol X^*,\boldsymbol X)&amp;k(\boldsymbol X^*,\boldsymbol X^*)
  \end{matrix}\right]
  )
\end{aligned}\]

<p>从这个联合分布中派生出来的条件概率 $\boldsymbol Y^*\vert \boldsymbol Y$ 同样也服从无限维高斯分布。套用高维高斯分布的公式</p>

\[\begin{aligned}
  \boldsymbol Y^*\vert \boldsymbol Y &amp;\sim N(\mu^*,k^*) \Rightarrow p(\boldsymbol Y^*\vert \boldsymbol X,\boldsymbol Y,\boldsymbol X^*)= N(\mu^*,k^*)\\
  \mu^* &amp;= k(\boldsymbol X^*,\boldsymbol X)[k(\boldsymbol X,\boldsymbol X)+\sigma^2_{noise}\boldsymbol I]^{-1}(\boldsymbol Y-\mu(\boldsymbol X))+\mu(\boldsymbol X^*)\\
  &amp;= k(\boldsymbol X^*,\boldsymbol X)k(\boldsymbol X,\boldsymbol X)^{-1}\boldsymbol Y\\
  k^* &amp;= k(\boldsymbol X^*,\boldsymbol X^*)-k(\boldsymbol X^*,\boldsymbol X)[k(\boldsymbol X,\boldsymbol X)+\sigma^2_{noise}\boldsymbol I]^{-1}k(\boldsymbol X,\boldsymbol X^*)\\
  &amp;=k(\boldsymbol X^*,\boldsymbol X^*)-k(\boldsymbol X^*,\boldsymbol X)k(\boldsymbol X,\boldsymbol X)^{-1}k(\boldsymbol X,\boldsymbol X^*)
\end{aligned}\]

<p>均值 $\mu^<em>$ 实际上是观测点 $\boldsymbol Y^</em>$ 的一个线性函数。</p>

<p>协方差项 $k^*$ 的第一部分是我们的先验的协方差，减掉的后面的那一项实际上表示了观测到数据后函数分布不确定性的减少。如果第二项非常接近于 0，说明观测数据后我们的不确定性几乎不变，反之如果第二项非常大，则说明不确定性降低了很多。</p>

<blockquote>
  <p><strong>PS1：</strong>
高斯分布有一个很好的特性，即高斯分布的联合概率、边缘概率、条件概率仍然是满足高斯分布的，假设 $n$ 维随机变量满足高斯分布  $\boldsymbol x \sim N(\mu,\Sigma_{n\times n})$</p>

  <p>把随机变量分成两部分：$p$ 维 $\boldsymbol x_a$ 和 $q$ 维 $\boldsymbol x_b$，满足 $n=p+q$，按照分块规则可以写成
\(\begin{aligned}
  x=\left[\begin{matrix}
    x_a\\x_b
  \end{matrix}\right],
  \mu=\left[\begin{matrix}
    \mu_a\\\mu_b
  \end{matrix}\right],
  \Sigma=\left[\begin{matrix}
    \Sigma_{aa} &amp; \Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}
  \end{matrix}\right]
\end{aligned}\)
则下列条件分布依然是高维高斯分布
\(\begin{aligned}
x_b\vert x_a &amp;\sim N(\mu_{b\vert a},\Sigma_{b\vert a})\\
\mu_{b\vert a} &amp;= \Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a)+\mu_b\\
\Sigma_{b\vert a} &amp;= \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\end{aligned}\)
由此可推广到高斯过程。</p>
</blockquote>

<p>下图是高斯过程的可视化，其中蓝线是高斯过程的均值，浅蓝色区域 95% 置信区间（由协方差矩阵的对角线得到），每条虚线代表一个函数采样（这里用了 100 维模拟连续无限维）。左上角第一幅图是高斯过程的先验（这里用了零均值作为先验），后面几幅图展示了当观测到新的数据点的时候，高斯过程如何更新自身的均值函数和协方差函数。</p>

<p><a href="/assets/img/postsimg/20210118/10.gif" class="popup img-link "><img data-src="/assets/img/postsimg/20210118/10.gif" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<h2 id="34-深度核回归"><span class="me-2">3.4. 深度核回归</span><a href="#34-深度核回归" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<blockquote>
  <p>Andrew Gordon Wilson, et al. Deep Kernel Learning. 2016.</p>
</blockquote>

<p>数据集如下，输入 $n$ 个 $D$ 维数据 $\boldsymbol X=[\boldsymbol x_1,\cdots,\boldsymbol x_n]$，输出 $n$ 个 1 维数据 $\boldsymbol Y = [y(\boldsymbol x_1),\cdots,y(\boldsymbol x_n)]$。</p>

<p>网络结构如下：</p>

<p><a href="/assets/img/postsimg/20210401/03.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20210401/03.jpg" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<p>$n&lt;6000$ 时网络为 $[D-1000-500-50-2-gp]$ 结构，$n\leq 6000$ 时网络为 $[D-1000-1000-500-50-2-gp]$ 结构。</p>

<p>其中前面为全连接的 MLP，输入 $D$ 维数据，输出 2 维特征，最后一层为高斯过程回归层，</p>

<h3 id="341-初始化"><span class="me-2">3.4.1. 初始化</span><a href="#341-初始化" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="n">NNRegressor</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
<span class="o">|--</span><span class="nf">first_run</span><span class="p">()</span>
    <span class="o">|--</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">initialize_ws</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></div></div>

<ul>
  <li><strong>全连接层</strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Dense()</code>，初始化为</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">initialize_ws</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
  <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_inp</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">n_out</span><span class="p">)</span><span class="o">*</span><span class="n">numpy</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">n_inp</span><span class="p">)</span>
  <span class="n">self</span><span class="p">.</span><span class="n">b</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">n_out</span><span class="p">))</span>
  <span class="n">self</span><span class="p">.</span><span class="n">dW</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">n_inp</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">n_out</span><span class="p">))</span>
  <span class="n">self</span><span class="p">.</span><span class="n">db</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">n_out</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></div></div>

<p>即</p>

\[\begin{aligned}
w&amp;\sim N(0,\sqrt{\frac{1}{D}}) \in \mathbb R^{D_i\times D_o}\\
b &amp;= [0,\cdots,0] \in \mathbb R^{D_o}\\
dw &amp;= \left[\begin{matrix}
  0&amp;\cdots&amp;0\\
  \vdots&amp;\ddots&amp;\vdots\\
  0&amp;\cdots&amp;0\\
  \end{matrix}
\right] \in \mathbb R^{D_i\times D_o}\\
db &amp;= [0,\cdots,0] \in \mathbb R^{D_o}
\end{aligned}\]

<ul>
  <li><strong>高斯层</strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">CovMat()</code>，初始化为</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">initialize_ws</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
  <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">*</span><span class="n">numpy</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="n">numpy</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s_alpha</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">s_alpha</span><span class="p">)),</span><span class="n">numpy</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">)]])</span>
  <span class="n">self</span><span class="p">.</span><span class="n">b</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
  <span class="n">self</span><span class="p">.</span><span class="n">dW</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
  <span class="n">self</span><span class="p">.</span><span class="n">db</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></div></div>

<p>即</p>

\[\begin{aligned}
\boldsymbol w &amp;= [w_1,w_2] = [{\rm ln}\frac{\alpha}{1-\alpha},\sqrt{var}],\quad \alpha = 0.1,\; var = 1\\
b &amp;= [0]\\
dw &amp;= [0,0]\\
db &amp;= [0]
\end{aligned}\]

<h3 id="342-前向传播"><span class="me-2">3.4.2. 前向传播</span><a href="#342-前向传播" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">NNRegressor</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
<span class="o">|--</span><span class="n">Adam</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
    <span class="o">|--</span><span class="n">NNRegressor</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>
        <span class="o">|--</span><span class="n">CoreNN</span><span class="p">.</span><span class="nf">forward</span><span class="p">()</span>
            <span class="o">|--</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<ul>
  <li><strong>全连接层</strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Dense()</code>，前向传播为</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
  <span class="n">self</span><span class="p">.</span><span class="n">inp</span><span class="o">=</span><span class="n">X</span>
  <span class="n">self</span><span class="p">.</span><span class="n">out</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inp</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">b</span>
  <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">out</span>
</pre></td></tr></tbody></table></code></div></div>

<p>即</p>

\[\boldsymbol {o} = \boldsymbol x_{ND_i} \cdot \boldsymbol w_{D_iD_o} + \boldsymbol b_{D_o}\; \in \mathbb R^{N\times D_o}\]

<p>其中，$N$ 是样本数量；$D_i$ 是该层输入维度；$D_o$ 是该层输出维度，也是下一层输入维度。</p>

<p>经过多层全连接的 MLP，输入数据集从 $N\times D$ 维变为 $N\times M$ 维特征。</p>

<ul>
  <li><strong>高斯层</strong></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">CovMat()</code>，前向传播为</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">forward_rbf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
  <span class="n">self</span><span class="p">.</span><span class="n">inp</span><span class="o">=</span><span class="n">X</span>
  
  <span class="c1">#Calculate distances
</span>  <span class="n">ll</span><span class="o">=</span><span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">tmp</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ll</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tmp</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
  <span class="n">self</span><span class="p">.</span><span class="n">z</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span><span class="n">ll</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
  
  <span class="c1">#Apply RBF function to distance
</span>  <span class="n">self</span><span class="p">.</span><span class="n">s0</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">numpy</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">z</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
  
  <span class="c1">#Multiply with variance
</span>  <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
  <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">s0</span>
  
  <span class="c1">#Add noise / whitekernel
</span>  <span class="n">self</span><span class="p">.</span><span class="n">s_alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="n">numpy</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span><span class="mf">1.0</span><span class="p">)</span>
  <span class="n">self</span><span class="p">.</span><span class="n">out</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="o">+</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s_alpha</span><span class="o">+</span><span class="mf">1e-8</span><span class="p">)</span><span class="o">*</span><span class="n">numpy</span><span class="p">.</span><span class="nf">identity</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">out</span>
</pre></td></tr></tbody></table></code></div></div>

<p>首先<strong>计算每个样本对所有样本的距离矩阵</strong>：</p>

<p>第一步 <code class="language-plaintext highlighter-rouge">X[:,i].reshape(1,-1)-X[:,i].reshape(-1,1)</code>，对数据的每一列转置成行，然后扩充成方阵，然后减去直接对列扩充成的方阵。这里相当于对数据的每一列逐一减去各个列元素形成一个矩阵。</p>

<blockquote>
  <p>$1m$ 维行向量减 $n1$ 维列向量时，python 会把 $1m$ 维行向量自动扩充为 $nm$ 维，每一行都是行向量的复制； 把 $n1$ 维列向量扩充为 $nm$ 维，增加的每一列都是列向量的复制，然后做差得到 $nm$ 维矩阵。</p>
</blockquote>

<p><a href="/assets/img/postsimg/20210401/04.jpg" class="popup img-link "><img data-src="/assets/img/postsimg/20210401/04.jpg" alt="alt text" class="lazyload" data-proofer-ignore></a></p>

<p>第二步，把上述矩阵重新排列为 $N\times N\times 1$ 的形式；</p>

<p>第三步，逐一遍历所有列，得到 $D$ 个 $N\times N\times 1$ 的矩阵组成的列表，重新拼接为 $N\times N\times D$ 维矩阵。</p>

<p>其实本<strong>质上就是做了对数据集中的每个样本对所有其它样本做差的操作</strong>，假设输入高斯过程的数据集为经过 MLP 的 $N\times M$ 维特征 $\boldsymbol x$</p>

\[\boldsymbol x = 
\left[
  \begin{matrix}
    \boldsymbol x_1\\
    \boldsymbol x_2\\
    \vdots\\
    \boldsymbol x_N
  \end{matrix}
\right]
= \left[
  \begin{matrix}
    x_{11}&amp;x_{12}&amp;\cdots&amp;x_{1M}\\
    x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2M}\\
    \vdots\\
    x_{N1}&amp;x_{N2}&amp;\cdots&amp;x_{NM}\\
  \end{matrix}
\right]\in \mathbb R^{N\times M}\]

<p>那么距离为</p>

\[\boldsymbol z =
\left[
\left[
  \begin{matrix}
    \boldsymbol x_1 - \boldsymbol x_1\\
    \boldsymbol x_2 - \boldsymbol x_1\\
    \vdots\\
    \boldsymbol x_N - \boldsymbol x_1\\
  \end{matrix}
\right],
\left[
  \begin{matrix}
    \boldsymbol x_1 - \boldsymbol x_2\\
    \boldsymbol x_2 - \boldsymbol x_2\\
    \vdots\\
    \boldsymbol x_N - \boldsymbol x_2\\
  \end{matrix}
\right],
\cdots,
\left[
  \begin{matrix}
    \boldsymbol x_1 - \boldsymbol x_N\\
    \boldsymbol x_2 - \boldsymbol x_N\\
    \vdots\\
    \boldsymbol x_N - \boldsymbol x_N\\
  \end{matrix}
\right]
\right]\in \mathbb R^{N\times N\times M}\]

<p>对 $\boldsymbol z$ 的最后一维（$M$ 维）分量计算二范数的平方</p>

\[\vert\vert\boldsymbol z\vert\vert^2 = 
\left[
\begin{matrix}
  \vert\vert\boldsymbol x_1 - \boldsymbol x_1\vert\vert^2 &amp; \cdots &amp; \vert\vert\boldsymbol x_1 - \boldsymbol x_N\vert\vert^2\\
  \vert\vert\boldsymbol x_2 - \boldsymbol x_1\vert\vert^2 &amp; \cdots &amp; \vert\vert\boldsymbol x_2 - \boldsymbol x_N\vert\vert^2\\
  \vdots&amp;\ddots&amp;\vdots\\
  \vert\vert\boldsymbol x_N - \boldsymbol x_1\vert\vert^2 &amp; \cdots &amp; \vert\vert\boldsymbol x_N - \boldsymbol x_N\vert\vert^2\\
\end{matrix}
\right]\in \mathbb R^{N\times N}\]

<p>其中二范数为</p>

\[\vert\vert\boldsymbol x_i - \boldsymbol x_j\vert\vert = \sqrt{\sum_{k=1}^M (x_{ik}-x_{jk})^2}\]

<p>其次<strong>计算RBF</strong>：</p>

\[\boldsymbol s_0 = e^{-0.5\cdot \vert\vert\boldsymbol z\vert\vert^2}\quad\in \mathbb R_{N\times N}\]

<p>乘以方差（之前定义的第 2 个权重系数 $w_2$）</p>

\[\boldsymbol s = w_2^2 \cdot \boldsymbol s_0  = var\cdot \boldsymbol s_0\quad\in \mathbb R_{N\times N}\]

<p>加噪声（之前定义的第 1 个权重系数 $w_1$）</p>

\[\begin{aligned}
s_\alpha &amp;= 1/{(e^{-w_1}+1}) = 1/({\frac{1-\alpha}{\alpha}}+1) = \alpha\\
\boldsymbol {out} &amp;= \boldsymbol s + (s_\alpha+10^{-8})\cdot \boldsymbol I_{N\times N}
\end{aligned}\]

<p>形式上等效于</p>

\[\boldsymbol K = var\cdot \boldsymbol K(X,X) + \alpha\cdot \boldsymbol I\]

<p>核函数<strong>参数</strong>分别为两个权重 $var, \alpha$，最后输出 $N\times N$ 维的核矩阵 $\boldsymbol {out} = \boldsymbol K$。</p>

<h3 id="343-反向传播"><span class="me-2">3.4.3. 反向传播</span><a href="#343-反向传播" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>根据前文，极大似然估计即最小化负对数似然函数：</p>

\[{\rm argmin}_\theta \quad loss = \frac{1}{2}{\rm ln}{\vert\boldsymbol K\vert}+\frac{n}{2}{\rm ln}(2\pi)+\frac{1}{2}\boldsymbol Y^T\boldsymbol K^{-1}\boldsymbol Y\]

<blockquote>
  <p><strong>定理1</strong>：设 $\boldsymbol K$ 为一 $n\times n$ 正定对称矩阵矩阵，对 $\boldsymbol K$ 进行 Cholesky 分解</p>

  <p>\(\vert\boldsymbol K\vert=\boldsymbol L \boldsymbol L^T\)</p>
  <blockquote>

    <p>因为三角矩阵的行列式 $\vert\boldsymbol L\vert = \prod_{i=1}^n L_{ii}$，而 $\vert\boldsymbol K\vert = \vert\boldsymbol L\vert\vert\boldsymbol L^T\vert$，则有</p>
  </blockquote>

\[\vert\boldsymbol K\vert=\prod_{i=1}^n L_{ii}^2\]

\[{\rm ln}\vert\boldsymbol K\vert=2\sum_{i=1}^n {\rm ln}L_{ii}\]
</blockquote>

<p>则 $loss$ 可改写为</p>

\[loss = \sum_{i=1}^n {\rm ln}L_{ii}+\frac{n}{2}{\rm ln}(2\pi)+\frac{1}{2}\boldsymbol Y^T\boldsymbol K^{-1}\boldsymbol Y\]

<p>主要计算量在于求解核矩阵的逆 $\boldsymbol K^{-1}$。下面结合代码进行说明。</p>

<hr />

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="n">NNRegressor</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,...)</span>
<span class="o">|--</span><span class="n">Adam</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
  <span class="o">|--</span><span class="n">NNRegressor</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
    <span class="o">|--</span><span class="n">CoreNN</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
      <span class="n">self</span><span class="p">.</span><span class="n">j</span><span class="p">,</span><span class="n">err</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">cost</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">out</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">))):</span>
        <span class="n">err</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">backward</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">err</span>
</pre></td></tr></tbody></table></code></div></div>

<p>首先计算损失函数。</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre><span class="n">NNRegressor</span><span class="p">.</span><span class="nf">__init__</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">gp</span><span class="p">:</span>
    <span class="n">self</span><span class="p">.</span><span class="n">cost</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">gp_loss</span>
<span class="n">NNRegressor</span><span class="p">.</span><span class="nf">gp_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
  <span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="o">=</span><span class="n">y</span>
  <span class="n">self</span><span class="p">.</span><span class="n">A</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">].</span><span class="n">out</span>
  <span class="n">self</span><span class="p">.</span><span class="n">K</span><span class="o">=</span><span class="n">K</span>

  <span class="n">self</span><span class="p">.</span><span class="n">L_</span> <span class="o">=</span> <span class="nf">cholesky</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">L_inv</span> <span class="o">=</span> <span class="nf">solve_triangular</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">L_</span><span class="p">.</span><span class="n">T</span><span class="p">,</span><span class="n">numpy</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">L_</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
  <span class="n">self</span><span class="p">.</span><span class="n">K_inv</span> <span class="o">=</span> <span class="n">L_inv</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">L_inv</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
  
  <span class="n">self</span><span class="p">.</span><span class="n">alpha_</span> <span class="o">=</span> <span class="nf">cho_solve</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">L_</span><span class="p">,</span> <span class="bp">True</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
  <span class="n">self</span><span class="p">.</span><span class="n">nlml</span><span class="o">=</span><span class="mf">0.0</span>
  <span class="n">self</span><span class="p">.</span><span class="n">nlml_grad</span><span class="o">=</span><span class="mf">0.0</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">gg1</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha_</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">y</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">self</span><span class="p">.</span><span class="n">nlml</span><span class="o">+=</span><span class="mf">0.5</span><span class="o">*</span><span class="n">gg1</span><span class="o">+</span><span class="n">numpy</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">numpy</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">numpy</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">L_</span><span class="p">)))</span><span class="o">+</span><span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">0.5</span><span class="o">*</span><span class="n">numpy</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">numpy</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>
    <span class="n">yy</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">y</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">self</span><span class="p">.</span><span class="n">nlml_grad</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span> <span class="n">numpy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">numpy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">K_inv</span><span class="p">,</span><span class="n">yy</span><span class="p">),</span><span class="n">self</span><span class="p">.</span><span class="n">K_inv</span><span class="p">)</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">K_inv</span><span class="p">)</span><span class="o">*</span><span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">nlml</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">nlml_grad</span>
</pre></td></tr></tbody></table></code></div></div>

<p>设 $\boldsymbol K \in \mathbb R^{N\times N}$ 是高斯层最终输出的核矩阵，$\boldsymbol A\in \mathbb R^{N\times M}$ 是全连接层输出的特征。</p>

<p>对核矩阵求逆得到  $\boldsymbol K^{-1}$ 。因为 $\boldsymbol K$ 为对称正定矩阵，可采用 Cholesky 矩阵分解加速求逆过程（<code class="language-plaintext highlighter-rouge">cholesky()</code> 和 <code class="language-plaintext highlighter-rouge">solve_triangular()</code>）。</p>

<blockquote>
  <p>Cholesky 分解是把一个对称正定的矩阵表示成一个下三角矩阵 $\boldsymbol L$ 和其转置的乘积的分解。</p>

\[\boldsymbol K = \boldsymbol L\boldsymbol L^T\]

  <p>它要求矩阵的所有特征值必须大于零，故分解的下三角的对角元也是大于零的。
由于 $L$ 是可逆方阵，因此求逆和转置可以交换次序，则</p>

\[\boldsymbol K^{-1} = (\boldsymbol L^T)^{-1}\boldsymbol L^{-1} = (\boldsymbol L^T)^{-1}[{(\boldsymbol L^T)^{-1}}]^T\]

  <p>那么只需要求 $(\boldsymbol L^T)^{-1}$ 就可以求出 $\boldsymbol K^{-1}$。</p>
</blockquote>

<p>设 $\boldsymbol y\in \mathbb R^{N\times 1}$ 是训练集标签，根据 $\boldsymbol L\boldsymbol \alpha=\boldsymbol y$ 求出 $\boldsymbol \alpha$（<code class="language-plaintext highlighter-rouge">cho_solve()</code>）。</p>

\[\boldsymbol \alpha = \boldsymbol L^{-1} \boldsymbol y  \in \mathbb R^{N\times 1}\]

<p>则</p>

\[gg1 = \boldsymbol \alpha^T\boldsymbol y = \boldsymbol y^T (\boldsymbol L^{-1})^T\boldsymbol y \\\]

<p>有 negative log marginal likelihood (nlml)</p>

\[\begin{aligned}
nlml &amp;= \frac{1}{2}gg1 + \sum_{i=1}^N {\rm ln}L_{ii} + \frac{N}{2}{\rm ln} 2\pi\\
&amp;=\frac{1}{2}\boldsymbol y^T (\boldsymbol L^{-1})^T\boldsymbol y + \sum_{i=1}^N {\rm ln}L_{ii} + \frac{N}{2}{\rm ln} 2\pi\\
&amp;= -loss\quad ?
\end{aligned}\]

<p><strong>【WARNING】</strong>：关于为啥公式是 $y^TK^{-1}y$ 而代码是 $y^TL^{-1}y$ 没想明白。</p>

<hr />

<p>要使得极大似然估计最大也就是 $loss$ 最大，就要使得 $nlml$ 最小，二者相差一个负号。</p>

<p>求极值则对参数求偏导，因为核矩阵可表示为</p>

\[k(\boldsymbol x_i, \boldsymbol x_j\vert\boldsymbol \theta) \rightarrow k(g(\boldsymbol x_i,\boldsymbol w), g(\boldsymbol x_j,\boldsymbol w)\vert\boldsymbol \theta)\]

<p>其中 $g(\boldsymbol x,\boldsymbol w)$ 是深度神经网络的映射，那么</p>

\[\begin{aligned}
L = -loss &amp;= \frac{1}{2}{\rm ln}{\vert \boldsymbol K\vert}+\frac{n}{2}{\rm ln}(2\pi)+\frac{1}{2}\boldsymbol Y^T\boldsymbol K^{-1}\boldsymbol Y\\
\frac{\partial L}{\partial \boldsymbol \theta} &amp;= \frac{\partial L}{\partial \boldsymbol K}\frac{\partial \boldsymbol K}{\partial \boldsymbol \theta}\\
\frac{\partial L}{\partial \boldsymbol w} &amp;= \frac{\partial L}{\partial \boldsymbol K}\frac{\partial \boldsymbol K}{\partial g(\boldsymbol x,\boldsymbol w)}\frac{\partial g(\boldsymbol x,\boldsymbol w)}{\partial \boldsymbol w}\\
\end{aligned}\]

<p>其中共同项可以首先求解</p>

\[\begin{aligned}
\frac{\partial L}{\partial \boldsymbol K} &amp;= -\frac{N}{2}[\boldsymbol K^{-1}\boldsymbol Y\boldsymbol Y^T\boldsymbol K^{-1} - \boldsymbol K^{-1}]\\
\end{aligned}\]

<blockquote>
  <p>常用公式 1 （matrix cookbook 124）：
\(\frac{\partial }{\partial \boldsymbol X}Tr(\boldsymbol A\boldsymbol X^{-1}\boldsymbol B) = -(\boldsymbol X^{-1})^T\boldsymbol A^T\boldsymbol B(\boldsymbol X^{-1})^T\)
常用公式 1 （网络）：
\(\frac{\partial }{\partial \boldsymbol A}(\boldsymbol x^T\boldsymbol A^{-1}\boldsymbol x) = -(\boldsymbol A^{-1})^T\boldsymbol x\boldsymbol x^T(\boldsymbol A^{-1})^T\)
常用公式 2 （matrix cookbook 141）（对应第二项）：
\(\frac{\partial {\rm ln\ det}(\boldsymbol X)}{\partial \boldsymbol X} = 2\boldsymbol X^{-1}-(\boldsymbol X^{-1}\cdot \boldsymbol I)\)
常用公式 2 （维基百科）（对应第二项）：
\(\frac{\partial {\rm ln\ det}(\boldsymbol X)}{\partial \boldsymbol X} = \boldsymbol X^{-1}\)</p>
</blockquote>

<p><strong>【WARNING】</strong>：多余的 $N$ 怎么来的？</p>

<hr />

<p>继续对高斯层反向传播</p>

\[a_{err} = \frac{\partial L}{\partial \boldsymbol K}(s_\alpha)(1-s_\alpha)\]

<h3 id="344-预测"><span class="me-2">3.4.4. 预测</span><a href="#344-预测" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>对于 <strong>全连接层</strong>，在 <code class="language-plaintext highlighter-rouge">first_run()</code> 中，直接定义调用 <code class="language-plaintext highlighter-rouge">forward()</code> 函数进行预测（<code class="language-plaintext highlighter-rouge">predict = forward</code>）。</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">NNRegressor</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
<span class="o">|--</span><span class="nf">first_run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">)):</span>
      <span class="k">if</span> <span class="nf">type</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">!=</span> <span class="n">Dropout</span> <span class="ow">and</span> <span class="nf">type</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">!=</span> <span class="n">CovMat</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">predict</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">forward</span>
</pre></td></tr></tbody></table></code></div></div>

<p>对于<strong>高斯层</strong>，预测代码如下</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre></td><td class="rouge-code"><pre><span class="n">NNRegressor</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
  <span class="n">A</span><span class="o">=</span><span class="n">X</span>
  <span class="n">A2</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">x</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">A2</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">predict</span><span class="p">(</span><span class="n">A2</span><span class="p">)</span>
    <span class="n">A</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">predict</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    
  <span class="n">self</span><span class="p">.</span><span class="n">K</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">forward</span><span class="p">(</span><span class="n">A2</span><span class="p">)</span>
  <span class="n">self</span><span class="p">.</span><span class="n">L_</span> <span class="o">=</span> <span class="nf">cholesky</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">K</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  
  <span class="n">L_inv</span> <span class="o">=</span> <span class="nf">solve_triangular</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">L_</span><span class="p">.</span><span class="n">T</span><span class="p">,</span><span class="n">numpy</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">L_</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
  <span class="n">self</span><span class="p">.</span><span class="n">K_inv</span> <span class="o">=</span> <span class="n">L_inv</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">L_inv</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
  
  <span class="n">self</span><span class="p">.</span><span class="n">alpha_</span> <span class="o">=</span> <span class="nf">cho_solve</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">L_</span><span class="p">,</span> <span class="bp">True</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="p">)</span>
  
  
  <span class="n">K2</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
  <span class="n">K3</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">self</span><span class="p">.</span><span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
  
  <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">kernel</span><span class="o">==</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">d1</span><span class="o">=</span><span class="mf">0.0</span>
    <span class="n">d2</span><span class="o">=</span><span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
      <span class="n">d1</span><span class="o">+=</span><span class="p">(</span><span class="n">A</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">A</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
      <span class="n">d2</span><span class="o">+=</span><span class="p">(</span><span class="n">A</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">A2</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">K2</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">var</span><span class="o">*</span><span class="n">numpy</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">d1</span><span class="p">)</span><span class="o">+</span><span class="n">numpy</span><span class="p">.</span><span class="nf">identity</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">s_alpha</span><span class="o">+</span><span class="mf">1e-8</span><span class="p">)</span>
    <span class="n">K3</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">var</span><span class="o">*</span><span class="n">numpy</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">d2</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">kernel</span><span class="o">==</span><span class="sh">'</span><span class="s">dot</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">K2</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">A</span><span class="p">.</span><span class="n">T</span><span class="p">)</span><span class="o">+</span><span class="n">numpy</span><span class="p">.</span><span class="nf">identity</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">s_alpha</span><span class="o">+</span><span class="mf">1e-8</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">var</span>
    <span class="n">K3</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">A2</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">var</span>
    
  <span class="n">preds</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">alpha_</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">preds</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">K3</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">alpha_</span><span class="p">[:,</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))[:,</span><span class="mi">0</span><span class="p">]</span>
  
  <span class="k">return</span> <span class="n">preds</span><span class="p">,</span> <span class="n">numpy</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">numpy</span><span class="p">.</span><span class="nf">diagonal</span><span class="p">(</span><span class="n">K2</span><span class="o">-</span><span class="n">numpy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">K3</span><span class="p">,</span><span class="n">numpy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">K_inv</span><span class="p">,</span><span class="n">K3</span><span class="p">.</span><span class="n">T</span><span class="p">))))</span>
</pre></td></tr></tbody></table></code></div></div>

<p>设 $\boldsymbol A=\boldsymbol X\in \mathbb R^{n\times D}$ 为测试集，$\boldsymbol A_2=\boldsymbol x\in \mathbb R^{N\times D}$ 为训练集.</p>

<p>首先经过全连接层前向传播后特征维度为 $M$，得到的输出分别依然记作 $\boldsymbol A\in \mathbb R^{n\times M}, \boldsymbol A_2\in \mathbb R^{N\times M}$。</p>

<p>对于训练集 $\boldsymbol A_2$，调用高斯层的前向传播函数，计算出训练集的核矩阵</p>

\[\boldsymbol K = \boldsymbol s + (s_\alpha+10^{-8})\cdot \boldsymbol I_{N\times N}\]

<p>然后对核矩阵求逆。因为 $\boldsymbol K$ 为对称正定矩阵，可采用 Cholesky 矩阵分解加速求逆过程（<code class="language-plaintext highlighter-rouge">cholesky()</code> 和 <code class="language-plaintext highlighter-rouge">solve_triangular()</code>）。</p>

<p>设 $\boldsymbol y\in \mathbb R^{N\times1}$ 是训练集的标签，则根据 $\boldsymbol L\boldsymbol \alpha=\boldsymbol y$ 求出 $\boldsymbol \alpha$（<code class="language-plaintext highlighter-rouge">cho_solve()</code>）。</p>

\[\boldsymbol \alpha = \boldsymbol y \boldsymbol L^{-1}  \in \mathbb R^{N\times 1}\]

<p>和高斯层的前向传播类似，分别计算测试集的核函数 $\boldsymbol K_2 \in \mathbb R^{n\times n}$，以及测试集与训练集之间的核函数 $\boldsymbol K_3 \in \mathbb R^{n\times N}$。</p>

\[\begin{aligned}
\vert\vert\boldsymbol d_1\vert\vert^2 &amp;= 
\left[
\begin{matrix}
  \vert\vert\boldsymbol X_1 - \boldsymbol X_1\vert\vert^2 &amp; \cdots &amp; \vert\vert\boldsymbol X_1 - \boldsymbol X_n\vert\vert^2\\
  \vert\vert\boldsymbol X_2 - \boldsymbol X_1\vert\vert^2 &amp; \cdots &amp; \vert\vert\boldsymbol X_2 - \boldsymbol X_n\vert\vert^2\\
  \vdots&amp;\ddots&amp;\vdots\\
  \vert\vert\boldsymbol X_n - \boldsymbol X_1\vert\vert^2 &amp; \cdots &amp; \vert\vert\boldsymbol X_n - \boldsymbol X_n\vert\vert^2\\
\end{matrix}
\right]\in \mathbb R^{n\times n}\\
\vert\vert\boldsymbol d_2\vert\vert^2 &amp;= 
\left[
\begin{matrix}
  \vert\vert\boldsymbol X_1 - \boldsymbol x_1\vert\vert^2 &amp; \cdots &amp; \vert\vert\boldsymbol X_1 - \boldsymbol x_N\vert\vert^2\\
  \vert\vert\boldsymbol X_2 - \boldsymbol x_1\vert\vert^2 &amp; \cdots &amp; \vert\vert\boldsymbol X_2 - \boldsymbol x_N\vert\vert^2\\
  \vdots&amp;\ddots&amp;\vdots\\
  \vert\vert\boldsymbol X_n - \boldsymbol x_1\vert\vert^2 &amp; \cdots &amp; \vert\vert\boldsymbol X_n - \boldsymbol x_N\vert\vert^2\\
\end{matrix}
\right]\in \mathbb R^{n\times N}\\
\boldsymbol K_2 &amp;= var\cdot e^{-0.5\cdot \vert\vert\boldsymbol d_1\vert\vert^2} + (s_\alpha + 10^{-8})\boldsymbol I_{n\times n}\\
\boldsymbol K_3 &amp;= var\cdot e^{-0.5\cdot \vert\vert\boldsymbol d_2\vert\vert^2}\\
\end{aligned}\]

<p>预测输出的值为</p>

\[\begin{aligned}
\boldsymbol Y &amp;= \boldsymbol K_3 \cdot \boldsymbol \alpha\\
std &amp;= \sqrt{diag[\boldsymbol K_2-\boldsymbol K_3\boldsymbol K^{-1}\boldsymbol K_3^T]}
\end{aligned}\]

<h1 id="4-参考文献">4. 参考文献</h1>

<p>[1] bingjianing. <a href="https://www.cnblogs.com/bingjianing/p/9117330.html">多元高斯分布（The Multivariate normal distribution）</a></p>

<p>[2] 论智. <a href="https://zhuanlan.zhihu.com/p/32152162">图文详解高斯过程（一）——含代码</a></p>

<p>[3] 我能说什么好. <a href="https://zhuanlan.zhihu.com/p/73832253">通俗理解高斯过程及其应用</a></p>

<p>[4] 石溪. <a href="https://www.zhihu.com/question/46631426">如何通俗易懂地介绍 Gaussian Process</a></p>

<p>[5] 钱默吟. <a href="https://zhuanlan.zhihu.com/p/58987388">多元高斯分布完全解析</a></p>

<p>[6] li Eta. <a href="https://www.zhihu.com/question/46631426/answer/102314452">如何通俗易懂地介绍 Gaussian Process？</a></p>

<p>[7] 蓦风星吟. <a href="https://zhuanlan.zhihu.com/p/39118553">Gaussian process 的最后一步——话说超参学习</a></p>

<p>[7] 蓦风星吟. <a href="https://zhuanlan.zhihu.com/p/51012226">【答疑解惑III】说说高斯过程中的多维输入和多维输出</a></p>


</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw me-1"></i>
    
      <a href='/categories/academic/'>Academic</a>,
      <a href='/categories/knowledge/'>Knowledge</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw me-1"></i>
      
      <a href="/tags/deep-learning/"
          class="post-tag no-text-decoration" >deep learning</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        本文由作者按照 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         进行授权

      
    </div>

    <!-- Post sharing snippet -->

<div class="share-wrapper">
  <span class="share-label text-muted me-1">分享</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B%EF%BC%89%20-%20SIRLIS&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fdeep-learning-gaussian-process%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B%EF%BC%89%20-%20SIRLIS&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fdeep-learning-gaussian-process%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fdeep-learning-gaussian-process%2F&text=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B%EF%BC%89%20-%20SIRLIS"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <i
      id="copy-link"
      class="fa-fw fas fa-link small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="分享链接"
      data-title-succeed="链接已复制！"
    >
    </i>
  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
      
    </div>
  </div>
  <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
    <div class="access">
      <!-- Get the last 5 posts from lastmod list. -->














  <div id="access-lastmod" class="post">
    <div class="panel-heading">最近更新</div>
    <ul class="post-content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-DNN/">模式识别（经典神经网络）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Pattern-Recognition-Nonlinear-Classifier/">模式识别（非线性分类器）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/artificial-intelligence-search-strategy/">人工智能（搜索策略）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/artificial-intelligence-knowledge-etc/">人工智能（知识表示与推理）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/artificial-intelligence-introduction/">人工智能（绪论）</a>
        </li>
      
    </ul>
  </div>
  <!-- #access-lastmod -->


      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pattern-recognition/">pattern recognition</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial intelligence</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
    </div>
  </div>


    </div>

    
      
      



  <div id="toc-wrapper" class="ps-0 pe-4 mb-5">
    <div class="panel-heading ps-3 pt-2 mb-2">文章内容</div>
    <nav id="toc"></nav>
  </div>


    
  </div>
</div>

<!-- tail -->

  <div class="row">
    <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-3 pe-xl-4 mt-5">
      
        
        <!--
  Recommend the other 3 posts according to the tags and categories of the current post,
  if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->








  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  






<!-- Fill with the other newlest posts -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ms-1" data-toc-skip>
      相关文章
    </h3>
    <div class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        
        
        <div class="col">
          <a href="/posts/deep-learning-basic-math/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1741170619"
  data-df="YYYY/MM/DD"
  
>
  2025/03/05
</em>

              <h4 class="pt-0 my-2" data-toc-skip>深度学习（基础数学知识）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了机器（深度）学习的基础数学知识，作为后续深入学习的基础。






  1. 矩阵
    
      1.1. 迹（trace）
      1.2. 范数
        
          1.2.1. 向量范数
          1.2.2. 矩阵范数
        
      
      1.3. 正交矩阵
      1.4. 矩阵的值域、零空间和秩
  ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/deep-learning-basic-hp-and-opt/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1592036659"
  data-df="YYYY/MM/DD"
  
>
  2020/06/13
</em>

              <h4 class="pt-0 my-2" data-toc-skip>深度学习基础（基本超参数和优化器）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了深度学习中的基本概念，包括 batch、epoch、iteration、optimizer等，其中优化器包括 BGD、SGD、Adam等，为后深度学习提供基础。






  1. 基本超参数
    
      1.1. epoch
      1.2. batch \&amp;amp; batch_size
      1.3. iteration
    
  
  2. 优化器...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/deep-learning-basic-conv2d/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1592123059"
  data-df="YYYY/MM/DD"
  
>
  2020/06/14
</em>

              <h4 class="pt-0 my-2" data-toc-skip>深度学习基础（PyTorch的CNN组成）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了深度学习中，卷积网络的基本知识，包括2d卷积层、池化层、线性层、softmax 激活函数、交叉熵损失函数等，并结合它们在 Pytorch 中的定义和实现进行说明。






  1. 层
    
      1.1. Conv2d
        
          1.1.1. dilation
          1.1.2. padding
        
     ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
    </div>
    <!-- .card-deck -->
  </div>
  <!-- #related-posts -->


      
        
        <!-- Navigation buttons at the bottom of the post. -->

<div class="post-navigation d-flex justify-content-between">
  
    <a
      href="/posts/vscode-R/"
      class="btn btn-outline-primary"
      prompt="上一篇"
    >
      <p>VSCode部署R开发环境</p>
    </a>
  

  
    <a
      href="/posts/optimal-control-numerical/"
      class="btn btn-outline-primary"
      prompt="下一篇"
    >
      <p>最优控制基础（微分方程的解法）</p>
    </a>
  
</div>

      
        
        <!--  The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script src="https://utteranc.es/client.js"
        repo="sirlis/sirlis.github.io"
        issue-term="pathname"
        crossorigin="anonymous"
        async>
</script>

<script type="text/javascript">
  $(function() {
    const origin = "https://utteranc.es";
    const iframe = "iframe.utterances-frame";
    const lightTheme = "github-light";
    const darkTheme = "github-dark";
    let initTheme = lightTheme;

    if ($("html[data-mode=dark]").length > 0
        || ($("html[data-mode]").length == 0
            && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
      initTheme = darkTheme;
    }

    addEventListener("message", (event) => {
      let theme;

      /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */
      if (event.origin === origin) {
        /* page initial */
        theme = initTheme;

      } else if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {
        /* global theme mode changed */
        const mode = event.data.message;
        theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme);

      } else {
        return;
      }

      const message = {
        type: "set-theme",
        theme: theme
      };

      const utterances = document.querySelector(iframe).contentWindow;
      utterances.postMessage(message, origin);
    });

  });
</script>



      
    </div>
  </div>


        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 post-content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pattern-recognition/">pattern recognition</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial intelligence</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>
    </div>

    <!-- The Footer -->

<footer>
  <div class="container px-lg-4">
    <div class="d-flex justify-content-center align-items-center text-muted mx-md-3">
      <p>本站采用 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 主题 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
      </p>

      <p>©
        2025
        <a href="https://github.com/sirlis">sirlis</a>.
        
          <span
            data-bs-toggle="tooltip"
            data-bs-placement="top"
            title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。"
          >保留部分权利。</span>
        
      </p>
    </div>
  </div>
</footer>


    <div id="mask"></div>

    <button id="back-to-top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow">
      <i class="fas fa-angle-up"></i>
    </button>

    
      <div
        id="notification"
        class="toast"
        role="alert"
        aria-live="assertive"
        aria-atomic="true"
        data-bs-animation="true"
        data-bs-autohide="false"
      >
        <div class="toast-header">
          <button
            type="button"
            class="btn-close ms-auto"
            data-bs-dismiss="toast"
            aria-label="Close"
          ></button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="px-2 mb-3">发现新版本的内容。</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            更新
          </button>
        </div>
      </div>
    

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.0/dist/jquery.min.js,npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.7/dayjs.min.js,npm/dayjs@1.11.7/locale/zh.min.js,npm/dayjs@1.11.7/plugin/relativeTime.min.js,npm/dayjs@1.11.7/plugin/localizedFormat.min.js,npm/tocbot@4.21.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '<div class="px-1 px-sm-2 px-lg-4 px-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

