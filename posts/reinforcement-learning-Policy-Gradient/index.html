<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="zh-CN" 
  
>
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="强化学习（策略梯度法）" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="本文介绍了强化学习的策略梯度法（Policy Gradient）。" />
<meta property="og:description" content="本文介绍了强化学习的策略梯度法（Policy Gradient）。" />
<link rel="canonical" href="http://localhost:4000/posts/reinforcement-learning-Policy-Gradient/" />
<meta property="og:url" content="http://localhost:4000/posts/reinforcement-learning-Policy-Gradient/" />
<meta property="og:site_name" content="SIRLIS" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-11-27T16:43:19+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="强化学习（策略梯度法）" />
<meta name="twitter:site" content="@none" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-01-01T20:28:20+08:00","datePublished":"2023-11-27T16:43:19+08:00","description":"本文介绍了强化学习的策略梯度法（Policy Gradient）。","headline":"强化学习（策略梯度法）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/reinforcement-learning-Policy-Gradient/"},"url":"http://localhost:4000/posts/reinforcement-learning-Policy-Gradient/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>强化学习（策略梯度法） | SIRLIS
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SIRLIS">
<meta name="application-name" content="SIRLIS">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.0/dist/tocbot.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/head.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <div class="site-title">
      <a href="/">SIRLIS</a>
    </div>
    <div class="site-subtitle fst-italic">分享科研和生活的日常</div>
  </div>
  <!-- .profile-wrapper -->

  <ul class="nav flex-column flex-grow-1 w-100 ps-0">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home"></i>
        <span>首页</span>
      </a>
    </li>
    <!-- the real tabs -->
    
      <li class="nav-item">
        <a href="/categories/" class="nav-link">
          <i class="fa-fw fas fa-stream"></i>
          

          <span>分类</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/tags/" class="nav-link">
          <i class="fa-fw fas fa-tags"></i>
          

          <span>标签</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/archives/" class="nav-link">
          <i class="fa-fw fas fa-archive"></i>
          

          <span>归档</span>
        </a>
      </li>
      <!-- .nav-item -->
    
      <li class="nav-item">
        <a href="/about/" class="nav-link">
          <i class="fa-fw fas fa-info-circle"></i>
          

          <span>关于</span>
        </a>
      </li>
      <!-- .nav-item -->
    
  </ul>
  <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/sirlis"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/none"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['lihongjue','buaa.edu.cn'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</div>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container px-xxl-5">
        <!-- The Top Bar -->

<div id="topbar-wrapper">
  <div
    id="topbar"
    class="container d-flex align-items-center justify-content-between h-100"
  >
    <span id="breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                首页
              </a>
            </span>

          
        
          
        
          
            
              <span>强化学习（策略梯度法）</span>
            

          
        
      
    </span>
    <!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      文章
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="搜索..."
      >
    </span>
    <span id="search-cancel">取消</span>
  </div>
</div>

        











<div class="row">
  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pe-xl-4">
    

    <div class="post px-1 px-md-2">
      

      
        
      
        <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->




<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  

  
  

  




<!-- return -->




<h1 data-toc-skip>强化学习（策略梯度法）</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      发表于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1701074599"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2023/11/27
</em>

    </span>

    <!-- lastmod date -->
    
    <span>
      更新于
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class=""
  data-ts="1704112100"
  data-df="YYYY/MM/DD"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  2024/01/01
</em>

    </span>
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      作者

      <em>
      
        <a href="https://github.com/sirlis">sirlis</a>
      
      </em>
    </span>

    <div>
      <!-- read time -->
      <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="6595 字"
>
  <em>36 分钟</em>阅读</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <p>本文介绍了强化学习的策略梯度法（Policy Gradient）。</p>

<!--more-->

<hr />

<ul>
  <li><a href="#1-回顾">1. 回顾</a></li>
  <li><a href="#2-策略梯度">2. 策略梯度</a>
    <ul>
      <li><a href="#21-策略函数">2.1. 策略函数</a></li>
      <li><a href="#22-策略函数的分布形式">2.2. 策略函数的分布形式</a></li>
      <li><a href="#23-策略梯度的概念">2.3. 策略梯度的概念</a></li>
    </ul>
  </li>
  <li><a href="#3-策略梯度的计算">3. 策略梯度的计算</a>
    <ul>
      <li><a href="#31-基于累计收益的策略梯度">3.1. 基于累计收益的策略梯度</a></li>
      <li><a href="#32-基于初始状态价值的策略梯度">3.2. 基于初始状态价值的策略梯度</a></li>
      <li><a href="#33-策略梯度定理">3.3. 策略梯度定理</a></li>
    </ul>
  </li>
  <li><a href="#4-基于策略梯度的强化学习">4. 基于策略梯度的强化学习</a>
    <ul>
      <li><a href="#41-reinforce">4.1. REINFORCE</a></li>
      <li><a href="#42-reinforce-with-baseline">4.2. REINFORCE with baseline</a></li>
      <li><a href="#43-actor-critic-related-to-v">4.3. Actor-Critic (related to V)</a></li>
      <li><a href="#44-actor-critic-related-to-q">4.4. Actor-Critic (related to Q)</a></li>
      <li><a href="#45-advantage-actor-critic-a2c">4.5. Advantage Actor-Critic (A2C)</a></li>
    </ul>
  </li>
  <li><a href="#5-参考文献">5. 参考文献</a></li>
</ul>

<h2 id="1-回顾"><span class="me-2">1. 回顾</span><a href="#1-回顾" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>基于价值函数的强化学习方法</p>

<ul>
  <li>动态规划方法（Dynamic Programming, DP）</li>
  <li>蒙特卡洛方法（Monte Carlo, MC）</li>
  <li>时序差分方法（Temporal-Difference, TD）</li>
</ul>

<p>这三种方法存在共同特点：</p>

<ul>
  <li>在求解强化学习任务时，最终目标是求解满足规则的最优策略 $\pi$。但以上三种方法并 <strong>没有直接求解</strong> 策略 $\pi$ 这个变量，而是先计算状态价值函数 $V_\pi(s)$ 或状态-动作价值函数 $q_\pi(s,a)$；然后再基于价值函数结果改进策略 $\pi$，是一种间接方式;</li>
  <li>在策略改进过程中，本质上均选择<strong>最优价值函数</strong>对应的动作作为新的策略；</li>
  <li>均为<strong>表格式</strong>强化学习的代表方法。</li>
</ul>

<p>特别地，对于 DQN 方法，神经网络的输入是原始的状态信息，如游戏画面，输出是在这种状态下执行各种动作的回报，即价值函数（Q函数）。训练完成之后，神经网络逼近的是最优 Q 函数 $Q(s,a)$。</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre>          +----+ ----&gt;Q(s,a1)                       +----+
  s  ---&gt; |f(x)| ----&gt;Q(s,a2)       or     s,ai ---&gt;|f(x)| ----&gt;Q(s,ai)
          +----+ ----&gt;Q(s,a3)                       +----+
</pre></td></tr></tbody></table></code></div></div>

<p>以 DQN 为代表的基于价值的强化学习方法，虽然在某些问题上取得了成功，但存在以下问题：</p>

<ul>
  <li>
    <p><strong>无法表示随机策略</strong>。某些问题的最优策略是随机策略，需要以不同的概率选择不同的动作。而基于价值的强化学习算法在实现时采用了贪心策略，给定一个状态 ，选择的策略是确定性的，显然无法实现这种按照概率执行各种候选动作的要求。比如石头剪刀布的最优策略是以 (1/3, 1/3, 1/3）的概率来选择石头/剪刀/布，基于价值的强化学习无法实现这种策略。</p>
  </li>
  <li>
    <p><strong>无法处理高维动作空间</strong>。DQN 要求动作空间是离散的，且只能是有限个，这样才能寻找使动作值函数最大化的动作的操作。但在很多问题特别是物理控制任务中，具有连续实值和高维的动作空间，例如要控制在 x y z 方向的速度、加速度。DQN 不能直接应用。</p>
  </li>
  <li>
    <p><strong>难以收敛</strong>。基于价值的强化学习方法输出的价值（如各个动作的最优 <code class="language-plaintext highlighter-rouge">Q</code> 函数值）的微小改变会导致某一动作被选中或不选中，这种不连续的变化会影响算法的收敛。这很容易理解，假设一个动作<code class="language-plaintext highlighter-rouge">a</code>的<code class="language-plaintext highlighter-rouge">Q</code>函数值本来在所有动作中是第 2 大的，把它增加 0.0001，就变成第最大的，那这种微小的变化会导致策略完全改变。因为之前它不是最优动作，现在变成最优动作了。</p>
  </li>
  <li>
    <p><strong>存在徘徊</strong>。在观察受限的情况下，基于价值的强化学习方法会出现徘徊现象。即出现不同状态导致的观察相同，使得对应相同的 “最佳” 策略，但明显不符合任务实际情况的现象。</p>
  </li>
</ul>

<h2 id="2-策略梯度"><span class="me-2">2. 策略梯度</span><a href="#2-策略梯度" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="21-策略函数"><span class="me-2">2.1. 策略函数</span><a href="#21-策略函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>相比之下，策略梯度算法是一种更为直接的方法，它让神经网络直接输出策略函数 $\pi(s)$，即在状态s下应该执行何种动作。对于非确定性策略，输出的是这种状态下执行各种动作的概率值，即如下的条件概率</p>

\[\pi(a\vert s) = p(a\vert s)\]

<p>所谓确定性策略，是只在某种状态下要执行的动作是确定即唯一的，而非确定性动作在每种状态下要执行的动作是随机的，可以按照一定的概率值进行选择。这种做法的原理如下图所示。</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>          +----+ ----&gt;p(a1|s)
  s  ---&gt; |f(x)| ----&gt;p(a2|s)
          |  θ | ----&gt;...
          +----+ ----&gt;p(a3|s)
</pre></td></tr></tbody></table></code></div></div>

<p>若动作 $a$ 是连续性随机变量，因此此时的策略 $\pi$ 不再是上述概率集合的形式，而是可微函数的形式——动作发生的概率受到某个概率密度函数的控制。假设动作 $a$ 服从某一概率密度函数 $P(a\vert s;\theta)$ (换种思路理解——将动作 $a$ 理解成从概率模型 $P(a\vert s;\theta)$ 产生的样本)。最终目标从求解 $P(a\vert s;\theta)$ 转换成求解策略中的参数 $\theta$。因此，将含参数 $\theta$ 的策略称为<strong>策略函数</strong>。记作 $\pi(a \vert s;\theta)$，通常情况下可简写成 $\pi_{\theta}$。</p>

<h3 id="22-策略函数的分布形式"><span class="me-2">2.2. 策略函数的分布形式</span><a href="#22-策略函数的分布形式" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ul>
  <li>如果动作 $a$ 是<strong>离散型随机变量</strong> $\to$ 使用 $softmax$ 函数将其映射为 <strong>指数族分布</strong></li>
</ul>

<p>这里并不是一定要使用 $softmax$ 函数进行映射 $\to$ 只要能够映射为 ‘指数族分布’ 即可。</p>

<p>将 $h(s,a;\theta)$ 函数定义为 ‘动作偏好值’ $\to$ 将离散型的策略 $\pi(a \vert s;\theta)$ 视为关于 $\theta$ 的一个函数，有
\(\pi(a \vert s;\theta) \rightarrow \frac{e^{h(s,a;\theta)}}{\sum_{a'}e^{h(s,a';\theta)}}\)</p>

<ul>
  <li>如果动作 $a$ 是<strong>连续型随机变量</strong> $\to$ $a$ 服从的分布可以有很多种，不妨设 $a$ 服从 <strong>高斯分布</strong></li>
</ul>

<p>这里将高斯分布中的 $\mu,\sigma$ 看作参数 $\theta$ 的函数，若 $\theta$ 能够求解， $\mu,\sigma$ 同样也可以被求解，最终求解整个分布</p>

\[\mu=\mu(s;\theta),\sigma=\sigma(s;\theta)\]

<p>至此，假设 $a$ 是服从 1 维随机变量的高斯分布，策略函数 $\pi(a \vert s;\theta)$ 表示如下</p>

\[\pi(a \vert s;\theta) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(a-\mu)^2}{2\sigma^2}}\]

<p>为了改进策略函数，需要围绕参数 $\theta$ 构建一个目标函数 $J(\theta)$，然后通过梯度下降（上升）的方式更新参数。</p>

<blockquote>
  <p>在后面我们会发现，最终需要计算目标函数的梯度，相当于计算策略函数的对数的梯度
但通常我们不会直接使用策略函数的对数的梯度，因为可以采用线程的深度学习框架（PyTorch或者TensorFlow）自动求导来完成。</p>
</blockquote>

<h3 id="23-策略梯度的概念"><span class="me-2">2.3. 策略梯度的概念</span><a href="#23-策略梯度的概念" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ul>
  <li><strong>回顾价值梯度</strong></li>
</ul>

<p>首先回顾 Q-Learning 的核心思想，即通过一步采样更新 Q 值（即TD方法的思想）</p>

\[Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_{t+1}+\gamma\max_a 𝑄(s_{t+1},a)−Q(s_t,a_t)]\]

<p>上述更新过程<strong>可以收敛</strong>，意味着</p>

\[r_{t+1}+\gamma\max_a 𝑄(s_{t+1},a)−Q(s_t,a_t) \rightarrow 0\]

<p>那么我们可以构造一个参数为 $\theta$ 的深度神经网络，用来估计价值函数，即 DQN 方法</p>

\[Q(s,a) = Q(s,a; \theta)\]

<p>则训练用的损失函数可设计为</p>

\[L(\theta) = \mathbb{E} \{[r_{t+1}+\gamma\max_a 𝑄(s_{t+1},a; \theta)−Q(s_t,a_t; \theta)]^2\}\]

<p>然后，通过梯度下降更新神经网络的参数，使得损失函数 $L(\theta)$ 最小。
\(\theta \leftarrow \theta - \alpha \nabla_{\theta} L(\theta)\)</p>

<p>注意到损失函数中只有最后一项是关于 $\theta$ 的函数，因此损失函数的梯度可以写为</p>

\[\nabla_{\theta} L(\theta) = \alpha[r_{t+1}+\gamma\max_a 𝑄(s_{t+1},a; \theta)−Q(s_t,a_t; \theta)]\nabla_{\theta}  Q(s_t,a_t; \theta)\]

<p>其中，$\alpha$ 是学习率，$\theta$ 是神经网络的参数。</p>

<p>可以看出，在基于价值的强化学习中，价值函数是我们的学习目标，从而定义出损失函数。通过更新式收敛保证价值函数的最优。</p>

<ul>
  <li><strong>策略梯度</strong></li>
</ul>

<p>类似地，策略梯度算法的核心思想就是通过计算策略函数的梯度，来更新神经网络的参数。对于非确定性策略，神经网络的输出的是这种状态下执行各种动作的概率值，即如下的条件概率</p>

\[\pi(a\vert s) = p(a\vert s)\]

<p>那么损失函数的定义必然包含输出的条件概率（这样才能衡量输出于真值之间的误差），对应的参数更新梯度计算式形如</p>

\[\theta \leftarrow \theta + \nabla_\theta J(\pi(a\vert s ; \theta))\]

<p>其中 $J(\pi(a\vert s;\theta))$ 可简记为 $J(\theta)$。 上式为何要用梯度上升（即加号）后文再来解释。那么问题就转化为：</p>
<ul>
  <li>目标函数（损失函数）是否存在？</li>
  <li>目标函数（损失函数）的梯度（即策略梯度）是否存在？</li>
  <li>如何计算策略梯度？</li>
</ul>

<h2 id="3-策略梯度的计算"><span class="me-2">3. 策略梯度的计算</span><a href="#3-策略梯度的计算" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="31-基于累计收益的策略梯度"><span class="me-2">3.1. 基于累计收益的策略梯度</span><a href="#31-基于累计收益的策略梯度" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>计算策略梯度的目的，是为了更新神经网络的参数，使神经网络产生的能够得到大的奖励的动作的概率变大。回想价值学习的目标是什么？强化学习的目标是使得获得的奖励最大化。</p>

<p>定义神经网络输出的策略为 $\pi:=p(a\vert s;\theta)$，其中 $\theta$ 是神经网络的参数。假设采样一条马尔科夫轨迹如下</p>

\[\tau = {s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T}\]

<p>那么该轨迹发生的概率为</p>

\[p_{\theta}(\tau) = p(s_1)\prod_{t=1}^T p(a_t\vert s_t;\theta)p(s_{t+1}\vert s_t,a_t)\]

<p>其中，$p(s_1)$ 为初始状态的概率；$p(s_{t+1}\vert s_t,a_t)$ 为环境转移概率，即在状态 $s_t$ 下执行动作 $a_t$ 后，进入状态 $s_{t+1}$ 的概率。注意这两个概率均与网络参数无关。</p>

<p>不考虑折扣衰减，该轨迹获得的收益为累计回报</p>

\[R(\tau) = \sum_{t=0}^T r_t\]

<p>由于马尔科夫链是采样得到的，因此当前策略可获得的期望奖励为</p>

\[\mathbb{E} [R(\tau)] = \sum_{\tau} p_{\theta}(\tau)R(\tau)\]

<p>因此我们可以定义<strong>目标函数</strong>为</p>

\[J(\theta) = \mathbb{E} [R(\tau)]\]

<p>定义<strong>策略梯度</strong>为</p>

\[\nabla_{\theta}J(\theta) = \nabla_{\theta}\mathbb{E} [R(\tau)] = \nabla_{\theta}\sum_{\tau} p_{\theta}(\tau) R(\tau) = \sum_{\tau} \nabla_{\theta}p_{\theta}(\tau) R(\tau)\]

<p>注意，强化学习的目的是最大化这个期望奖励，因此需要用梯度上升来更新神经网络参数。</p>

\[\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)\]

<ul>
  <li><strong>问题转化为</strong>：如何计算 $\nabla_{\theta}p_{\theta}(\tau)$</li>
</ul>

<p>注意到 $p_{\theta}(\tau)$ 是许多概率连乘的形式，如果直接计算其梯度，势必存在数据下溢（概率都是小于等于 1 的，多个概率连乘后数值会非常小），需要想办法避免。一种直觉思路是通过取对数把连乘转化为连加。</p>

<p>对 $log(f(x))$ 函数求梯度具备如下性质</p>

\[\nabla_{\theta} \ln f(x) = \frac{\nabla_{\theta} f(x)}{f(x)}\]

<p>因此有</p>

\[\nabla_{\theta} f(x) = f(x)\nabla_{\theta} \ln f(x)\]

<p>将 $p_{\theta}(\tau)$ 带入即可得</p>

\[\nabla_{\theta}p_{\theta}(\tau) = p_{\theta}(\tau)\nabla_{\theta} \ln p_{\theta}(\tau)\]

<p>因此有</p>

\[\begin{aligned}
\nabla_{\theta}\mathbb{E} [R(\tau)] &amp;= \sum_{\tau} \nabla_{\theta}p_{\theta}(\tau) R(\tau)\\
&amp;= \sum_{\tau} p_{\theta}(\tau)\nabla_{\theta} \ln p_{\theta}(\tau) R(\tau) \\
&amp;=\mathbb{E}_{\tau\sim p_{\theta(\tau)}}[\nabla_{\theta} \ln p_{\theta}(\tau) R(\tau)]
\end{aligned}\]

<ul>
  <li><strong>问题转化为</strong>：如何计算 $\nabla_{\theta} \ln p_{\theta}(\tau)$</li>
</ul>

<p>首先通过蒙特卡洛方法进行 $N$ 次采样来近似估计期望，可将期望式打开得到</p>

\[\begin{aligned}
\nabla_{\theta}\mathbb{E} [R(\tau)] &amp;=\mathbb{E}_{\tau\sim p_{\theta(\tau)}}[\nabla_{\theta} \ln p_{\theta}(\tau) R(\tau)]\\
&amp;=\frac{1}{N} \sum_{n=1}^N \nabla_{\theta} \ln p_{\theta}(\tau_n) R(\tau_n)
\end{aligned}\]

<p>其中，$\tau_n$ 是第 $n$ 次采样得到的轨迹。那么对于这条具体的轨迹有</p>

\[\begin{aligned}
\nabla_{\theta} \ln p_{\theta}(\tau_n) &amp;= \nabla_{\theta} \left[\ln p(s_1) + \sum_{t=1}^T \ln p_{\theta}(a_t\vert s_t) +\sum_{t=1}^T \ln p(s_{t+1}\vert s_t,a_t)\right]_n\\
&amp;= \nabla_{\theta}\sum_{t=1}^T\ln p_{\theta}(a_t^n\vert s_t^n)
= \sum_{t=1}^T\nabla_{\theta}\ln p_{\theta}(a_t^n\vert s_t^n)\\
\end{aligned}\]

<p>上式可以看出，充分利用了对数函数把连乘转为连加的好处。其中第一项和第三项与网络参数 $\theta$ 无关，因此对其求梯度为 0，仅剩第二项。这里 $p_{\theta}(a_t^n\vert s_t^n) = p(a_t^n\vert s_t^n;\theta)$。</p>

<p>最后可得到如下更新式</p>

\[\nabla_{\theta}J(\theta) = \frac{1}{N}\sum_{n=0}^N \sum_{t=0}^T \nabla_{\theta}\ln p_{\theta}(a_t^n\vert s_t^n) R(\tau_n)\]

\[\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)\]

<p>最后，如果我们重新把策略梯度写为期望的形式，注意到之前定义的 $\pi :=p(a\vert s)$，有</p>

\[\nabla_{\theta}J(\theta)=\mathbb{E}_{\pi(\theta)} [\nabla_{\theta}\ln \pi_{\theta}(a\vert s) R]\]

<p>到后面我们可以发现，策略梯度的定义不局限于这一种形式，主要反映在期望括号内后一项的选择上。</p>

<h3 id="32-基于初始状态价值的策略梯度"><span class="me-2">3.2. 基于初始状态价值的策略梯度</span><a href="#32-基于初始状态价值的策略梯度" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>前面基于累计收益的策略梯度方法存在一个局限，即只适用于蒙特卡洛采样能够采样到终止状态的情况，此时累计收益可以计算得到特定的值。如果采样无法达到终止状态，蒙特卡洛序列就无限延申，就无法计算累计收益了。为了解决这个问题，需要换个角度思考。</p>

<p>之前基于累计收益的策略梯度，并没有考虑折扣因子。如果考虑包含折扣因子的情况，相当于利用初始状态回报的期望，即<strong>初始状态的状态价值函数</strong>来衡量轨迹的优劣，对应的目标函数为</p>

\[J(\theta) = \mathbb{E} [R(\tau)]=  \mathbb{E}[\sum_{t=0}^T \gamma^{t-1} r_t] = V_{\pi(a\vert s ; \theta)}(s_0)\]

<p>简化起见，写为</p>

\[J(\theta) = \mathbb{E} [R(\tau)]= V_{\pi}(s)\]

<p>那么有</p>

\[\nabla_{\theta}J(\theta) = \nabla_{\theta}V_{\pi}(s)\]

<h3 id="33-策略梯度定理"><span class="me-2">3.3. 策略梯度定理</span><a href="#33-策略梯度定理" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>策略梯度定理本质就是求解目标函数 $J(\theta)$ 的梯度。首先使用贝尔曼期望方程对 $V(s)$ 进行展开</p>

\[\nabla_{\theta}V_{\pi}(s) = \nabla_{\theta}\sum_{a\in \mathcal{A(s)}} \pi(a\vert s)q_{\pi}(s,a) = \sum_{a\in \mathcal{A(s)}}\nabla_{\theta}[\pi(a\vert s)q_{\pi}(s,a)]\]

<p>注意到求梯度是对网络参数 $\theta$ 进行的，而所有和策略 $\pi$ 有关的项均隐含 $\theta$，所以上述式子需要采用乘法求导的形式展开</p>

\[\begin{aligned}
\nabla_{\theta}V_{\pi}(s) &amp;= \sum_{a\in \mathcal{A(s)}}\nabla_{\theta} [\pi(a\vert s)q_{\pi}(s,a)]\\
&amp;= \sum_{a\in \mathcal{A(s)}} [\nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a) + \pi(a\vert s) \nabla_{\theta}q_{\pi}(s,a)]\\
\end{aligned}\]

<p>其中<strong>第二项</strong></p>

\[\begin{aligned}
\nabla q_{\pi} (s,a) &amp;= \nabla \sum_{s^\prime,r} P(s^\prime,r\vert s,a)[r+\gamma V_{\pi}(s^\prime)]\\
&amp;=\nabla \sum_{s^\prime,r}^{s^\prime,r}  P(s^\prime,r|s,a)  r + \nabla \sum_{s^\prime,r} \gamma P(s^\prime,r|s,a)V_{\pi}(s^\prime)\\
&amp;=\nabla \sum_{s^\prime,r} \gamma P(s^\prime,r|s,a)V_{\pi}(s^\prime)\\
&amp;=\gamma \sum_r P(r\vert s,a)\sum_{s^\prime} P(s^\prime\vert s,a) \nabla V_{\pi}(s^\prime)\\
&amp;=\gamma \sum_{s^\prime} P(s^\prime\vert s,a) \nabla V_{\pi}(s^\prime)
\end{aligned}\]

<p>带回梯度式子，即</p>

\[\nabla_{\theta}V_{\pi}(s) = \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a) + \sum_{a\in \mathcal{A(s)}} \pi(a\vert s)\gamma \sum_{s^\prime} P(s^\prime\vert s,a) \nabla_{\theta} V_{\pi}(s^\prime)\]

<p>可以看出，上式呈现出 $\nabla_{\theta}V_{\pi}(s)$ 的迭代形式。为了更进一步确定迭代关系，再次对 $\nabla_{\theta}V_{\pi}(s^\prime)$ 展开，有</p>

\[\nabla_{\theta}V_{\pi}(s^\prime) = \sum_{a^\prime\in \mathcal{A(s^\prime)}} \nabla_{\theta}\pi(a^\prime\vert s^\prime) q_{\pi}(s^\prime,a^\prime) + \sum_{a^\prime\in \mathcal{A(s^\prime)}} \pi(a^\prime\vert s^\prime)\gamma \sum_{s^{\prime\prime}} P(s^{\prime\prime}\vert s^\prime,a^\prime) \nabla_{\theta} V_{\pi}(s^{\prime\prime})\]

<p>带回梯度式子并且<strong>完全展开</strong>，有</p>

<ul>
  <li><strong>第一项</strong></li>
</ul>

\[\sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a)\]

<ul>
  <li>
    <p><strong>第二项</strong></p>

\[\begin{aligned}
      &amp;\sum_{a\in \mathcal{A(s)}} \pi(a\vert s)\gamma \sum_{s^\prime} P(s^\prime\vert s,a)\cdot \sum_{a^\prime\in \mathcal{A(s^\prime)}} \nabla_{\theta}\pi(a^\prime\vert s^\prime) q_{\pi}(s^\prime,a^\prime) \\
      =&amp;\gamma \sum_{s^\prime}\sum_{a\in \mathcal{A(s)}} \pi(a\vert s) P(s^\prime\vert s,a)\cdot \sum_{a^\prime\in \mathcal{A(s^\prime)}} \nabla_{\theta}\pi(a^\prime\vert s^\prime) q_{\pi}(s^\prime,a^\prime) \\
      =&amp;\gamma \sum_{s^\prime} \mathcal{P}(s^\prime\vert s)\cdot \sum_{a^\prime\in \mathcal{A(s^\prime)}} \nabla_{\theta}\pi(a^\prime\vert s^\prime) q_{\pi}(s^\prime,a^\prime)\\
      \end{aligned}\]

    <p>上式仅包含前后状态的变量 $s,s^\prime$ 和转移次数常量 $k=1$（$a$ 最终会被求和吸收积分掉），因此可进一步写为通项式：</p>

\[\gamma \sum_{x\in S} \mathcal{P}(s\rightarrow x, k, \pi)\cdot \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert x) q_{\pi}(x,a)\]

    <p>同时，注意到第一项也符合上述通项式（$k=0,\gamma = 1$）</p>
  </li>
  <li>
    <p><strong>第三项</strong></p>

    <p>\(\sum_{a\in \mathcal{A(s)}} \pi(a\vert s)\gamma \sum_{s^\prime} P(s^\prime\vert s,a) \cdot \sum_{a^\prime\in \mathcal{A(s^\prime)}} \pi(a^\prime\vert s^\prime)\gamma \sum_{s^{\prime\prime}} P(s^{\prime\prime}\vert s^\prime,a^\prime) \nabla_{\theta} V_{\pi}(s^{\prime\prime})\)
注意到第三项仍然是个迭代式，可以继续展开，且展开除了最后一项，前面均符合前述通项式。</p>
  </li>
</ul>

<p>至此， $\nabla V_\pi(s)$ 的迭代式表示如下</p>

\[\nabla_{\theta}V_{\pi}(s) = \sum_{s\in S} \sum_{k=0}^{\infty} \gamma^{k} \mathcal{P}(s\rightarrow x, k, \pi)\cdot \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert x) q_{\pi}(x,a)\]

<p>将公式中所有 $s$ 替换为初始状态 $s_0$，将所有 $x$ 替换为 $s$，我们得到了初始状态价值函数的梯度</p>

\[\nabla_{\theta}V_{\pi}(s_0) = \sum_{s\in S} \sum_{k=0}^{\infty} \gamma^{k} \mathcal{P}(s_0\rightarrow s, k, \pi)\cdot \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a)\]

<p>其中，</p>

<ul>
  <li>$\mathcal{P}(s_0\rightarrow s, k, \pi)$ 表示初始状态 $s_0$ 经过 $k$ 次状态转移最终到达 $s$ 的概率；</li>
  <li>$\sum_{k=0}^{\infty} \mathcal{P}(s_0\rightarrow s, k, \pi)$ 表示在整条轨迹序列中，状态 $s$ 出现的<strong>平均次数</strong>，将其记为 $\eta(s)$；</li>
  <li>那么状态 $s$ 出现的<strong>概率</strong> $\mu(s)$ 可定义为其出现的平均次数除以所有状态出现的平均次数的和，即 $\mu(s) = {\eta(s)}/{\sum_{s^\prime} \eta(s^\prime)}$</li>
</ul>

<p>则</p>

\[\begin{aligned}
\nabla_{\theta}V_{\pi}(s_0) &amp;= \sum_{s\in S}  \gamma^{k} \eta(s) \cdot \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a)\\
&amp;= \sum_{s\in S}  \gamma^{k} \mu(s)\sum_{s^\prime} \eta(s^\prime)  \cdot \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a)\\
&amp;= \sum_{s^\prime} \eta(s^\prime) \cdot \sum_{s\in S} \gamma^{k} \mu(s)  \cdot \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a)\\
\end{aligned}\]

<p>其中，第一项为一个常数（归一化因子），$\gamma$ 折扣率也是一个常数，它们只影响梯度的具体数值。我们更关注梯度的方向，因此</p>

\[\nabla_{\theta}V_{\pi}(s_0) \propto \sum_{s\in S} \mu(s)  \cdot \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a)\]

<p>观察前面策略梯度方向的求解结果，可以发现 $\mu(s)$ 本身是状态 $s$ 出现的概率，因此我们可以其联通前面的求和符号一起表示为某种期望的形式</p>

\[\nabla_{\theta}J(\theta) \propto \sum_{s\in S} \mu(s)  \cdot \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a)=\mathbb{E}_{?}\left[ \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a) \right]\]

<p>问题转化为，确定期望符号中的概率分布是什么（也即上式中的 <code class="language-plaintext highlighter-rouge">?</code> 部分）。既然是关于状态的概率分布，我们定义这样一个分布符号：$\rho^{\pi_{\theta}}$，使得状态 $s$ 的出现概率服从该分布。该分布不仅和策略函数 $\pi(a\vert s;\theta)$ 相关，还与环境（状态转移概率）相关。</p>

\[\forall s\in S \rightarrow s\sim \rho^{\pi_{\theta}}(s)=lim_{t\rightarrow \infty} P(S_t=s\vert S_{t-1}=s\vert A_{0:t}\sim \pi)\]

<p>则</p>

\[\nabla_{\theta}J(\theta) \propto \mathbb{E}_{s\sim\rho^{\pi_{\theta}}}\left[ \sum_{a\in \mathcal{A(s)}} \nabla_{\theta}\pi(a\vert s) q_{\pi}(s,a) \right]\]

<p>为了使公式规范化</p>

<ul>
  <li>将所有状态 $s$ 和动作 $a$ 加上特定时刻 $t$ 下标；</li>
  <li>将策略函数的参数补充上；</li>
</ul>

\[\nabla_{\theta}J(\theta) \propto \mathbb{E}_{s_t\sim\rho^{\pi_{\theta}}}\left[ \sum_{a_t\in \mathcal{A(s_t)}} \nabla_{\theta}\pi(a_t\vert s_t;\theta) q_{\pi}(s_t,a_t) \right]\]

<p>继续观察，上式还存在一个求和符号，因此存在一个想法是将该部分也化为期望的形式。但 $\nabla_{\theta}\pi(a_t\vert s_t;\theta)$ 并不是策略函数，因此需要引入策略函数，借助之前 <code class="language-plaintext highlighter-rouge">log</code> 函数梯度的性质，有</p>

\[\begin{aligned}
\nabla_{\theta}J(\theta) \propto&amp; \mathbb{E}_{s_t\sim\rho^{\pi_{\theta}}}\left[ \sum_{a_t\in \mathcal{A(s_t)}} \nabla_{\theta}\pi(a_t\vert s_t;\theta) q_{\pi}(s_t,a_t) \right]\\
\propto&amp; \mathbb{E}_{s_t\sim\rho^{\pi_{\theta}}}\left[ \sum_{a_t\in \mathcal{A(s_t)}} \pi(a_t\vert s_t;\theta)\nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) q_{\pi}(s_t,a_t) \right]\\
\propto&amp; \mathbb{E}_{s_t\sim\rho^{\pi_{\theta}}, a_t\sim \pi} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) q_{\pi}(s_t,a_t) \right]\\
\end{aligned}\]

<p>将期望的下标进行简化，最终我们得到了基于价值的策略梯度</p>

\[\nabla_{\theta}J(\theta) \propto \mathbb{E} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) q_{\pi}(s_t,a_t) \right]\\\]

<p>那么如何计算 $q_{\pi}(s_t, a_t)$ 呢？有两种思路：</p>

<ul>
  <li>MC：使用采样的 $G_t$ 来近似估计，对应 REINFORCE 方法，或称为蒙特卡洛策略梯度方法；</li>
  <li>TD：使用 TD 算法，对应 Actor Critic 方法。</li>
</ul>

<h2 id="4-基于策略梯度的强化学习"><span class="me-2">4. 基于策略梯度的强化学习</span><a href="#4-基于策略梯度的强化学习" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="41-reinforce"><span class="me-2">4.1. REINFORCE</span><a href="#41-reinforce" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>前面之所以将 $\nabla_{\theta}J(\theta)$ 化简为期望形式，是因为期望形式就可以通过 <strong>蒙特卡洛方法</strong> 采样的方式来近似求解。</p>

<p>根据 Q 函数的定义有</p>

\[q_{\pi_{\theta}}(s_t, a_t) = \mathbb{E} [G_t\vert s_t, a_t]\]

<p>带入策略梯度定理有</p>

\[\nabla_{\theta}J(\theta) \propto \mathbb{E} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) G_t \right]\]

<p>至此，我们可以通过采样 $\nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) G_t$ 神经网络参数 $\theta$ 进行更新。这就是 <strong>REINFORECE</strong> 方法。REINFORCE 的本质就是用 $G_t$ 代替 $q_{\pi}(s_t, a_t)$。</p>

<p>用于估计最优策略的REINFORCE算法表示如下（采用增量更新的方式）</p>

<ul>
  <li><strong>输入</strong>：可微策略函数 $\pi(a \vert s;\theta)$，衰减因子 $\gamma$，学习率 $\alpha$</li>
  <li><strong>初始化</strong>：策略函数的参数 $\theta$</li>
  <li><strong>训练过程</strong>：
    <ul>
      <li>repeat 根据策略函数采样一条轨迹：$s_0, a_0, r_1, s_2, a_2, r_3, \cdots$</li>
      <li>loop $t=T-1, T-2, \cdots, 1,0$</li>
      <li>$\quad G\leftarrow \gamma G + R_{t+1}$</li>
      <li>$\quad \theta \leftarrow \theta + \alpha G \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta)$</li>
    </ul>
  </li>
</ul>

<p>根据定义，因为 $G_t$ 是 $Q(s,a)$ 的无偏估计，所以基于蒙特卡洛采样的方式进行参数更新的 REINFORCE 算法，得到的结果就是策略梯度的无偏估计，那么其方差呢？</p>

<p>结论：基于蒙特卡洛采样的方式进行参数更新的 REINFORCE 算法方差很大。通过观察策略梯度式</p>

\[\nabla_{\theta}J(\theta) \propto \mathbb{E} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) G_t \right]\]

<p>我们可以看出，$\nabla_{\theta}\ln\pi(a_t\vert s_t;\theta)$ 相当于 $G$ 的一个系数或者说权重，对于方差而言只起到放缩作用（根据公式 $Var(aX) = a^2Var(X)$），实际方差还是取决于 $G$ 本身。而 $G$ 的方差</p>

\[Var(G_t) = Var(R_{t+1}) + Var(R_{t+2}) + \cdots\]

<p>由此可以看出，方差大来源于两个原因：</p>

<ul>
  <li>若不同 $s$ 对应的 $R$ 范围相差较大，再加上 MC 的随机性，那么每一步的 $R$ 都会有较大的方差;</li>
  <li>因为采样轨迹长度的原因，出现方差累积。</li>
</ul>

<p>相应的解决方法如下：</p>
<ul>
  <li>通过添加基线缓解;</li>
  <li>使用 TD 方法代替 MC 方法，即通过Actor-Critic方法解决。</li>
</ul>

<h3 id="42-reinforce-with-baseline"><span class="me-2">4.2. REINFORCE with baseline</span><a href="#42-reinforce-with-baseline" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>通过添加基线（baseline）可以减小方差。具体形式如下</p>

\[\nabla_{\theta}J(\theta) \propto \mathbb{E} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) G_t \right]\]

<p>添加基线 $b(s)$ 后，目标函数的梯度变为</p>

\[\nabla_{\theta}J(\theta) \propto \mathbb{E} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) (G_t-b(s_t)) \right]\]

<p>参数更新方式变为</p>

\[\theta \leftarrow \theta + \alpha (G_t - b(s_t)) \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta)\]

<p>基线 $b(s)$ 可为任意变量或者函数，只要不与动作 $a$ 有关。因为它并不会改变策略梯度的期望，证明如下</p>

\[\begin{aligned}
\nabla_{\theta}J(\theta) &amp;\propto \mathbb{E} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) (G_t-b(s_t)) \right]\\
&amp;\propto \sum_{a}\pi(a\vert s) \nabla_{\theta}\ln \pi(a\vert s;\theta)(G_t-b(s_t))\\
&amp;\propto \sum_{a} \nabla_{\theta}\pi(a\vert s;\theta)(G_t-b(s_t))
\end{aligned}\]

<p>因为 $b(s)$ 与动作 $a$ 无关，可以提到求和符号外面，但注意 $G_t$ 不可以提到求和符号外面</p>

\[\begin{aligned}
\nabla_{\theta}J(\theta) &amp;\propto \sum_{a} \nabla_{\theta}\pi(a\vert s;\theta)(G_t-b(s_t))\\
&amp;\propto \sum_{a}  G_t \nabla_{\theta}\pi(a\vert s;\theta) - b(s_t) \sum_{a} \nabla_{\theta}\pi(a\vert s;\theta)\\
\end{aligned}\]

<p>对于后一项，交换梯度符号和求和符号，有</p>

\[b(s_t) \sum_{a} \nabla_{\theta}\pi(a\vert s;\theta) =b(s_t)\nabla_{\theta} \sum_{a} \pi(a\vert s;\theta)  = b(s_t)\nabla_{\theta} 1 = 0\]

<p>得证。</p>

<blockquote>
  <p>关于基线的更多内容参考：https://zhuanlan.zhihu.com/p/636907461</p>
</blockquote>

<p>为什么引入基线可以减小方差？证明如下：</p>

<p>首先根据方差的定义</p>

\[Var(X) = E(X^2)-E(X)^2\]

<p>那么有</p>

\[Var(\nabla_{\theta}J(\theta)) = Var(\nabla_{\theta}^2)-\nabla_{\theta}^2 E(\nabla_{\theta})\]

<p>我们又知道引入基线不改变期望，因此方差的第二项取值与是否包含基线无关，则只需要关注第一项即可</p>

\[\begin{aligned}
Var \nabla_{\theta}J(\theta)^2 &amp;= E[\nabla_{\theta}\ln\pi_{\theta}(a_t\vert s_t)^2\cdot (G_t - b(s_t))^2] \\
 &amp;= E[\nabla_{\theta}\ln\pi_{\theta}(a_t\vert s_t)^2]\cdot E[(G_t - b(s_t))^2]
\end{aligned}\]

<p>当基线 $b(s_t)=\mathbb{E}[G_t]$ 是 $G_t$ 的期望的某个估计的时候，方差最小，即</p>

\[E[\nabla_{\theta}\ln\pi_{\theta}(a_t\vert s_t)^2]\cdot E[(G_t - \mathbb{E}[G(s_t)])^2] &lt; E[\nabla_{\theta}\ln\pi_{\theta}(a_t\vert s_t)^2]\cdot E[(G_t - 0)^2]\]

<h3 id="43-actor-critic-related-to-v"><span class="me-2">4.3. Actor-Critic (related to V)</span><a href="#43-actor-critic-related-to-v" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>虽然基线 $b(s)$ 可为任意变量或者函数，但实际情况下，一种自然而然的想法是将基线设置为状态价值的估计，即</p>

\[b(s) = {v}(s_t) = \mathbb{E}(G_t \vert s=s_t)\]

<p>这样可以使得 $G_t - b(s)$ 更加稳定，随机性更少。更进一步，因为 REINFORCE 本身是基于蒙特卡洛方法，那么自然而然可以同样使用蒙特卡洛方法来学习价值函数，即</p>

\[b(s) = {v}(s_t;{w})\]

<p>其中 $w$ 是另一套习得的参数。</p>

<p>到此为止，我们就可以将 REINFORCE 算法转化为 Actor-Critic 算法，其中参数 $\theta$ 对应策略梯度部分的神经网络，扮演 Actor 的角色；而 $w$ 对应价值估计部分的神经网络，扮演 Critic 的角色。此时目标函数的梯度变为</p>

\[\nabla_{\theta}J(\theta) \propto \mathbb{E} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) (G_t-{v}(s_t;{w})) \right]\]

<p>更进一步，类似于基于价值的强化学习方法中，MC 到 TD 方法的逻辑演变，这里我们同样不需要采样完整的一条轨迹，而是仅采样一步，后续通过自举的方式逐步估计完整的轨迹。</p>

<p>此时价值网络的更新形式为</p>

\[\begin{aligned}
\delta_t &amp;= R_{t+1}+\gamma v(s_{t+1};\omega)-v(s_t;\omega)\\
J(\omega) &amp;= \frac{1}{2} \delta_t^2\\
\omega &amp;\leftarrow \omega - \alpha_{\omega} \delta_t \nabla_{\omega}v(s;\omega)
\end{aligned}\]

<p>此时策略网络的更新形式为</p>

\[\begin{aligned}
\delta_t &amp;= R_{t+1} + \gamma {v}(s_{t+1};w)-{v}(s_t;{w})\\
\nabla_{\theta}J(\theta) &amp;\propto \mathbb{E} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) \delta_t \right]\\
\theta &amp;\leftarrow \theta + \alpha \delta_t \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta)
\end{aligned}\]

<p><strong>Actor-Critic related to V</strong> 算法的训练过程如下</p>

<ul>
  <li><strong>输入</strong>：
    <ul>
      <li>可微策略函数 $\pi(a \vert s;\theta)$</li>
      <li>可微的价值函数 ${v}(s;w)$</li>
      <li>衰减因子 $\gamma$，学习率 $\alpha_\theta,\alpha_w$</li>
    </ul>
  </li>
  <li><strong>初始化</strong>：
    <ul>
      <li>策略函数的参数 $\theta$</li>
      <li>价值函数的参数 $w$</li>
    </ul>
  </li>
  <li><strong>训练过程</strong>：
    <ul>
      <li>loop 状态 $s$ 没有结束时
        <ul>
          <li>根据策略采样：$a\sim \pi(\cdot, s;\theta)$</li>
          <li>执行动作 $a$， 得到奖励 $R$，转移状态到 $s^\prime$</li>
          <li>$\delta \leftarrow R + \gamma {v}(s^\prime;w) - {v}(s;w)$</li>
          <li>$w \leftarrow w + \alpha_w \delta\nabla_{w}{v}(s;w)$</li>
          <li>$\beta \leftarrow \gamma \delta$</li>
          <li>$\theta \leftarrow \theta + \alpha \beta \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta)$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="44-actor-critic-related-to-q"><span class="me-2">4.4. Actor-Critic (related to Q)</span><a href="#44-actor-critic-related-to-q" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>前面我们采用状态价值函数 $v(s)$ 作为基线函数，我们同样可以采用状态-动作价值函数 $q(s,a)$ 作为基线函数</p>

\[b(s_t) = q(s_t, a_t;\omega)\]

<p>由于对于一个状态而言，其状态-动作价值函数有多个取值，我们一般使用其最大值来计算基线，</p>

<p>此时价值网络的更新形式为</p>

\[\begin{aligned}
\delta_t &amp;= R_{t+1}+\gamma q(s_t, a_t;\omega)-q(s_t, a_t;\omega)\\
J(\omega) &amp;= \frac{1}{2} \delta_t^2\\
\omega &amp;\leftarrow \omega - \alpha_{\omega} \delta_t \nabla_{\omega}q(s,a;\omega)
\end{aligned}\]

<p>对于策略网络的目标函数的梯度，应该使用原始的策略网络的目标函数的梯度形式，将其中动作价值函数 $q(s,a)$ 替换为使用了价值网络来的估计值，得到</p>

\[\nabla_{\theta}J(\theta) \propto \mathbb{E} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) q(s_t,a_t;\omega) \right]\]

<p>此时策略网络的更新形式为</p>

\[\begin{aligned}
\nabla_{\theta}J(\theta) &amp;\propto \mathbb{E} [ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) q(s_t,a_t;\omega)]\\
\theta &amp;\leftarrow \theta + \alpha_{\theta}{q(s,a;\omega)}\nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) 
\end{aligned}\]

<h3 id="45-advantage-actor-critic-a2c"><span class="me-2">4.5. Advantage Actor-Critic (A2C)</span><a href="#45-advantage-actor-critic-a2c" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>前面介绍 VAC 的时候，我们引入一个基线，即在计算期望的时候用累计奖励 $G_t$ 减去一个 baseline ，这样做的好处是可以让梯度减小，因此梯度下降的步子也就更平缓，从而使训练过程更稳定。进一步，我们讨论了将基线设置为状态价值函数是一个较优的选择。本着这种思想，我们再次回顾策略梯度的原始目标函数梯度式，同样引入状态价值函数 $v_{\pi}(s_t)$ 作为基线，有</p>

\[\nabla_{\theta}J(\theta) \propto \mathbb{E} \left[ \nabla_{\theta}\ln\pi(a_t\vert s_t;\theta) (q_{\pi}(s_t,a_t)-v_{\pi}(s_t)) \right]\]

<p>我们称 $A(s_t, a_t) = q_{\pi}(s_t,a_t)-v_{\pi}(s_t)$ 为优势函数。优势函数表示在状态 $s$ 下采取动作 $a$ 相对于平均水平的优势。因此原先用 Critic 网络对 $q$ 函数的估计就可以改成对优势函数的估计，估算每个 $q(s_t,a_t)$ 对相对于平均值 $V(s_t)$ 的优势。这种算法就是 Advantage Actor Critic ，即 A2C。</p>

<p>注意到，A2C 在具体实现的时候需要同时估计 $q(s_t, a_t)$ 和 $v(s_t)$，估计一个价值函数已经会存在误差了，估计两个价值函数误差会更大。但是我们知道 $q(s_t, a_t)$ 和 $v(s_t)$ 之间是存在关系的，在具体采样时，有</p>

\[q(s_t, a_t) = r_{t+1} + \gamma v(s_{t+1})\]

<p>此时我们发现，A2C 即前面介绍过的 VAC 算法，价值网络只用估计 $v(s_t)$ 的值就够了。</p>

<h2 id="5-参考文献"><span class="me-2">5. 参考文献</span><a href="#5-参考文献" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>[1] 静静的喝酒. <a href="https://blog.csdn.net/qq_34758157/article/details/125886399">CSDN 策略梯度方法介绍——Value-Based强化学习方法 VS Policy-Based强化学习方法</a></p>

<p>[2] 静静的喝酒. <a href="https://blog.csdn.net/qq_34758157/article/details/125929926">CSDN 策略梯度方法介绍——策略梯度定理推导过程</a></p>

<p>[3] 静静的喝酒. <a href="https://blog.csdn.net/qq_34758157/article/details/125945391">策略梯度方法介绍——蒙特卡洛策略梯度方法(REINFORCE)</a></p>

<p>[4] Richard S. Sutton and Andrew G. Barto. <a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction</a></p>

</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw me-1"></i>
    
      <a href='/categories/academic/'>Academic</a>,
      <a href='/categories/knowledge/'>Knowledge</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw me-1"></i>
      
      <a href="/tags/reinforcement-learning/"
          class="post-tag no-text-decoration" >reinforcement learning</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        本文由作者按照 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         进行授权

      
    </div>

    <!-- Post sharing snippet -->

<div class="share-wrapper">
  <span class="share-label text-muted me-1">分享</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%B3%95%EF%BC%89%20-%20SIRLIS&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Policy-Gradient%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fab fa-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%B3%95%EF%BC%89%20-%20SIRLIS&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Policy-Gradient%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Freinforcement-learning-Policy-Gradient%2F&text=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%B3%95%EF%BC%89%20-%20SIRLIS"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <i
      id="copy-link"
      class="fa-fw fas fa-link small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="分享链接"
      data-title-succeed="链接已复制！"
    >
    </i>
  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
      
    </div>
  </div>
  <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
    <div class="access">
      <!-- Get the last 5 posts from lastmod list. -->














  <div id="access-lastmod" class="post">
    <div class="panel-heading">最近更新</div>
    <ul class="post-content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Value-Approximation/">强化学习（值函数近似）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/vscode-c/">VSCode部署C/C++开发环境</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-markov-process/">强化学习（马尔可夫决策过程）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Temporal-Differences/">强化学习（时序差分法）</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/reinforcement-learning-Monte-Carlo/">强化学习（蒙特卡洛法）</a>
        </li>
      
    </ul>
  </div>
  <!-- #access-lastmod -->


      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>

    
      
      



  <div id="toc-wrapper" class="ps-0 pe-4 mb-5">
    <div class="panel-heading ps-3 pt-2 mb-2">文章内容</div>
    <nav id="toc"></nav>
  </div>


    
  </div>
</div>

<!-- tail -->

  <div class="row">
    <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-3 pe-xl-4 mt-5">
      
        
        <!--
  Recommend the other 3 posts according to the tags and categories of the current post,
  if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->








  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  






<!-- Fill with the other newlest posts -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ms-1" data-toc-skip>
      相关文章
    </h3>
    <div class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Value-Approximation/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1672654339"
  data-df="YYYY/MM/DD"
  
>
  2023/01/02
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（值函数近似）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文首先介绍了值函数近似（Value Approximation），然后分别结合 SARSA 和 Q-Learning 给出了两种 Q 函数近似的方法。通过分析线性函数作为估计函数的局限性，自然引入神经网络来进行非线性函数近似，引出了基于深度学习的 Q 函数估计网络：Deep Q-Network（DQN）。






  1. 引言
  2. 状态价值函数近似
    
      2.1...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-markov-process/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1667968579"
  data-df="YYYY/MM/DD"
  
>
  2022/11/09
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（马尔可夫决策过程）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的基本概念和模型，主要包括马尔可夫过程、马尔可夫奖励过程和马尔可夫决策过程。






  1. 强化学习
    
      1.1. 状态空间
        
          1.1.1. 状态
          1.1.2. 观测
        
      
      1.2. 动作空间
      1.3. 策略
    
  
  2. 马尔可夫...
                </p>
              </div>
            </div>
          </a>
        </div>
      
        
        
        <div class="col">
          <a href="/posts/reinforcement-learning-Dynamic-Programming/" class="card post-preview h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em
  class="small"
  data-ts="1669433059"
  data-df="YYYY/MM/DD"
  
>
  2022/11/26
</em>

              <h4 class="pt-0 my-2" data-toc-skip>强化学习（动态规划）</h4>
              <div class="text-muted small">
                <p>
                  





                  本文介绍了强化学习的动态规划法（Dynamic Programming，DP），采用动态规划的思想，分别介绍策略迭代和价值迭代方法。






  1. 强化学习问题的求解
  2. 动态规划
    
      2.1. 策略迭代
        
          2.1.1. 策略评估
          2.1.2. 策略改进
          2.1.3. 算法流程
   ...
                </p>
              </div>
            </div>
          </a>
        </div>
      
    </div>
    <!-- .card-deck -->
  </div>
  <!-- #related-posts -->


      
        
        <!-- Navigation buttons at the bottom of the post. -->

<div class="post-navigation d-flex justify-content-between">
  
    <a
      href="/posts/linux-reduce-repo-size-through-git-folder/"
      class="btn btn-outline-primary"
      prompt="上一篇"
    >
      <p>如何清理 .git 文件夹来减小 github 仓库大小</p>
    </a>
  

  
    <a
      href="/posts/Pattern-Recognition-Linear-Classifier/"
      class="btn btn-outline-primary"
      prompt="下一篇"
    >
      <p>模式识别（线性分类器）</p>
    </a>
  
</div>

      
        
        <!--  The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script src="https://utteranc.es/client.js"
        repo="sirlis/sirlis.github.io"
        issue-term="pathname"
        crossorigin="anonymous"
        async>
</script>

<script type="text/javascript">
  $(function() {
    const origin = "https://utteranc.es";
    const iframe = "iframe.utterances-frame";
    const lightTheme = "github-light";
    const darkTheme = "github-dark";
    let initTheme = lightTheme;

    if ($("html[data-mode=dark]").length > 0
        || ($("html[data-mode]").length == 0
            && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
      initTheme = darkTheme;
    }

    addEventListener("message", (event) => {
      let theme;

      /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */
      if (event.origin === origin) {
        /* page initial */
        theme = initTheme;

      } else if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {
        /* global theme mode changed */
        const mode = event.data.message;
        theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme);

      } else {
        return;
      }

      const message = {
        type: "set-theme",
        theme: theme
      };

      const utterances = document.querySelector(iframe).contentWindow;
      utterances.postMessage(message, origin);
    });

  });
</script>



      
    </div>
  </div>


        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 post-content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">热门标签</div>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/fuzzy/">fuzzy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/vscode/">vscode</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/other/">other</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/astronomy/">astronomy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/c-c/">c/c++</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/computer-vision/">computer vision</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/meta-learning/">meta learning</a>
      
    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>
    </div>

    <!-- The Footer -->

<footer>
  <div class="container px-lg-4">
    <div class="d-flex justify-content-center align-items-center text-muted mx-md-3">
      <p>本站采用 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 主题 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
      </p>

      <p>©
        2025
        <a href="https://github.com/sirlis">sirlis</a>.
        
          <span
            data-bs-toggle="tooltip"
            data-bs-placement="top"
            title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。"
          >保留部分权利。</span>
        
      </p>
    </div>
  </div>
</footer>


    <div id="mask"></div>

    <button id="back-to-top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow">
      <i class="fas fa-angle-up"></i>
    </button>

    
      <div
        id="notification"
        class="toast"
        role="alert"
        aria-live="assertive"
        aria-atomic="true"
        data-bs-animation="true"
        data-bs-autohide="false"
      >
        <div class="toast-header">
          <button
            type="button"
            class="btn-close ms-auto"
            data-bs-dismiss="toast"
            aria-label="Close"
          ></button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="px-2 mb-3">发现新版本的内容。</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            更新
          </button>
        </div>
      </div>
    

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.0/dist/jquery.min.js,npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.7/dayjs.min.js,npm/dayjs@1.11.7/locale/zh.min.js,npm/dayjs@1.11.7/plugin/relativeTime.min.js,npm/dayjs@1.11.7/plugin/localizedFormat.min.js,npm/tocbot@4.21.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '<div class="px-1 px-sm-2 px-lg-4 px-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

